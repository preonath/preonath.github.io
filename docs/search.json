[
  {
    "objectID": "cv/index.html",
    "href": "cv/index.html",
    "title": "",
    "section": "",
    "text": "Download current CV"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "",
    "text": "Liver disorders pose a significant health burden worldwide, leading to increased morbidity and mortality rates. Early detection and accurate diagnosis of liver disorders are crucial for effective treatment and improved patient outcomes. However, the complexity and diverse etiologies of liver disorders make their diagnosis challenging for clinicians. Therefore, there is a pressing need for the development of reliable and efficient predictive models using machine learning techniques to aid in the early identification and prediction of liver disorders."
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#problem",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#problem",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "",
    "text": "Liver disorders pose a significant health burden worldwide, leading to increased morbidity and mortality rates. Early detection and accurate diagnosis of liver disorders are crucial for effective treatment and improved patient outcomes. However, the complexity and diverse etiologies of liver disorders make their diagnosis challenging for clinicians. Therefore, there is a pressing need for the development of reliable and efficient predictive models using machine learning techniques to aid in the early identification and prediction of liver disorders."
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#objective",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#objective",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Objective",
    "text": "Objective\nThe objective of this study is to develop a machine learning-based predictive model for the early detection and prediction of liver disorders.\nData Source: https://www.kaggle.com/datasets/uciml/indian-liver-patient-records\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv(\"../data/indian_liver_patient.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nAge\nGender\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nOutcome\n\n\n\n\n0\n65\nFemale\n0.7\n0.1\n187\n16\n18\n6.8\n3.3\n0.90\n1\n\n\n1\n62\nMale\n10.9\n5.5\n699\n64\n100\n7.5\n3.2\n0.74\n1\n\n\n2\n62\nMale\n7.3\n4.1\n490\n60\n68\n7.0\n3.3\n0.89\n1\n\n\n3\n58\nMale\n1.0\n0.4\n182\n14\n20\n6.8\n3.4\n1.00\n1\n\n\n4\n72\nMale\n3.9\n2.0\n195\n27\n59\n7.3\n2.4\n0.40\n1"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#exploring-data",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#exploring-data",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(583, 11)\n\n\n\n# dtypes \ndata.dtypes\n\nAge                             int64\nGender                         object\nTotal_Bilirubin               float64\nDirect_Bilirubin              float64\nAlkaline_Phosphotase            int64\nAlamine_Aminotransferase        int64\nAspartate_Aminotransferase      int64\nTotal_Protiens                float64\nAlbumin                       float64\nAlbumin_and_Globulin_Ratio    float64\nOutcome                         int64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 583 entries, 0 to 582\nData columns (total 11 columns):\n #   Column                      Non-Null Count  Dtype  \n---  ------                      --------------  -----  \n 0   Age                         583 non-null    int64  \n 1   Gender                      583 non-null    object \n 2   Total_Bilirubin             583 non-null    float64\n 3   Direct_Bilirubin            583 non-null    float64\n 4   Alkaline_Phosphotase        583 non-null    int64  \n 5   Alamine_Aminotransferase    583 non-null    int64  \n 6   Aspartate_Aminotransferase  583 non-null    int64  \n 7   Total_Protiens              583 non-null    float64\n 8   Albumin                     583 non-null    float64\n 9   Albumin_and_Globulin_Ratio  579 non-null    float64\n 10  Outcome                     583 non-null    int64  \ndtypes: float64(5), int64(5), object(1)\nmemory usage: 50.2+ KB\n\n\n\ndata['Outcome'] = data['Outcome'].astype(object)\n\n\n# check missing data \ndata.isnull().sum() \n\nAge                           0\nGender                        0\nTotal_Bilirubin               0\nDirect_Bilirubin              0\nAlkaline_Phosphotase          0\nAlamine_Aminotransferase      0\nAspartate_Aminotransferase    0\nTotal_Protiens                0\nAlbumin                       0\nAlbumin_and_Globulin_Ratio    4\nOutcome                       0\ndtype: int64"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#descriptive-statistics",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#descriptive-statistics",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# select numeric data \nnum_cols = data.select_dtypes(exclude = 'object')\nnum_cols.head() \n\n\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\n0\n65\n0.7\n0.1\n187\n16\n18\n6.8\n3.3\n0.90\n\n\n1\n62\n10.9\n5.5\n699\n64\n100\n7.5\n3.2\n0.74\n\n\n2\n62\n7.3\n4.1\n490\n60\n68\n7.0\n3.3\n0.89\n\n\n3\n58\n1.0\n0.4\n182\n14\n20\n6.8\n3.4\n1.00\n\n\n4\n72\n3.9\n2.0\n195\n27\n59\n7.3\n2.4\n0.40\n\n\n\n\n\n\n\n\n# summary statistics of numerical variables \nrp.summary_cont(num_cols[['Age', 'Total_Bilirubin', 'Direct_Bilirubin', 'Alkaline_Phosphotase',\n       'Alamine_Aminotransferase', 'Aspartate_Aminotransferase',\n       'Total_Protiens', 'Albumin', 'Albumin_and_Globulin_Ratio']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nAge\n583.0\n44.7461\n16.1898\n0.6705\n43.4292\n46.0631\n\n\n1\nTotal_Bilirubin\n583.0\n3.2988\n6.2095\n0.2572\n2.7937\n3.8039\n\n\n2\nDirect_Bilirubin\n583.0\n1.4861\n2.8085\n0.1163\n1.2577\n1.7146\n\n\n3\nAlkaline_Phosphotase\n583.0\n290.5763\n242.9380\n10.0615\n270.8151\n310.3375\n\n\n4\nAlamine_Aminotransferase\n583.0\n80.7136\n182.6204\n7.5634\n65.8587\n95.5684\n\n\n5\nAspartate_Aminotransferase\n583.0\n109.9108\n288.9185\n11.9658\n86.4094\n133.4122\n\n\n6\nTotal_Protiens\n583.0\n6.4832\n1.0855\n0.0450\n6.3949\n6.5715\n\n\n7\nAlbumin\n583.0\n3.1419\n0.7955\n0.0329\n3.0771\n3.2066\n\n\n8\nAlbumin_and_Globulin_Ratio\n579.0\n0.9471\n0.3196\n0.0133\n0.9210\n0.9732\n\n\n\n\n\n\n\n\n# select categorical data \ncat_cols = data.select_dtypes(include = 'object')\ncat_cols.head() \n\n\n\n\n\n\n\n\nGender\nOutcome\n\n\n\n\n0\nFemale\n1\n\n\n1\nMale\n1\n\n\n2\nMale\n1\n\n\n3\nMale\n1\n\n\n4\nMale\n1\n\n\n\n\n\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(cat_cols[['Gender', 'Outcome']])\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:225: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\nGender\nMale\n441\n75.64\n\n\n1\n\nFemale\n142\n24.36\n\n\n2\nOutcome\n1\n416\n71.36\n\n\n3\n\n2\n167\n28.64"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#correlations-between-variables",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#correlations-between-variables",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_19768\\427603040.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.corr(method='pearson')\n\n\n\n\n\n\n\n\n\nAge\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\nAge\n1.000000\n0.011763\n0.007529\n0.080425\n-0.086883\n-0.019910\n-0.187461\n-0.265924\n-0.216408\n\n\nTotal_Bilirubin\n0.011763\n1.000000\n0.874618\n0.206669\n0.214065\n0.237831\n-0.008099\n-0.222250\n-0.206267\n\n\nDirect_Bilirubin\n0.007529\n0.874618\n1.000000\n0.234939\n0.233894\n0.257544\n-0.000139\n-0.228531\n-0.200125\n\n\nAlkaline_Phosphotase\n0.080425\n0.206669\n0.234939\n1.000000\n0.125680\n0.167196\n-0.028514\n-0.165453\n-0.234166\n\n\nAlamine_Aminotransferase\n-0.086883\n0.214065\n0.233894\n0.125680\n1.000000\n0.791966\n-0.042518\n-0.029742\n-0.002375\n\n\nAspartate_Aminotransferase\n-0.019910\n0.237831\n0.257544\n0.167196\n0.791966\n1.000000\n-0.025645\n-0.085290\n-0.070040\n\n\nTotal_Protiens\n-0.187461\n-0.008099\n-0.000139\n-0.028514\n-0.042518\n-0.025645\n1.000000\n0.784053\n0.234887\n\n\nAlbumin\n-0.265924\n-0.222250\n-0.228531\n-0.165453\n-0.029742\n-0.085290\n0.784053\n1.000000\n0.689632\n\n\nAlbumin_and_Globulin_Ratio\n-0.216408\n-0.206267\n-0.200125\n-0.234166\n-0.002375\n-0.070040\n0.234887\n0.689632\n1.000000"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#skewness",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#skewness",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_19768\\942340472.py:2: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.skew()\n\n\nAge                           -0.029385\nTotal_Bilirubin                4.907474\nDirect_Bilirubin               3.212403\nAlkaline_Phosphotase           3.765106\nAlamine_Aminotransferase       6.549192\nAspartate_Aminotransferase    10.546177\nTotal_Protiens                -0.285672\nAlbumin                       -0.043685\nAlbumin_and_Globulin_Ratio     0.992299\nOutcome                        0.947140\ndtype: float64"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#data-visualizations",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#data-visualizations",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(3,3))\nplt.show() \n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(3,3))\nplt.show() \n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(20,6))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_19768\\4023993825.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr = data.corr()"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#setup",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#setup",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nAge\nGender\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nOutcome\n\n\n\n\n0\n65\nFemale\n0.7\n0.1\n187\n16\n18\n6.8\n3.3\n0.90\n1\n\n\n1\n62\nMale\n10.9\n5.5\n699\n64\n100\n7.5\n3.2\n0.74\n1\n\n\n2\n62\nMale\n7.3\n4.1\n490\n60\n68\n7.0\n3.3\n0.89\n1\n\n\n3\n58\nMale\n1.0\n0.4\n182\n14\n20\n6.8\n3.4\n1.00\n1\n\n\n4\n72\nMale\n3.9\n2.0\n195\n27\n59\n7.3\n2.4\n0.40\n1\n\n\n\n\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.classification import *\nsetup(data, target = 'Outcome', session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nOutcome\n\n\n2\nTarget type\nBinary\n\n\n3\nTarget mapping\n1: 0, 2: 1\n\n\n4\nOriginal data shape\n(583, 11)\n\n\n5\nTransformed data shape\n(583, 11)\n\n\n6\nTransformed train set shape\n(408, 11)\n\n\n7\nTransformed test set shape\n(175, 11)\n\n\n8\nOrdinal features\n1\n\n\n9\nNumeric features\n9\n\n\n10\nCategorical features\n1\n\n\n11\nRows with missing values\n0.7%\n\n\n12\nPreprocess\nTrue\n\n\n13\nImputation type\nsimple\n\n\n14\nNumeric imputation\nmean\n\n\n15\nCategorical imputation\nmode\n\n\n16\nMaximum one-hot encoding\n25\n\n\n17\nEncoding method\nNone\n\n\n18\nFold Generator\nStratifiedKFold\n\n\n19\nFold Number\n10\n\n\n20\nCPU Jobs\n-1\n\n\n21\nUse GPU\nFalse\n\n\n22\nLog Experiment\nFalse\n\n\n23\nExperiment Name\nclf-default-name\n\n\n24\nUSI\n6a11\n\n\n\n\n\n&lt;pycaret.classification.oop.ClassificationExperiment at 0x28b2b8db0d0&gt;"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#compare-models",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#compare-models",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\nlr\nLogistic Regression\n0.7133\n0.7275\n0.2144\n0.4688\n0.2851\n0.1487\n0.1644\n0.9860\n\n\ndummy\nDummy Classifier\n0.7133\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.4000\n\n\net\nExtra Trees Classifier\n0.7109\n0.7232\n0.2561\n0.4951\n0.3326\n0.1735\n0.1902\n0.4540\n\n\nada\nAda Boost Classifier\n0.7107\n0.7245\n0.3856\n0.4792\n0.4186\n0.2354\n0.2402\n0.4120\n\n\nridge\nRidge Classifier\n0.7084\n0.0000\n0.0515\n0.2000\n0.0810\n0.0302\n0.0328\n0.3890\n\n\nlda\nLinear Discriminant Analysis\n0.7035\n0.7020\n0.0848\n0.3000\n0.1317\n0.0459\n0.0543\n0.3950\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7009\n0.6985\n0.3591\n0.4820\n0.4020\n0.2126\n0.2206\n0.5030\n\n\nrf\nRandom Forest Classifier\n0.6888\n0.6992\n0.2227\n0.4330\n0.2818\n0.1140\n0.1284\n0.4490\n\n\nsvm\nSVM - Linear Kernel\n0.6865\n0.0000\n0.1598\n0.2006\n0.1632\n0.0548\n0.0566\n0.3830\n\n\ngbc\nGradient Boosting Classifier\n0.6813\n0.6986\n0.2508\n0.4298\n0.3047\n0.1196\n0.1324\n0.4300\n\n\nknn\nK Neighbors Classifier\n0.6671\n0.6361\n0.2765\n0.3822\n0.3137\n0.1069\n0.1104\n0.4030\n\n\ndt\nDecision Tree Classifier\n0.6445\n0.5741\n0.4091\n0.3807\n0.3850\n0.1403\n0.1445\n0.3870\n\n\nqda\nQuadratic Discriminant Analysis\n0.5344\n0.7108\n0.8977\n0.3743\n0.5265\n0.2044\n0.2841\n0.3920\n\n\nnb\nNaive Bayes\n0.5271\n0.7272\n0.9402\n0.3741\n0.5341\n0.2096\n0.3066\n0.3850"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#create-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#create-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \nlr = create_model('lr')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.7317\n0.6303\n0.4545\n0.5000\n0.4762\n0.2964\n0.2970\n\n\n1\n0.6341\n0.7213\n0.0000\n0.0000\n0.0000\n-0.1326\n-0.1807\n\n\n2\n0.7805\n0.7845\n0.3333\n0.8000\n0.4706\n0.3605\n0.4155\n\n\n3\n0.7561\n0.7500\n0.3333\n0.6667\n0.4444\n0.3098\n0.3403\n\n\n4\n0.6829\n0.6695\n0.1667\n0.4000\n0.2353\n0.0763\n0.0879\n\n\n5\n0.7073\n0.6638\n0.1667\n0.5000\n0.2500\n0.1214\n0.1498\n\n\n6\n0.6585\n0.6149\n0.0833\n0.2500\n0.1250\n-0.0250\n-0.0308\n\n\n7\n0.7317\n0.8075\n0.3333\n0.5714\n0.4211\n0.2619\n0.2780\n\n\n8\n0.7000\n0.8370\n0.0909\n0.3333\n0.1429\n0.0283\n0.0372\n\n\n9\n0.7500\n0.7962\n0.1818\n0.6667\n0.2857\n0.1903\n0.2498\n\n\nMean\n0.7133\n0.7275\n0.2144\n0.4688\n0.2851\n0.1487\n0.1644\n\n\nStd\n0.0433\n0.0751\n0.1357\n0.2216\n0.1567\n0.1539\n0.1764\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(lr)\n\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#tune-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#tune-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of rf\ntuned_lr = tune_model(lr)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.7805\n0.6636\n0.2727\n0.7500\n0.4000\n0.2998\n0.3575\n\n\n1\n0.6341\n0.7241\n0.0000\n0.0000\n0.0000\n-0.1326\n-0.1807\n\n\n2\n0.7805\n0.8046\n0.3333\n0.8000\n0.4706\n0.3605\n0.4155\n\n\n3\n0.7805\n0.7414\n0.3333\n0.8000\n0.4706\n0.3605\n0.4155\n\n\n4\n0.6585\n0.6466\n0.0833\n0.2500\n0.1250\n-0.0250\n-0.0308\n\n\n5\n0.7317\n0.6580\n0.1667\n0.6667\n0.2667\n0.1694\n0.2309\n\n\n6\n0.6341\n0.5920\n0.0833\n0.2000\n0.1176\n-0.0659\n-0.0759\n\n\n7\n0.7561\n0.7989\n0.3333\n0.6667\n0.4444\n0.3098\n0.3403\n\n\n8\n0.7000\n0.8558\n0.0000\n0.0000\n0.0000\n-0.0480\n-0.0986\n\n\n9\n0.7500\n0.8119\n0.0909\n1.0000\n0.1667\n0.1266\n0.2600\n\n\nMean\n0.7206\n0.7297\n0.1697\n0.5133\n0.2462\n0.1355\n0.1634\n\n\nStd\n0.0568\n0.0828\n0.1303\n0.3462\n0.1797\n0.1821\n0.2218\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_lr, tuner = tune_model(lr, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.7805\n0.6636\n0.2727\n0.7500\n0.4000\n0.2998\n0.3575\n\n\n1\n0.6341\n0.7241\n0.0000\n0.0000\n0.0000\n-0.1326\n-0.1807\n\n\n2\n0.7805\n0.8046\n0.3333\n0.8000\n0.4706\n0.3605\n0.4155\n\n\n3\n0.7805\n0.7414\n0.3333\n0.8000\n0.4706\n0.3605\n0.4155\n\n\n4\n0.6585\n0.6466\n0.0833\n0.2500\n0.1250\n-0.0250\n-0.0308\n\n\n5\n0.7317\n0.6580\n0.1667\n0.6667\n0.2667\n0.1694\n0.2309\n\n\n6\n0.6341\n0.5920\n0.0833\n0.2000\n0.1176\n-0.0659\n-0.0759\n\n\n7\n0.7561\n0.7989\n0.3333\n0.6667\n0.4444\n0.3098\n0.3403\n\n\n8\n0.7000\n0.8558\n0.0000\n0.0000\n0.0000\n-0.0480\n-0.0986\n\n\n9\n0.7500\n0.8119\n0.0909\n1.0000\n0.1667\n0.1266\n0.2600\n\n\nMean\n0.7206\n0.7297\n0.1697\n0.5133\n0.2462\n0.1355\n0.1634\n\n\nStd\n0.0568\n0.0828\n0.1303\n0.3462\n0.1797\n0.1821\n0.2218\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\n\n\n\ntuned_lr\n\nLogisticRegression(C=0.056, class_weight={}, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LogisticRegressionLogisticRegression(C=0.056, class_weight={}, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000,\n                   multi_class='auto', n_jobs=None, penalty='l2',\n                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,\n                   warm_start=False)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                   param_distributions={'actual_estimator__C': [0.001, 0.002,\n                                                                0.003, 0.004,\n                                                                0.005, 0.006,\n                                                                0.007, 0.008,\n                                                                0.009, 0.01,\n                                                                0.011, 0.012,\n                                                                0.013, 0.014,\n                                                                0.015, 0.016,\n                                                                0.017, 0.018,\n                                                                0.019, 0.02,\n                                                                0.021, 0.022,\n                                                                0.023, 0.024,\n                                                                0.025, 0.026,\n                                                                0.027, 0.028,\n                                                                0.029, 0.03, ...],\n                                        'actual_estimator__class_weight': ['balanced',\n                                                                           {}]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                   param_distributions={'actual_estimator__C': [0.001, 0.002,\n                                                                0.003, 0.004,\n                                                                0.005, 0.006,\n                                                                0.007, 0.008,\n                                                                0.009, 0.01,\n                                                                0.011, 0.012,\n                                                                0.013, 0.014,\n                                                                0.015, 0.016,\n                                                                0.017, 0.018,\n                                                                0.019, 0.02,\n                                                                0.021, 0.022,\n                                                                0.023, 0.024,\n                                                                0.025, 0.026,\n                                                                0.027, 0.028,\n                                                                0.029, 0.03, ...],\n                                        'actual_estimator__class_weight': ['balanced',\n                                                                           {}]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['Age', 'Total_Bilirubin',\n                                             'Direct_Bilirubin',\n                                             'Alkaline_Phosphotase',\n                                             'Alamine_Aminotransferase',\n                                             'Asp...\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=1000,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=123,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(include=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\n                            'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\n                            'Aspartate_Aminotransferase', 'Total_Protiens',\n                            'Albumin', 'Albumin_and_Globulin_Ratio'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])LogisticRegressionLogisticRegression(max_iter=1000, random_state=123)"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#analyze-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#analyze-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# plot confusion matrix\nplot_model(lr, plot = 'confusion_matrix')\n\n\n\n\n\n\n\n\n# plot AUC\nplot_model(lr, plot = 'auc')\n\n\n\n\n\n\n\n\n# plot class report\nplot_model(lr, plot = 'class_report')\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(lr, plot = 'feature')"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#evaluate-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#evaluate-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(lr)"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#finalize-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#finalize-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(lr)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['Age', 'Total_Bilirubin',\n                                             'Direct_Bilirubin',\n                                             'Alkaline_Phosphotase',\n                                             'Alamine_Aminotransferase',\n                                             'Asp...\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=1000,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=123,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['Age', 'Total_Bilirubin',\n                                             'Direct_Bilirubin',\n                                             'Alkaline_Phosphotase',\n                                             'Alamine_Aminotransferase',\n                                             'Asp...\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=1000,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=123,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\n                            'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\n                            'Aspartate_Aminotransferase', 'Total_Protiens',\n                            'Albumin', 'Albumin_and_Globulin_Ratio'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['Gender'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])LogisticRegressionLogisticRegression(max_iter=1000, random_state=123)"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#prediction",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#prediction",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(lr)\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\nLogistic Regression\n0.7029\n0.7816\n0.2200\n0.4583\n0.2973\n0.1374\n0.1523\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nAge\nGender\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nOutcome\nprediction_label\nprediction_score\n\n\n\n\n532\n62\nMale\n0.7\n0.2\n162\n12\n17\n8.2\n3.2\n0.6\n1\n1\n0.7702\n\n\n406\n45\nMale\n0.7\n0.2\n180\n18\n58\n6.7\n3.7\n1.2\n1\n1\n0.5262\n\n\n350\n37\nMale\n1.8\n0.8\n145\n62\n58\n5.7\n2.9\n1.0\n0\n1\n0.7040\n\n\n573\n32\nMale\n3.7\n1.6\n612\n50\n88\n6.2\n1.9\n0.4\n0\n1\n0.8988\n\n\n543\n40\nMale\n1.2\n0.6\n204\n23\n27\n7.6\n4.0\n1.1\n0\n1\n0.5881\n\n\n\n\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('Outcome', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nAge\nGender\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\n\n\n\n\n0\n65\nFemale\n0.7\n0.1\n187\n16\n18\n6.8\n3.3\n0.90\n\n\n1\n62\nMale\n10.9\n5.5\n699\n64\n100\n7.5\n3.2\n0.74\n\n\n2\n62\nMale\n7.3\n4.1\n490\n60\n68\n7.0\n3.3\n0.89\n\n\n3\n58\nMale\n1.0\n0.4\n182\n14\n20\n6.8\n3.4\n1.00\n\n\n4\n72\nMale\n3.9\n2.0\n195\n27\n59\n7.3\n2.4\n0.40\n\n\n\n\n\n\n\n\n# predict model on new_data\npredictions = predict_model(lr, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nAge\nGender\nTotal_Bilirubin\nDirect_Bilirubin\nAlkaline_Phosphotase\nAlamine_Aminotransferase\nAspartate_Aminotransferase\nTotal_Protiens\nAlbumin\nAlbumin_and_Globulin_Ratio\nprediction_label\nprediction_score\n\n\n\n\n0\n65\nFemale\n0.7\n0.1\n187\n16\n18\n6.8\n3.3\n0.90\n1\n0.6171\n\n\n1\n62\nMale\n10.9\n5.5\n699\n64\n100\n7.5\n3.2\n0.74\n1\n0.9816\n\n\n2\n62\nMale\n7.3\n4.1\n490\n60\n68\n7.0\n3.3\n0.89\n1\n0.9507\n\n\n3\n58\nMale\n1.0\n0.4\n182\n14\n20\n6.8\n3.4\n1.00\n1\n0.6395\n\n\n4\n72\nMale\n3.9\n2.0\n195\n27\n59\n7.3\n2.4\n0.40\n1\n0.9249"
  },
  {
    "objectID": "projects/Liver_Disorder/04_Liver Disorder.html#save-model",
    "href": "projects/Liver_Disorder/04_Liver Disorder.html#save-model",
    "title": "Machine Learning-Based Liver Disorder Prediction: Towards Early Detection and Diagnosis",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(lr, '../models/liver_disorder')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('label_encoding',\n                  TransformerWrapperWithInverse(exclude=None, include=None,\n                                                transformer=LabelEncoder())),\n                 ('numerical_imputer',\n                  TransformerWrapper(exclude=None,\n                                     include=['Age', 'Total_Bilirubin',\n                                              'Direct_Bilirubin',\n                                              'Alkaline_Phosphotase',\n                                              'Alamine_Aminotransferase',\n                                              'Asp...\n                                                                          'data_type': dtype('O'),\n                                                                          'mapping': Female    0\n Male      1\n NaN      -1\n dtype: int64}],\n                                                                return_df=True,\n                                                                verbose=0))),\n                 ('trained_model',\n                  LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                     fit_intercept=True, intercept_scaling=1,\n                                     l1_ratio=None, max_iter=1000,\n                                     multi_class='auto', n_jobs=None,\n                                     penalty='l2', random_state=123,\n                                     solver='lbfgs', tol=0.0001, verbose=0,\n                                     warm_start=False))],\n          verbose=False),\n '../models/liver_disorder.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/liver_disorder')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['Age', 'Total_Bilirubin',\n                                             'Direct_Bilirubin',\n                                             'Alkaline_Phosphotase',\n                                             'Alamine_Aminotransferase',\n                                             'Asp...\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('trained_model',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=1000,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=123,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['Age', 'Total_Bilirubin',\n                                             'Direct_Bilirubin',\n                                             'Alkaline_Phosphotase',\n                                             'Alamine_Aminotransferase',\n                                             'Asp...\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('trained_model',\n                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n                                    fit_intercept=True, intercept_scaling=1,\n                                    l1_ratio=None, max_iter=1000,\n                                    multi_class='auto', n_jobs=None,\n                                    penalty='l2', random_state=123,\n                                    solver='lbfgs', tol=0.0001, verbose=0,\n                                    warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['Age', 'Total_Bilirubin', 'Direct_Bilirubin',\n                            'Alkaline_Phosphotase', 'Alamine_Aminotransferase',\n                            'Aspartate_Aminotransferase', 'Total_Protiens',\n                            'Albumin', 'Albumin_and_Globulin_Ratio'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['Gender'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': Female    0\nMale      1\nNaN      -1\ndtype: int64}])LogisticRegressionLogisticRegression(max_iter=1000, random_state=123)"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html",
    "href": "projects/Heart_Disease/05_Heart Diseases.html",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "",
    "text": "Heart disease is a leading cause of mortality and morbidity worldwide, making its timely detection and accurate prediction crucial for effective management and prevention. However, the diagnosis of heart disease can be challenging due to its complex nature and the presence of various risk factors. Traditional diagnostic methods may lack the precision and efficiency needed for early detection and prediction. Therefore, there is a pressing need to develop advanced machine learning models that can analyze patient data and accurately predict the likelihood of heart disease, enabling proactive healthcare interventions."
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#problem",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#problem",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "",
    "text": "Heart disease is a leading cause of mortality and morbidity worldwide, making its timely detection and accurate prediction crucial for effective management and prevention. However, the diagnosis of heart disease can be challenging due to its complex nature and the presence of various risk factors. Traditional diagnostic methods may lack the precision and efficiency needed for early detection and prediction. Therefore, there is a pressing need to develop advanced machine learning models that can analyze patient data and accurately predict the likelihood of heart disease, enabling proactive healthcare interventions."
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#objective",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#objective",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Objective",
    "text": "Objective\nThe objective of this study is to utilize machine learning techniques for the analysis and prediction of heart disease.\nData Source: https://www.kaggle.com/datasets/johnsmith88/heart-disease-dataset\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv(\"../data/heart.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#exploring-data",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#exploring-data",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(303, 14)\n\n\n\n# dtypes \ndata.dtypes\n\nage           int64\nsex           int64\ncp            int64\ntrestbps      int64\nchol          int64\nfbs           int64\nrestecg       int64\nthalach       int64\nexang         int64\noldpeak     float64\nslope         int64\nca            int64\nthal          int64\ntarget        int64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 303 entries, 0 to 302\nData columns (total 14 columns):\n #   Column    Non-Null Count  Dtype  \n---  ------    --------------  -----  \n 0   age       303 non-null    int64  \n 1   sex       303 non-null    int64  \n 2   cp        303 non-null    int64  \n 3   trestbps  303 non-null    int64  \n 4   chol      303 non-null    int64  \n 5   fbs       303 non-null    int64  \n 6   restecg   303 non-null    int64  \n 7   thalach   303 non-null    int64  \n 8   exang     303 non-null    int64  \n 9   oldpeak   303 non-null    float64\n 10  slope     303 non-null    int64  \n 11  ca        303 non-null    int64  \n 12  thal      303 non-null    int64  \n 13  target    303 non-null    int64  \ndtypes: float64(1), int64(13)\nmemory usage: 33.3 KB\n\n\n\ndata[['sex','target']] = data[['sex','target']].astype(object)\n\n\n# check missing data \ndata.isnull().sum() \n\nage         0\nsex         0\ncp          0\ntrestbps    0\nchol        0\nfbs         0\nrestecg     0\nthalach     0\nexang       0\noldpeak     0\nslope       0\nca          0\nthal        0\ntarget      0\ndtype: int64"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#descriptive-statistics",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#descriptive-statistics",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# select numeric data \nnum_cols = data.select_dtypes(exclude = 'object')\nnum_cols.head() \n\n\n\n\n\n\n\n\nage\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n0\n63\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n\n\n1\n37\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n\n\n2\n41\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n\n\n3\n56\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n\n\n4\n57\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n\n\n\n\n\n\n\n\n# summary statistics of numerical variables \nrp.summarize(num_cols[['age', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang',\n       'oldpeak', 'slope', 'ca', 'thal']])\n\n\n\n\n\n\n\n\nName\nN\nMean\nMedian\nVariance\nSD\nSE\n95% Conf. Interval\n\n\n\n\n0\nage\n303\n54.3663\n55.0\n82.4846\n9.0821\n0.5218\nage\n\n\n1\ncp\n303\n0.967\n1.0\n1.0651\n1.0321\n0.0593\ncp\n\n\n2\ntrestbps\n303\n131.6238\n130.0\n307.5865\n17.5381\n1.0075\ntrestbps\n\n\n3\nchol\n303\n246.264\n240.0\n2686.4267\n51.8308\n2.9776\nchol\n\n\n4\nfbs\n303\n0.1485\n0.0\n0.1269\n0.3562\n0.0205\nfbs\n\n\n5\nrestecg\n303\n0.5281\n1.0\n0.2765\n0.5259\n0.0302\nrestecg\n\n\n6\nthalach\n303\n149.6469\n153.0\n524.6464\n22.9052\n1.3159\nthalach\n\n\n7\nexang\n303\n0.3267\n0.0\n0.2207\n0.4698\n0.027\nexang\n\n\n8\noldpeak\n303\n1.0396\n0.8\n1.3481\n1.1611\n0.0667\noldpeak\n\n\n9\nslope\n303\n1.3993\n1.0\n0.3797\n0.6162\n0.0354\nslope\n\n\n10\nca\n303\n0.7294\n0.0\n1.0457\n1.0226\n0.0587\nca\n\n\n11\nthal\n303\n2.3135\n2.0\n0.3749\n0.6123\n0.0352\nthal\n\n\n\n\n\n\n\n\n# select categorical data \ncat_cols = data.select_dtypes(include = 'object')\ncat_cols.head() \n\n\n\n\n\n\n\n\nsex\ntarget\n\n\n\n\n0\n1\n1\n\n\n1\n1\n1\n\n\n2\n0\n1\n\n\n3\n1\n1\n\n\n4\n0\n1\n\n\n\n\n\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(cat_cols[['sex', 'target']])\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:225: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\nsex\n1\n207\n68.32\n\n\n1\n\n0\n96\n31.68\n\n\n2\ntarget\n1\n165\n54.46\n\n\n3\n\n0\n138\n45.54"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#correlations-between-variables",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#correlations-between-variables",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \nnum_cols.corr(method='pearson')\n\n\n\n\n\n\n\n\nage\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\nage\n1.000000\n-0.068653\n0.279351\n0.213678\n0.121308\n-0.116211\n-0.398522\n0.096801\n0.210013\n-0.168814\n0.276326\n0.068001\n\n\ncp\n-0.068653\n1.000000\n0.047608\n-0.076904\n0.094444\n0.044421\n0.295762\n-0.394280\n-0.149230\n0.119717\n-0.181053\n-0.161736\n\n\ntrestbps\n0.279351\n0.047608\n1.000000\n0.123174\n0.177531\n-0.114103\n-0.046698\n0.067616\n0.193216\n-0.121475\n0.101389\n0.062210\n\n\nchol\n0.213678\n-0.076904\n0.123174\n1.000000\n0.013294\n-0.151040\n-0.009940\n0.067023\n0.053952\n-0.004038\n0.070511\n0.098803\n\n\nfbs\n0.121308\n0.094444\n0.177531\n0.013294\n1.000000\n-0.084189\n-0.008567\n0.025665\n0.005747\n-0.059894\n0.137979\n-0.032019\n\n\nrestecg\n-0.116211\n0.044421\n-0.114103\n-0.151040\n-0.084189\n1.000000\n0.044123\n-0.070733\n-0.058770\n0.093045\n-0.072042\n-0.011981\n\n\nthalach\n-0.398522\n0.295762\n-0.046698\n-0.009940\n-0.008567\n0.044123\n1.000000\n-0.378812\n-0.344187\n0.386784\n-0.213177\n-0.096439\n\n\nexang\n0.096801\n-0.394280\n0.067616\n0.067023\n0.025665\n-0.070733\n-0.378812\n1.000000\n0.288223\n-0.257748\n0.115739\n0.206754\n\n\noldpeak\n0.210013\n-0.149230\n0.193216\n0.053952\n0.005747\n-0.058770\n-0.344187\n0.288223\n1.000000\n-0.577537\n0.222682\n0.210244\n\n\nslope\n-0.168814\n0.119717\n-0.121475\n-0.004038\n-0.059894\n0.093045\n0.386784\n-0.257748\n-0.577537\n1.000000\n-0.080155\n-0.104764\n\n\nca\n0.276326\n-0.181053\n0.101389\n0.070511\n0.137979\n-0.072042\n-0.213177\n0.115739\n0.222682\n-0.080155\n1.000000\n0.151832\n\n\nthal\n0.068001\n-0.161736\n0.062210\n0.098803\n-0.032019\n-0.011981\n-0.096439\n0.206754\n0.210244\n-0.104764\n0.151832\n1.000000"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#skewness",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#skewness",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \nnum_cols.skew() \n\nage        -0.202463\ncp          0.484732\ntrestbps    0.713768\nchol        1.143401\nfbs         1.986652\nrestecg     0.162522\nthalach    -0.537410\nexang       0.742532\noldpeak     1.269720\nslope      -0.508316\nca          1.310422\nthal       -0.476722\ndtype: float64"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#data-visualizations",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#data-visualizations",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(4,3))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(4,3))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(20,6))\ncorr = num_cols.corr() \nsns.heatmap(corr, annot=True)\nplt.show()"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#setup",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#setup",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n1\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n\n\n\n\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.classification import *\nsetup(data, target = 'target', session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\ntarget\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(303, 14)\n\n\n4\nTransformed data shape\n(303, 14)\n\n\n5\nTransformed train set shape\n(212, 14)\n\n\n6\nTransformed test set shape\n(91, 14)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n12\n\n\n9\nCategorical features\n1\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmean\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nFold Generator\nStratifiedKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nclf-default-name\n\n\n22\nUSI\nb6bb\n\n\n\n\n\n&lt;pycaret.classification.oop.ClassificationExperiment at 0x203efa84a00&gt;"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#compare-models",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#compare-models",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\nnb\nNaive Bayes\n0.8201\n0.9212\n0.8576\n0.8186\n0.8303\n0.6358\n0.6510\n0.3820\n\n\nridge\nRidge Classifier\n0.8201\n0.0000\n0.8841\n0.8069\n0.8359\n0.6346\n0.6558\n0.3800\n\n\nrf\nRandom Forest Classifier\n0.8201\n0.8924\n0.8583\n0.8236\n0.8335\n0.6349\n0.6474\n0.4040\n\n\nlda\nLinear Discriminant Analysis\n0.8201\n0.9162\n0.8841\n0.8069\n0.8359\n0.6346\n0.6558\n0.3860\n\n\nlr\nLogistic Regression\n0.8154\n0.9189\n0.8750\n0.8061\n0.8310\n0.6245\n0.6463\n0.8670\n\n\net\nExtra Trees Classifier\n0.8108\n0.9003\n0.8508\n0.8150\n0.8240\n0.6180\n0.6342\n0.4090\n\n\nqda\nQuadratic Discriminant Analysis\n0.8013\n0.9013\n0.8152\n0.8273\n0.8112\n0.5986\n0.6111\n0.3820\n\n\ngbc\nGradient Boosting Classifier\n0.7972\n0.8802\n0.8152\n0.8122\n0.8084\n0.5900\n0.5992\n0.3940\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.7872\n0.8706\n0.8152\n0.8199\n0.8024\n0.5683\n0.5907\n0.4850\n\n\ndt\nDecision Tree Classifier\n0.7545\n0.7546\n0.7659\n0.7890\n0.7680\n0.5068\n0.5231\n0.3830\n\n\nada\nAda Boost Classifier\n0.7500\n0.8122\n0.7712\n0.7812\n0.7650\n0.4954\n0.5107\n0.3960\n\n\nknn\nK Neighbors Classifier\n0.6370\n0.6556\n0.7197\n0.6572\n0.6818\n0.2587\n0.2650\n0.4010\n\n\nsvm\nSVM - Linear Kernel\n0.5472\n0.0000\n0.6917\n0.5079\n0.5197\n0.1002\n0.1386\n0.3840\n\n\ndummy\nDummy Classifier\n0.5424\n0.5000\n1.0000\n0.5424\n0.7031\n0.0000\n0.0000\n0.3820"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#create-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#create-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \nnb = create_model('nb')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9091\n0.9917\n1.0000\n0.8571\n0.9231\n0.8136\n0.8281\n\n\n1\n0.8636\n0.9583\n1.0000\n0.8000\n0.8889\n0.7179\n0.7483\n\n\n2\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n3\n0.8095\n0.9444\n1.0000\n0.7500\n0.8571\n0.5882\n0.6455\n\n\n4\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n5\n0.8571\n0.9455\n0.8182\n0.9000\n0.8571\n0.7149\n0.7182\n\n\n6\n0.9048\n0.9000\n1.0000\n0.8462\n0.9167\n0.8073\n0.8228\n\n\n7\n0.6667\n0.8273\n0.7273\n0.6667\n0.6957\n0.3288\n0.3303\n\n\n8\n0.8571\n0.9364\n0.9091\n0.8333\n0.8696\n0.7123\n0.7156\n\n\n9\n0.6190\n0.7455\n0.4545\n0.7143\n0.5556\n0.2500\n0.2697\n\n\nMean\n0.8201\n0.9212\n0.8576\n0.8186\n0.8303\n0.6358\n0.6510\n\n\nStd\n0.0930\n0.0742\n0.1631\n0.0803\n0.1092\n0.1836\n0.1832\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(nb)\n\nGaussianNB(priors=None, var_smoothing=1e-09)"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#tune-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#tune-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of rf\ntuned_nb = tune_model(nb)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9091\n0.9917\n1.0000\n0.8571\n0.9231\n0.8136\n0.8281\n\n\n1\n0.8636\n0.9583\n1.0000\n0.8000\n0.8889\n0.7179\n0.7483\n\n\n2\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n3\n0.8095\n0.9444\n1.0000\n0.7500\n0.8571\n0.5882\n0.6455\n\n\n4\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n5\n0.8571\n0.9455\n0.8182\n0.9000\n0.8571\n0.7149\n0.7182\n\n\n6\n0.9048\n0.9000\n1.0000\n0.8462\n0.9167\n0.8073\n0.8228\n\n\n7\n0.6667\n0.8273\n0.7273\n0.6667\n0.6957\n0.3288\n0.3303\n\n\n8\n0.8571\n0.9364\n0.9091\n0.8333\n0.8696\n0.7123\n0.7156\n\n\n9\n0.6190\n0.7455\n0.4545\n0.7143\n0.5556\n0.2500\n0.2697\n\n\nMean\n0.8201\n0.9212\n0.8576\n0.8186\n0.8303\n0.6358\n0.6510\n\n\nStd\n0.0930\n0.0742\n0.1631\n0.0803\n0.1092\n0.1836\n0.1832\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_nb, tuner = tune_model(nb, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9091\n0.9917\n1.0000\n0.8571\n0.9231\n0.8136\n0.8281\n\n\n1\n0.8636\n0.9583\n1.0000\n0.8000\n0.8889\n0.7179\n0.7483\n\n\n2\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n3\n0.8095\n0.9444\n1.0000\n0.7500\n0.8571\n0.5882\n0.6455\n\n\n4\n0.8571\n0.9815\n0.8333\n0.9091\n0.8696\n0.7123\n0.7156\n\n\n5\n0.8571\n0.9455\n0.8182\n0.9000\n0.8571\n0.7149\n0.7182\n\n\n6\n0.9048\n0.9000\n1.0000\n0.8462\n0.9167\n0.8073\n0.8228\n\n\n7\n0.6667\n0.8273\n0.7273\n0.6667\n0.6957\n0.3288\n0.3303\n\n\n8\n0.8571\n0.9364\n0.9091\n0.8333\n0.8696\n0.7123\n0.7156\n\n\n9\n0.6190\n0.7455\n0.4545\n0.7143\n0.5556\n0.2500\n0.2697\n\n\nMean\n0.8201\n0.9212\n0.8576\n0.8186\n0.8303\n0.6358\n0.6510\n\n\nStd\n0.0930\n0.0742\n0.1631\n0.0803\n0.1092\n0.1836\n0.1832\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_nb\n\nGaussianNB(priors=None, var_smoothing=1e-09)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GaussianNBGaussianNB(priors=None, var_smoothing=1e-09)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(exclude=None,\n                                                                 include=['age',\n                                                                          'cp',\n                                                                          'trestbps',\n                                                                          'chol',\n                                                                          'fbs',\n                                                                          'restecg',\n                                                                          'thalach',\n                                                                          'exang',\n                                                                          'oldpeak',\n                                                                          'slope',\n                                                                          'ca',\n                                                                          'thal...\n                                      verbose=False),\n                   n_iter=10, n_jobs=-1,\n                   param_distributions={'actual_estimator__var_smoothing': [1e-09,\n                                                                            2e-09,\n                                                                            5e-09,\n                                                                            8e-09,\n                                                                            9e-09,\n                                                                            1e-07,\n                                                                            2e-07,\n                                                                            3e-07,\n                                                                            5e-07,\n                                                                            7e-07,\n                                                                            9e-07,\n                                                                            1e-05,\n                                                                            0.001,\n                                                                            0.002,\n                                                                            0.003,\n                                                                            0.004,\n                                                                            0.005,\n                                                                            0.007,\n                                                                            0.009,\n                                                                            0.004,\n                                                                            0.005,\n                                                                            0.006,\n                                                                            0.007,\n                                                                            0.008,\n                                                                            0.009,\n                                                                            0.01,\n                                                                            0.1,\n                                                                            1]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(exclude=None,\n                                                                 include=['age',\n                                                                          'cp',\n                                                                          'trestbps',\n                                                                          'chol',\n                                                                          'fbs',\n                                                                          'restecg',\n                                                                          'thalach',\n                                                                          'exang',\n                                                                          'oldpeak',\n                                                                          'slope',\n                                                                          'ca',\n                                                                          'thal...\n                                      verbose=False),\n                   n_iter=10, n_jobs=-1,\n                   param_distributions={'actual_estimator__var_smoothing': [1e-09,\n                                                                            2e-09,\n                                                                            5e-09,\n                                                                            8e-09,\n                                                                            9e-09,\n                                                                            1e-07,\n                                                                            2e-07,\n                                                                            3e-07,\n                                                                            5e-07,\n                                                                            7e-07,\n                                                                            9e-07,\n                                                                            1e-05,\n                                                                            0.001,\n                                                                            0.002,\n                                                                            0.003,\n                                                                            0.004,\n                                                                            0.005,\n                                                                            0.007,\n                                                                            0.009,\n                                                                            0.004,\n                                                                            0.005,\n                                                                            0.006,\n                                                                            0.007,\n                                                                            0.008,\n                                                                            0.009,\n                                                                            0.01,\n                                                                            0.1,\n                                                                            1]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['age', 'cp', 'trestbps', 'chol',\n                                             'fbs', 'restecg', 'thalach',\n                                             'exang', 'oldpeak', 'slope', 'ca',\n                                             'thal'],\n                                    transformer=SimpleImputer(add_indicator=False,\n                                                              copy=True,\n                                                              fill_value=None,\n                                                              keep_empty_features=False,\n                                                              missing_...\n                 TransformerWrapper(exclude=None, include=['sex'],\n                                    transformer=OrdinalEncoder(cols=['sex'],\n                                                               drop_invariant=False,\n                                                               handle_missing='return_nan',\n                                                               handle_unknown='value',\n                                                               mapping=[{'col': 'sex',\n                                                                         'data_type': dtype('float64'),\n                                                                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)numerical_imputer: TransformerWrapperTransformerWrapper(include=['age', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                            'thalach', 'exang', 'oldpeak', 'slope', 'ca',\n                            'thal'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['sex'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['sex'],\n                   transformer=OrdinalEncoder(cols=['sex'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'sex',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])GaussianNBGaussianNB()"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#analyze-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#analyze-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# plot confusion matrix\nplot_model(nb, plot = 'confusion_matrix')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot AUC\nplot_model(nb, plot = 'auc')\n\n\n\n\nTypeError: unsupported operand type(s) for -: 'int' and 'Categorical'\n\n\n&lt;Figure size 800x550 with 0 Axes&gt;\n\n\n\n# plot class report\nplot_model(nb, plot = 'class_report')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(nb, plot = 'feature')\n\nTypeError: Feature Importance and RFE plots not available for estimators that doesnt support coef_ or feature_importances_ attribute."
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#evaluate-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#evaluate-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(nb)"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#finalize-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#finalize-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(nb)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['age', 'cp', 'trestbps', 'chol',\n                                             'fbs', 'restecg', 'thalach',\n                                             'exang', 'oldpeak', 'slope', 'ca',\n                                             'thal'],\n                                    transformer=SimpleImputer(add_indicator=False,\n                                                              copy=True,\n                                                              fill_value=None,\n                                                              keep_empty_features=False,\n                                                              missing_...\n                 TransformerWrapper(exclude=None, include=['sex'],\n                                    transformer=OrdinalEncoder(cols=['sex'],\n                                                               drop_invariant=False,\n                                                               handle_missing='return_nan',\n                                                               handle_unknown='value',\n                                                               mapping=[{'col': 'sex',\n                                                                         'data_type': dtype('float64'),\n                                                                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['age', 'cp', 'trestbps', 'chol',\n                                             'fbs', 'restecg', 'thalach',\n                                             'exang', 'oldpeak', 'slope', 'ca',\n                                             'thal'],\n                                    transformer=SimpleImputer(add_indicator=False,\n                                                              copy=True,\n                                                              fill_value=None,\n                                                              keep_empty_features=False,\n                                                              missing_...\n                 TransformerWrapper(exclude=None, include=['sex'],\n                                    transformer=OrdinalEncoder(cols=['sex'],\n                                                               drop_invariant=False,\n                                                               handle_missing='return_nan',\n                                                               handle_unknown='value',\n                                                               mapping=[{'col': 'sex',\n                                                                         'data_type': dtype('float64'),\n                                                                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('actual_estimator',\n                 GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['age', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                            'thalach', 'exang', 'oldpeak', 'slope', 'ca',\n                            'thal'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['sex'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['sex'],\n                   transformer=OrdinalEncoder(cols=['sex'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'sex',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])GaussianNBGaussianNB()"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#prediction",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#prediction",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(nb)\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\nNaive Bayes\n0.8132\n0.8741\n0.8600\n0.8113\n0.8350\n0.6202\n0.6216\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\ntarget\nprediction_label\nprediction_score\n\n\n\n\n204\n62\n0\n0\n160\n164\n0\n0\n145\n0\n6.2\n0\n3\n3\n0\n0\n1.0000\n\n\n293\n67\n1\n2\n152\n212\n0\n0\n150\n0\n0.8\n1\n0\n3\n0\n1\n0.6576\n\n\n84\n42\n0\n0\n102\n265\n0\n0\n122\n0\n0.6\n1\n0\n2\n1\n1\n0.9235\n\n\n141\n43\n1\n0\n115\n303\n0\n1\n181\n0\n1.2\n1\n0\n2\n1\n1\n0.9485\n\n\n290\n61\n1\n0\n148\n203\n0\n1\n161\n0\n0.0\n2\n1\n3\n0\n1\n0.7094\n\n\n\n\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('target', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n\n\n\n\n\n\n\n\n# predict model on new_data\npredictions = predict_model(nb, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nage\nsex\ncp\ntrestbps\nchol\nfbs\nrestecg\nthalach\nexang\noldpeak\nslope\nca\nthal\nprediction_label\nprediction_score\n\n\n\n\n0\n63\n1\n3\n145\n233\n1\n0\n150\n0\n2.3\n0\n0\n1\n0\n0.8095\n\n\n1\n37\n1\n2\n130\n250\n0\n1\n187\n0\n3.5\n0\n0\n2\n0\n0.9799\n\n\n2\n41\n0\n1\n130\n204\n0\n0\n172\n0\n1.4\n2\n0\n2\n1\n0.9980\n\n\n3\n56\n1\n1\n120\n236\n0\n1\n178\n0\n0.8\n2\n0\n2\n1\n0.9932\n\n\n4\n57\n0\n0\n120\n354\n0\n1\n163\n1\n0.6\n2\n0\n2\n1\n0.9513"
  },
  {
    "objectID": "projects/Heart_Disease/05_Heart Diseases.html#save-model",
    "href": "projects/Heart_Disease/05_Heart Diseases.html#save-model",
    "title": "RSV Vaccine Impact Monte Carlo Simulation",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(nb, '../models/heart_diseases')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(exclude=None,\n                                     include=['age', 'cp', 'trestbps', 'chol',\n                                              'fbs', 'restecg', 'thalach',\n                                              'exang', 'oldpeak', 'slope', 'ca',\n                                              'thal'],\n                                     transformer=SimpleImputer(add_indicator=False,\n                                                               copy=True,\n                                                               fill_value=None,\n                                                               keep_empty_features=False,\n                                                               missing_...\n                  TransformerWrapper(exclude=None, include=['sex'],\n                                     transformer=OrdinalEncoder(cols=['sex'],\n                                                                drop_invariant=False,\n                                                                handle_missing='return_nan',\n                                                                handle_unknown='value',\n                                                                mapping=[{'col': 'sex',\n                                                                          'data_type': dtype('float64'),\n                                                                          'mapping': 0.0    0\n 1.0    1\n NaN   -1\n dtype: int64}],\n                                                                return_df=True,\n                                                                verbose=0))),\n                 ('trained_model',\n                  GaussianNB(priors=None, var_smoothing=1e-09))],\n          verbose=False),\n '../models/heart_diseases.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/heart_diseases')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['age', 'cp', 'trestbps', 'chol',\n                                             'fbs', 'restecg', 'thalach',\n                                             'exang', 'oldpeak', 'slope', 'ca',\n                                             'thal'],\n                                    transformer=SimpleImputer(add_indicator=False,\n                                                              copy=True,\n                                                              fill_value=None,\n                                                              keep_empty_features=False,\n                                                              missing_...\n                 TransformerWrapper(exclude=None, include=['sex'],\n                                    transformer=OrdinalEncoder(cols=['sex'],\n                                                               drop_invariant=False,\n                                                               handle_missing='return_nan',\n                                                               handle_unknown='value',\n                                                               mapping=[{'col': 'sex',\n                                                                         'data_type': dtype('float64'),\n                                                                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('trained_model',\n                 GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['age', 'cp', 'trestbps', 'chol',\n                                             'fbs', 'restecg', 'thalach',\n                                             'exang', 'oldpeak', 'slope', 'ca',\n                                             'thal'],\n                                    transformer=SimpleImputer(add_indicator=False,\n                                                              copy=True,\n                                                              fill_value=None,\n                                                              keep_empty_features=False,\n                                                              missing_...\n                 TransformerWrapper(exclude=None, include=['sex'],\n                                    transformer=OrdinalEncoder(cols=['sex'],\n                                                               drop_invariant=False,\n                                                               handle_missing='return_nan',\n                                                               handle_unknown='value',\n                                                               mapping=[{'col': 'sex',\n                                                                         'data_type': dtype('float64'),\n                                                                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                                               return_df=True,\n                                                               verbose=0))),\n                ('trained_model',\n                 GaussianNB(priors=None, var_smoothing=1e-09))],\n         verbose=False)numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['age', 'cp', 'trestbps', 'chol', 'fbs', 'restecg',\n                            'thalach', 'exang', 'oldpeak', 'slope', 'ca',\n                            'thal'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['sex'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['sex'],\n                   transformer=OrdinalEncoder(cols=['sex'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'sex',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['sex'], handle_missing='return_nan',\n               mapping=[{'col': 'sex', 'data_type': dtype('float64'),\n                         'mapping': 0.0    0\n1.0    1\nNaN   -1\ndtype: int64}])GaussianNBGaussianNB()"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "",
    "text": "Chronic kidney disease (CKD) is a prevalent and potentially life-threatening condition affecting millions of people worldwide. Early detection and accurate prediction of CKD can significantly improve patient outcomes by enabling timely interventions and personalized treatment plans. However, traditional diagnostic methods often rely on subjective assessments and are limited in their ability to predict CKD progression. Therefore, there is a pressing need for a machine learning-based approach that can effectively predict the development and progression of CKD using relevant risk factors and patient data."
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#problem",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#problem",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "",
    "text": "Chronic kidney disease (CKD) is a prevalent and potentially life-threatening condition affecting millions of people worldwide. Early detection and accurate prediction of CKD can significantly improve patient outcomes by enabling timely interventions and personalized treatment plans. However, traditional diagnostic methods often rely on subjective assessments and are limited in their ability to predict CKD progression. Therefore, there is a pressing need for a machine learning-based approach that can effectively predict the development and progression of CKD using relevant risk factors and patient data."
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#objective-s",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#objective-s",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Objective (s)",
    "text": "Objective (s)\nDevelop an accurate machine learning model for predicting chronic kidney disease (CKD) using relevant risk factors and patient data, improving early detection and personalized treatment plans.\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv(\"../data/kidney_disease.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nrbc\npc\npcc\nba\n...\npcv\nwc\nrc\nhtn\ndm\ncad\nappet\npe\nane\nclassification\n\n\n\n\n0\n0\n48.0\n80.0\n1.020\n1.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n44\n7800\n5.2\nyes\nyes\nno\ngood\nno\nno\nckd\n\n\n1\n1\n7.0\n50.0\n1.020\n4.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n38\n6000\nNaN\nno\nno\nno\ngood\nno\nno\nckd\n\n\n2\n2\n62.0\n80.0\n1.010\n2.0\n3.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n31\n7500\nNaN\nno\nyes\nno\npoor\nno\nyes\nckd\n\n\n3\n3\n48.0\n70.0\n1.005\n4.0\n0.0\nnormal\nabnormal\npresent\nnotpresent\n...\n32\n6700\n3.9\nyes\nno\nno\npoor\nyes\nyes\nckd\n\n\n4\n4\n51.0\n80.0\n1.010\n2.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n35\n7300\n4.6\nno\nno\nno\ngood\nno\nno\nckd\n\n\n\n\n5 rows × 26 columns"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#exploring-data",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#exploring-data",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(400, 26)\n\n\n\n# dtypes \ndata.dtypes\n\nid                  int64\nage               float64\nbp                float64\nsg                float64\nal                float64\nsu                float64\nrbc                object\npc                 object\npcc                object\nba                 object\nbgr               float64\nbu                float64\nsc                float64\nsod               float64\npot               float64\nhemo              float64\npcv                object\nwc                 object\nrc                 object\nhtn                object\ndm                 object\ncad                object\nappet              object\npe                 object\nane                object\nclassification     object\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 400 entries, 0 to 399\nData columns (total 26 columns):\n #   Column          Non-Null Count  Dtype  \n---  ------          --------------  -----  \n 0   id              400 non-null    int64  \n 1   age             391 non-null    float64\n 2   bp              388 non-null    float64\n 3   sg              353 non-null    float64\n 4   al              354 non-null    float64\n 5   su              351 non-null    float64\n 6   rbc             248 non-null    object \n 7   pc              335 non-null    object \n 8   pcc             396 non-null    object \n 9   ba              396 non-null    object \n 10  bgr             356 non-null    float64\n 11  bu              381 non-null    float64\n 12  sc              383 non-null    float64\n 13  sod             313 non-null    float64\n 14  pot             312 non-null    float64\n 15  hemo            348 non-null    float64\n 16  pcv             330 non-null    object \n 17  wc              295 non-null    object \n 18  rc              270 non-null    object \n 19  htn             398 non-null    object \n 20  dm              398 non-null    object \n 21  cad             398 non-null    object \n 22  appet           399 non-null    object \n 23  pe              399 non-null    object \n 24  ane             399 non-null    object \n 25  classification  400 non-null    object \ndtypes: float64(11), int64(1), object(14)\nmemory usage: 81.4+ KB\n\n\n\n# check missing data \ndata.isnull().sum() \n\nid                  0\nage                 9\nbp                 12\nsg                 47\nal                 46\nsu                 49\nrbc               152\npc                 65\npcc                 4\nba                  4\nbgr                44\nbu                 19\nsc                 17\nsod                87\npot                88\nhemo               52\npcv                70\nwc                105\nrc                130\nhtn                 2\ndm                  2\ncad                 2\nappet               1\npe                  1\nane                 1\nclassification      0\ndtype: int64"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#descriptive-statistics",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#descriptive-statistics",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# select numeric data \nnum_cols = data.select_dtypes(exclude = 'object')\nnum_cols.head() \n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nbgr\nbu\nsc\nsod\npot\nhemo\n\n\n\n\n0\n0\n48.0\n80.0\n1.020\n1.0\n0.0\n121.0\n36.0\n1.2\nNaN\nNaN\n15.4\n\n\n1\n1\n7.0\n50.0\n1.020\n4.0\n0.0\nNaN\n18.0\n0.8\nNaN\nNaN\n11.3\n\n\n2\n2\n62.0\n80.0\n1.010\n2.0\n3.0\n423.0\n53.0\n1.8\nNaN\nNaN\n9.6\n\n\n3\n3\n48.0\n70.0\n1.005\n4.0\n0.0\n117.0\n56.0\n3.8\n111.0\n2.5\n11.2\n\n\n4\n4\n51.0\n80.0\n1.010\n2.0\n0.0\n106.0\n26.0\n1.4\nNaN\nNaN\n11.6\n\n\n\n\n\n\n\n\n# summary statistics of numerical variables \nrp.summary_cont(num_cols[['age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu', 'sc', 'sod', 'pot',\n       'hemo']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nage\n391.0\n51.4834\n17.1697\n0.8683\n49.7762\n53.1905\n\n\n1\nbp\n388.0\n76.4691\n13.6836\n0.6947\n75.1033\n77.8349\n\n\n2\nsg\n353.0\n1.0174\n0.0057\n0.0003\n1.0168\n1.0180\n\n\n3\nal\n354.0\n1.0169\n1.3527\n0.0719\n0.8756\n1.1583\n\n\n4\nsu\n351.0\n0.4501\n1.0992\n0.0587\n0.3348\n0.5655\n\n\n5\nbgr\n356.0\n148.0365\n79.2817\n4.2019\n139.7727\n156.3003\n\n\n6\nbu\n381.0\n57.4257\n50.5030\n2.5873\n52.3384\n62.5130\n\n\n7\nsc\n383.0\n3.0725\n5.7411\n0.2934\n2.4957\n3.6493\n\n\n8\nsod\n313.0\n137.5288\n10.4088\n0.5883\n136.3711\n138.6864\n\n\n9\npot\n312.0\n4.6272\n3.1939\n0.1808\n4.2715\n4.9830\n\n\n10\nhemo\n348.0\n12.5264\n2.9126\n0.1561\n12.2194\n12.8335\n\n\n\n\n\n\n\n\n# select categorical data \ncat_cols = data.select_dtypes(include = 'object')\ncat_cols.head() \n\n\n\n\n\n\n\n\nrbc\npc\npcc\nba\npcv\nwc\nrc\nhtn\ndm\ncad\nappet\npe\nane\nclassification\n\n\n\n\n0\nNaN\nnormal\nnotpresent\nnotpresent\n44\n7800\n5.2\nyes\nyes\nno\ngood\nno\nno\nckd\n\n\n1\nNaN\nnormal\nnotpresent\nnotpresent\n38\n6000\nNaN\nno\nno\nno\ngood\nno\nno\nckd\n\n\n2\nnormal\nnormal\nnotpresent\nnotpresent\n31\n7500\nNaN\nno\nyes\nno\npoor\nno\nyes\nckd\n\n\n3\nnormal\nabnormal\npresent\nnotpresent\n32\n6700\n3.9\nyes\nno\nno\npoor\nyes\nyes\nckd\n\n\n4\nnormal\nnormal\nnotpresent\nnotpresent\n35\n7300\n4.6\nno\nno\nno\ngood\nno\nno\nckd\n\n\n\n\n\n\n\n\ncat_cols.columns\n\nIndex(['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad',\n       'appet', 'pe', 'ane', 'classification'],\n      dtype='object')\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(cat_cols[['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn', 'dm', 'cad',\n       'appet', 'pe', 'ane', 'classification']])\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:225: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\nrbc\nnormal\n201\n81.05\n\n\n1\n\nabnormal\n47\n18.95\n\n\n2\npc\nnormal\n259\n77.31\n\n\n3\n\nabnormal\n76\n22.69\n\n\n4\npcc\nnotpresent\n354\n89.39\n\n\n...\n...\n...\n...\n...\n\n\n207\nane\nno\n339\n84.96\n\n\n208\n\nyes\n60\n15.04\n\n\n209\nclassification\nckd\n248\n62.00\n\n\n210\n\nnotckd\n150\n37.50\n\n\n211\n\nckd\\t\n2\n0.50\n\n\n\n\n212 rows × 4 columns"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#correlations-between-variables",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#correlations-between-variables",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_6180\\427603040.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.corr(method='pearson')\n\n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nbgr\nbu\nsc\nsod\npot\nhemo\n\n\n\n\nid\n1.000000\n-0.185308\n-0.245744\n0.642156\n-0.541993\n-0.283416\n-0.338673\n-0.307175\n-0.268683\n0.364251\n-0.092347\n0.640298\n\n\nage\n-0.185308\n1.000000\n0.159480\n-0.191096\n0.122091\n0.220866\n0.244992\n0.196985\n0.132531\n-0.100046\n0.058377\n-0.192928\n\n\nbp\n-0.245744\n0.159480\n1.000000\n-0.218836\n0.160689\n0.222576\n0.160193\n0.188517\n0.146222\n-0.116422\n0.075151\n-0.306540\n\n\nsg\n0.642156\n-0.191096\n-0.218836\n1.000000\n-0.469760\n-0.296234\n-0.374710\n-0.314295\n-0.361473\n0.412190\n-0.072787\n0.602582\n\n\nal\n-0.541993\n0.122091\n0.160689\n-0.469760\n1.000000\n0.269305\n0.379464\n0.453528\n0.399198\n-0.459896\n0.129038\n-0.634632\n\n\nsu\n-0.283416\n0.220866\n0.222576\n-0.296234\n0.269305\n1.000000\n0.717827\n0.168583\n0.223244\n-0.131776\n0.219450\n-0.224775\n\n\nbgr\n-0.338673\n0.244992\n0.160193\n-0.374710\n0.379464\n0.717827\n1.000000\n0.143322\n0.114875\n-0.267848\n0.066966\n-0.306189\n\n\nbu\n-0.307175\n0.196985\n0.188517\n-0.314295\n0.453528\n0.168583\n0.143322\n1.000000\n0.586368\n-0.323054\n0.357049\n-0.610360\n\n\nsc\n-0.268683\n0.132531\n0.146222\n-0.361473\n0.399198\n0.223244\n0.114875\n0.586368\n1.000000\n-0.690158\n0.326107\n-0.401670\n\n\nsod\n0.364251\n-0.100046\n-0.116422\n0.412190\n-0.459896\n-0.131776\n-0.267848\n-0.323054\n-0.690158\n1.000000\n0.097887\n0.365183\n\n\npot\n-0.092347\n0.058377\n0.075151\n-0.072787\n0.129038\n0.219450\n0.066966\n0.357049\n0.326107\n0.097887\n1.000000\n-0.133746\n\n\nhemo\n0.640298\n-0.192928\n-0.306540\n0.602582\n-0.634632\n-0.224775\n-0.306189\n-0.610360\n-0.401670\n0.365183\n-0.133746\n1.000000"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#skewness",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#skewness",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_6180\\942340472.py:2: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.skew()\n\n\nid       0.000000\nage     -0.668259\nbp       1.605429\nsg      -0.172444\nal       0.998157\nsu       2.464262\nbgr      2.010773\nbu       2.634374\nsc       7.509538\nsod     -6.996569\npot     11.582956\nhemo    -0.335095\ndtype: float64"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#data-visualizations",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#data-visualizations",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(3,4))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(3,4))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(10,6))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_6180\\2539154758.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr = data.corr()"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#setup",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#setup",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nrbc\npc\npcc\nba\n...\npcv\nwc\nrc\nhtn\ndm\ncad\nappet\npe\nane\nclassification\n\n\n\n\n0\n0\n48.0\n80.0\n1.020\n1.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n44\n7800\n5.2\nyes\nyes\nno\ngood\nno\nno\nckd\n\n\n1\n1\n7.0\n50.0\n1.020\n4.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n38\n6000\nNaN\nno\nno\nno\ngood\nno\nno\nckd\n\n\n2\n2\n62.0\n80.0\n1.010\n2.0\n3.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n31\n7500\nNaN\nno\nyes\nno\npoor\nno\nyes\nckd\n\n\n3\n3\n48.0\n70.0\n1.005\n4.0\n0.0\nnormal\nabnormal\npresent\nnotpresent\n...\n32\n6700\n3.9\nyes\nno\nno\npoor\nyes\nyes\nckd\n\n\n4\n4\n51.0\n80.0\n1.010\n2.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n35\n7300\n4.6\nno\nno\nno\ngood\nno\nno\nckd\n\n\n\n\n5 rows × 26 columns\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.classification import *\nsetup(data, target = 'classification', session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nclassification\n\n\n2\nTarget type\nMulticlass\n\n\n3\nTarget mapping\nckd: 0, ckd : 1, notckd: 2\n\n\n4\nOriginal data shape\n(400, 26)\n\n\n5\nTransformed data shape\n(400, 32)\n\n\n6\nTransformed train set shape\n(280, 32)\n\n\n7\nTransformed test set shape\n(120, 32)\n\n\n8\nOrdinal features\n8\n\n\n9\nNumeric features\n12\n\n\n10\nCategorical features\n13\n\n\n11\nRows with missing values\n60.5%\n\n\n12\nPreprocess\nTrue\n\n\n13\nImputation type\nsimple\n\n\n14\nNumeric imputation\nmean\n\n\n15\nCategorical imputation\nmode\n\n\n16\nMaximum one-hot encoding\n25\n\n\n17\nEncoding method\nNone\n\n\n18\nFold Generator\nStratifiedKFold\n\n\n19\nFold Number\n10\n\n\n20\nCPU Jobs\n-1\n\n\n21\nUse GPU\nFalse\n\n\n22\nLog Experiment\nFalse\n\n\n23\nExperiment Name\nclf-default-name\n\n\n24\nUSI\n240f\n\n\n\n\n\n&lt;pycaret.classification.oop.ClassificationExperiment at 0x132a2c66cb0&gt;"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#compare-models",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#compare-models",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\nrf\nRandom Forest Classifier\n0.9964\n0.0000\n0.9964\n0.9931\n0.9947\n0.9926\n0.9929\n0.1460\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.9964\n0.0000\n0.9964\n0.9931\n0.9947\n0.9926\n0.9929\n0.2330\n\n\nada\nAda Boost Classifier\n0.9929\n0.0000\n0.9929\n0.9931\n0.9929\n0.9852\n0.9859\n0.1350\n\n\net\nExtra Trees Classifier\n0.9929\n0.0000\n0.9929\n0.9897\n0.9911\n0.9850\n0.9856\n0.1480\n\n\nknn\nK Neighbors Classifier\n0.9893\n0.0000\n0.9893\n0.9871\n0.9877\n0.9777\n0.9790\n0.2620\n\n\nlr\nLogistic Regression\n0.9857\n0.0000\n0.9857\n0.9931\n0.9892\n0.9709\n0.9722\n0.5920\n\n\ngbc\nGradient Boosting Classifier\n0.9821\n0.0000\n0.9821\n0.9931\n0.9870\n0.9649\n0.9679\n0.1530\n\n\ndt\nDecision Tree Classifier\n0.9786\n0.0000\n0.9786\n0.9931\n0.9852\n0.9577\n0.9610\n0.1230\n\n\nridge\nRidge Classifier\n0.9679\n0.0000\n0.9679\n0.9669\n0.9664\n0.9328\n0.9351\n0.1220\n\n\nlda\nLinear Discriminant Analysis\n0.9607\n0.0000\n0.9607\n0.9766\n0.9672\n0.9214\n0.9261\n0.1260\n\n\nnb\nNaive Bayes\n0.9536\n0.0000\n0.9536\n0.9560\n0.9523\n0.9038\n0.9088\n0.1270\n\n\nsvm\nSVM - Linear Kernel\n0.9536\n0.0000\n0.9536\n0.9575\n0.9503\n0.8981\n0.9079\n0.1260\n\n\ndummy\nDummy Classifier\n0.6214\n0.0000\n0.6214\n0.3865\n0.4765\n0.0000\n0.0000\n0.1260"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#create-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#create-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \nrf = create_model('rf')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n1\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n2\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n3\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n4\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n9\n0.9643\n0.0000\n0.9643\n0.9306\n0.9469\n0.9259\n0.9293\n\n\nMean\n0.9964\n0.0000\n0.9964\n0.9931\n0.9947\n0.9926\n0.9929\n\n\nStd\n0.0107\n0.0000\n0.0107\n0.0208\n0.0159\n0.0222\n0.0212\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(rf)\n\nRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       n_estimators=100, n_jobs=-1, oob_score=False,\n                       random_state=123, verbose=0, warm_start=False)"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#tune-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#tune-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of rf\ntuned_rf = tune_model(rf)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n1\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n2\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n3\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n4\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n9\n0.9643\n0.0000\n0.9643\n0.9306\n0.9469\n0.9259\n0.9293\n\n\nMean\n0.9964\n0.0000\n0.9964\n0.9931\n0.9947\n0.9926\n0.9929\n\n\nStd\n0.0107\n0.0000\n0.0107\n0.0208\n0.0159\n0.0222\n0.0212\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_rf, tuner = tune_model(rf, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n1\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n2\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n3\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n4\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n5\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n7\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n1.0000\n0.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n9\n0.9643\n0.0000\n0.9643\n0.9306\n0.9469\n0.9259\n0.9293\n\n\nMean\n0.9964\n0.0000\n0.9964\n0.9931\n0.9947\n0.9926\n0.9929\n\n\nStd\n0.0107\n0.0000\n0.0107\n0.0208\n0.0159\n0.0222\n0.0212\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_rf\n\nRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       n_estimators=100, n_jobs=-1, oob_score=False,\n                       random_state=123, verbose=0, warm_start=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomForestClassifierRandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n                       criterion='gini', max_depth=None, max_features='sqrt',\n                       max_leaf_nodes=None, max_samples=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       n_estimators=100, n_jobs=-1, oob_score=False,\n                       random_state=123, verbose=0, warm_start=False)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['id', 'age', 'bp', 'sg', 'al',\n                                             'su', 'bgr', 'bu', 'sc', 'sod',\n                                             'pot', 'hemo'],\n                                    transformer=SimpleImputer(add_...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='sqrt',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=-1,\n                                        oob_score=False, random_state=123,\n                                        verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(include=['id', 'age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu',\n                            'sc', 'sod', 'pot', 'hemo'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn',\n                            'dm', 'cad', 'appet', 'pe', 'ane'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe',\n                            'ane'],\n                   transformer=OrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba',\n                                                    'htn', 'appet', 'pe',\n                                                    'ane'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'rbc',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                                                       {'col': 'pc',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int6...\n                                                        'data_type': dtype('O'),\n                                                        'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                                                       {'col': 'htn',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'appet',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                                                       {'col': 'pe',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'ane',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])onehot_encoding: TransformerWrapperTransformerWrapper(include=['dm', 'cad'],\n                   transformer=OneHotEncoder(cols=['dm', 'cad'],\n                                             handle_missing='return_nan',\n                                             use_cat_names=True))transformer: OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)rest_encoding: TransformerWrapperTransformerWrapper(include=['pcv', 'wc', 'rc'],\n                   transformer=TargetEncoder(cols=['pcv', 'wc', 'rc'],\n                                             handle_missing='return_nan'))transformer: TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#analyze-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#analyze-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# plot confusion matrix\nplot_model(best, plot = 'confusion_matrix')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot AUC\nplot_model(best, plot = 'auc')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot class report\nplot_model(best, plot = 'class_report')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(best, plot = 'feature')"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#evaluate-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#evaluate-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(best)"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#finalize-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#finalize-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(rf)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['id', 'age', 'bp', 'sg', 'al',\n                                             'su', 'bgr', 'bu', 'sc', 'sod',\n                                             'pot', 'hemo'],\n                                    transformer=SimpleImputer(add_...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='sqrt',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=-1,\n                                        oob_score=False, random_state=123,\n                                        verbose=0, warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['id', 'age', 'bp', 'sg', 'al',\n                                             'su', 'bgr', 'bu', 'sc', 'sod',\n                                             'pot', 'hemo'],\n                                    transformer=SimpleImputer(add_...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='sqrt',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=-1,\n                                        oob_score=False, random_state=123,\n                                        verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['id', 'age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu',\n                            'sc', 'sod', 'pot', 'hemo'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn',\n                            'dm', 'cad', 'appet', 'pe', 'ane'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe',\n                            'ane'],\n                   transformer=OrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba',\n                                                    'htn', 'appet', 'pe',\n                                                    'ane'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'rbc',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                                                       {'col': 'pc',\n                                                        'data_t...\n                                                        'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                                                       {'col': 'htn',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'appet',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                                                       {'col': 'pe',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'ane',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])onehot_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['dm', 'cad'],\n                   transformer=OneHotEncoder(cols=['dm', 'cad'],\n                                             drop_invariant=False,\n                                             handle_missing='return_nan',\n                                             handle_unknown='value',\n                                             return_df=True, use_cat_names=True,\n                                             verbose=0))transformer: OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)rest_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['pcv', 'wc', 'rc'],\n                   transformer=TargetEncoder(cols=['pcv', 'wc', 'rc'],\n                                             drop_invariant=False,\n                                             handle_missing='return_nan',\n                                             handle_unknown='value',\n                                             hierarchy=None,\n                                             min_samples_leaf=20,\n                                             return_df=True, smoothing=10,\n                                             verbose=0))transformer: TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#prediction",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#prediction",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(best)\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\nRandom Forest Classifier\n0.9917\n0\n0.9917\n0.9834\n0.9875\n0.9824\n0.9826\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nrbc\npc\npcc\nba\n...\nrc\nhtn\ndm\ncad\nappet\npe\nane\nclassification\nprediction_label\nprediction_score\n\n\n\n\n247\n247\n54.0\n90.0\n1.025\n1.0\n0.0\nnormal\nabnormal\nnotpresent\nnotpresent\n...\nNaN\nno\nno\nno\npoor\nyes\nyes\n0\nckd\n0.93\n\n\n69\n69\n26.0\n70.0\n1.015\n0.0\n4.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n6.0\nno\nyes\nno\ngood\nno\nno\n0\nckd\n0.85\n\n\n289\n289\n42.0\n70.0\n1.020\n0.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n5.3\nno\nno\nno\ngood\nno\nno\n2\nnotckd\n1.00\n\n\n372\n372\n72.0\n60.0\n1.020\n0.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n5.5\nno\nno\nno\ngood\nno\nno\n2\nnotckd\n1.00\n\n\n139\n139\n41.0\n70.0\n1.015\n2.0\n0.0\nNaN\nabnormal\nnotpresent\npresent\n...\nNaN\nyes\nno\nno\ngood\nyes\nyes\n0\nckd\n1.00\n\n\n\n\n5 rows × 28 columns\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('classification', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nrbc\npc\npcc\nba\n...\nhemo\npcv\nwc\nrc\nhtn\ndm\ncad\nappet\npe\nane\n\n\n\n\n0\n0\n48.0\n80.0\n1.020\n1.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n15.4\n44\n7800\n5.2\nyes\nyes\nno\ngood\nno\nno\n\n\n1\n1\n7.0\n50.0\n1.020\n4.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n11.3\n38\n6000\nNaN\nno\nno\nno\ngood\nno\nno\n\n\n2\n2\n62.0\n80.0\n1.010\n2.0\n3.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n9.6\n31\n7500\nNaN\nno\nyes\nno\npoor\nno\nyes\n\n\n3\n3\n48.0\n70.0\n1.005\n4.0\n0.0\nnormal\nabnormal\npresent\nnotpresent\n...\n11.2\n32\n6700\n3.9\nyes\nno\nno\npoor\nyes\nyes\n\n\n4\n4\n51.0\n80.0\n1.010\n2.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n11.6\n35\n7300\n4.6\nno\nno\nno\ngood\nno\nno\n\n\n\n\n5 rows × 25 columns\n\n\n\n\n# predict model on new_data\npredictions = predict_model(best, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nid\nage\nbp\nsg\nal\nsu\nrbc\npc\npcc\nba\n...\nwc\nrc\nhtn\ndm\ncad\nappet\npe\nane\nprediction_label\nprediction_score\n\n\n\n\n0\n0\n48.0\n80.0\n1.020\n1.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n7800\n5.2\nyes\nyes\nno\ngood\nno\nno\nckd\n0.89\n\n\n1\n1\n7.0\n50.0\n1.020\n4.0\n0.0\nNaN\nnormal\nnotpresent\nnotpresent\n...\n6000\nNaN\nno\nno\nno\ngood\nno\nno\nckd\n0.98\n\n\n2\n2\n62.0\n80.0\n1.010\n2.0\n3.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n7500\nNaN\nno\nyes\nno\npoor\nno\nyes\nckd\n0.98\n\n\n3\n3\n48.0\n70.0\n1.005\n4.0\n0.0\nnormal\nabnormal\npresent\nnotpresent\n...\n6700\n3.9\nyes\nno\nno\npoor\nyes\nyes\nckd\n1.00\n\n\n4\n4\n51.0\n80.0\n1.010\n2.0\n0.0\nnormal\nnormal\nnotpresent\nnotpresent\n...\n7300\n4.6\nno\nno\nno\ngood\nno\nno\nckd\n1.00\n\n\n\n\n5 rows × 27 columns"
  },
  {
    "objectID": "projects/Choronic_Kidney/01_Choronic Kidney.html#save-model",
    "href": "projects/Choronic_Kidney/01_Choronic Kidney.html#save-model",
    "title": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(best, '../models/chronic_kidney')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('label_encoding',\n                  TransformerWrapperWithInverse(exclude=None, include=None,\n                                                transformer=LabelEncoder())),\n                 ('numerical_imputer',\n                  TransformerWrapper(exclude=None,\n                                     include=['id', 'age', 'bp', 'sg', 'al',\n                                              'su', 'bgr', 'bu', 'sc', 'sod',\n                                              'pot', 'hemo'],\n                                     transformer=SimpleImputer(add_...\n                  RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                         class_weight=None, criterion='gini',\n                                         max_depth=None, max_features='sqrt',\n                                         max_leaf_nodes=None, max_samples=None,\n                                         min_impurity_decrease=0.0,\n                                         min_samples_leaf=1, min_samples_split=2,\n                                         min_weight_fraction_leaf=0.0,\n                                         n_estimators=100, n_jobs=-1,\n                                         oob_score=False, random_state=123,\n                                         verbose=0, warm_start=False))],\n          verbose=False),\n '../models/chronic_kidney.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/chronic_kidney')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['id', 'age', 'bp', 'sg', 'al',\n                                             'su', 'bgr', 'bu', 'sc', 'sod',\n                                             'pot', 'hemo'],\n                                    transformer=SimpleImputer(add_...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='sqrt',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=-1,\n                                        oob_score=False, random_state=123,\n                                        verbose=0, warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['id', 'age', 'bp', 'sg', 'al',\n                                             'su', 'bgr', 'bu', 'sc', 'sod',\n                                             'pot', 'hemo'],\n                                    transformer=SimpleImputer(add_...\n                 RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n                                        class_weight=None, criterion='gini',\n                                        max_depth=None, max_features='sqrt',\n                                        max_leaf_nodes=None, max_samples=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        n_estimators=100, n_jobs=-1,\n                                        oob_score=False, random_state=123,\n                                        verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['id', 'age', 'bp', 'sg', 'al', 'su', 'bgr', 'bu',\n                            'sc', 'sod', 'pot', 'hemo'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['rbc', 'pc', 'pcc', 'ba', 'pcv', 'wc', 'rc', 'htn',\n                            'dm', 'cad', 'appet', 'pe', 'ane'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe',\n                            'ane'],\n                   transformer=OrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba',\n                                                    'htn', 'appet', 'pe',\n                                                    'ane'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'rbc',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                                                       {'col': 'pc',\n                                                        'data_t...\n                                                        'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                                                       {'col': 'htn',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'appet',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                                                       {'col': 'pe',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                                                       {'col': 'ane',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['rbc', 'pc', 'pcc', 'ba', 'htn', 'appet', 'pe', 'ane'],\n               handle_missing='return_nan',\n               mapping=[{'col': 'rbc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pc', 'data_type': dtype('O'),\n                         'mapping': abnormal    0\nnormal      1\nNaN        -1\ndtype: int64},\n                        {'col': 'pcc', 'data_type': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'...e': dtype('O'),\n                         'mapping': notpresent    0\npresent       1\nNaN          -1\ndtype: int64},\n                        {'col': 'htn', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'appet', 'data_type': dtype('O'),\n                         'mapping': good    0\npoor    1\nNaN    -1\ndtype: int64},\n                        {'col': 'pe', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64},\n                        {'col': 'ane', 'data_type': dtype('O'),\n                         'mapping': no     0\nyes    1\nNaN   -1\ndtype: int64}])onehot_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['dm', 'cad'],\n                   transformer=OneHotEncoder(cols=['dm', 'cad'],\n                                             drop_invariant=False,\n                                             handle_missing='return_nan',\n                                             handle_unknown='value',\n                                             return_df=True, use_cat_names=True,\n                                             verbose=0))transformer: OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)OneHotEncoderOneHotEncoder(cols=['dm', 'cad'], handle_missing='return_nan',\n              use_cat_names=True)rest_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['pcv', 'wc', 'rc'],\n                   transformer=TargetEncoder(cols=['pcv', 'wc', 'rc'],\n                                             drop_invariant=False,\n                                             handle_missing='return_nan',\n                                             handle_unknown='value',\n                                             hierarchy=None,\n                                             min_samples_leaf=20,\n                                             return_df=True, smoothing=10,\n                                             verbose=0))transformer: TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')TargetEncoderTargetEncoder(cols=['pcv', 'wc', 'rc'], handle_missing='return_nan')RandomForestClassifierRandomForestClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "",
    "text": "Accurate and early detection of breast cancer plays a vital role in improving patient outcomes and survival rates. However, existing detection methods often have limitations in terms of accuracy and efficiency. The aim of this project is to develop an advanced breast cancer detection system using Support Vector Machines (SVM) that can effectively classify breast tissue samples as malignant or benign, enabling timely intervention and improved patient care."
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#problem",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#problem",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "",
    "text": "Accurate and early detection of breast cancer plays a vital role in improving patient outcomes and survival rates. However, existing detection methods often have limitations in terms of accuracy and efficiency. The aim of this project is to develop an advanced breast cancer detection system using Support Vector Machines (SVM) that can effectively classify breast tissue samples as malignant or benign, enabling timely intervention and improved patient care."
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#objective-s",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#objective-s",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Objective (s)",
    "text": "Objective (s)\nDevelop a breast cancer detection system using Support Vector Machines (SVM) that can accurately classify breast tissue samples as malignant or benign.\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv(\"../data/breast_cancer.csv\")\ndata = data.drop(columns = ['Unnamed: 32', 'id'], axis = 1)\ndata.head()\n\n\n\n\n\n\n\n\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 31 columns"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#exploring-data",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#exploring-data",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(569, 31)\n\n\n\n# dtypes \ndata.dtypes\n\ndiagnosis                   object\nradius_mean                float64\ntexture_mean               float64\nperimeter_mean             float64\narea_mean                  float64\nsmoothness_mean            float64\ncompactness_mean           float64\nconcavity_mean             float64\nconcave points_mean        float64\nsymmetry_mean              float64\nfractal_dimension_mean     float64\nradius_se                  float64\ntexture_se                 float64\nperimeter_se               float64\narea_se                    float64\nsmoothness_se              float64\ncompactness_se             float64\nconcavity_se               float64\nconcave points_se          float64\nsymmetry_se                float64\nfractal_dimension_se       float64\nradius_worst               float64\ntexture_worst              float64\nperimeter_worst            float64\narea_worst                 float64\nsmoothness_worst           float64\ncompactness_worst          float64\nconcavity_worst            float64\nconcave points_worst       float64\nsymmetry_worst             float64\nfractal_dimension_worst    float64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 569 entries, 0 to 568\nData columns (total 31 columns):\n #   Column                   Non-Null Count  Dtype  \n---  ------                   --------------  -----  \n 0   diagnosis                569 non-null    object \n 1   radius_mean              569 non-null    float64\n 2   texture_mean             569 non-null    float64\n 3   perimeter_mean           569 non-null    float64\n 4   area_mean                569 non-null    float64\n 5   smoothness_mean          569 non-null    float64\n 6   compactness_mean         569 non-null    float64\n 7   concavity_mean           569 non-null    float64\n 8   concave points_mean      569 non-null    float64\n 9   symmetry_mean            569 non-null    float64\n 10  fractal_dimension_mean   569 non-null    float64\n 11  radius_se                569 non-null    float64\n 12  texture_se               569 non-null    float64\n 13  perimeter_se             569 non-null    float64\n 14  area_se                  569 non-null    float64\n 15  smoothness_se            569 non-null    float64\n 16  compactness_se           569 non-null    float64\n 17  concavity_se             569 non-null    float64\n 18  concave points_se        569 non-null    float64\n 19  symmetry_se              569 non-null    float64\n 20  fractal_dimension_se     569 non-null    float64\n 21  radius_worst             569 non-null    float64\n 22  texture_worst            569 non-null    float64\n 23  perimeter_worst          569 non-null    float64\n 24  area_worst               569 non-null    float64\n 25  smoothness_worst         569 non-null    float64\n 26  compactness_worst        569 non-null    float64\n 27  concavity_worst          569 non-null    float64\n 28  concave points_worst     569 non-null    float64\n 29  symmetry_worst           569 non-null    float64\n 30  fractal_dimension_worst  569 non-null    float64\ndtypes: float64(30), object(1)\nmemory usage: 137.9+ KB\n\n\n\n# check missing data \ndata.isnull().sum() \n\ndiagnosis                  0\nradius_mean                0\ntexture_mean               0\nperimeter_mean             0\narea_mean                  0\nsmoothness_mean            0\ncompactness_mean           0\nconcavity_mean             0\nconcave points_mean        0\nsymmetry_mean              0\nfractal_dimension_mean     0\nradius_se                  0\ntexture_se                 0\nperimeter_se               0\narea_se                    0\nsmoothness_se              0\ncompactness_se             0\nconcavity_se               0\nconcave points_se          0\nsymmetry_se                0\nfractal_dimension_se       0\nradius_worst               0\ntexture_worst              0\nperimeter_worst            0\narea_worst                 0\nsmoothness_worst           0\ncompactness_worst          0\nconcavity_worst            0\nconcave points_worst       0\nsymmetry_worst             0\nfractal_dimension_worst    0\ndtype: int64"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#descriptive-statistics",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#descriptive-statistics",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# select numeric data \nnum_cols = data.select_dtypes(exclude = 'object')\nnum_cols.head() \n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\nnum_cols.columns\n\nIndex(['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst'],\n      dtype='object')\n\n\n\n# summary statistics of numerical variables \nrp.summary_cont(num_cols[['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',\n       'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nradius_mean\n569.0\n14.1273\n3.5240\n0.1477\n13.8371\n14.4175\n\n\n1\ntexture_mean\n569.0\n19.2896\n4.3010\n0.1803\n18.9355\n19.6438\n\n\n2\nperimeter_mean\n569.0\n91.9690\n24.2990\n1.0187\n89.9682\n93.9698\n\n\n3\narea_mean\n569.0\n654.8891\n351.9141\n14.7530\n625.9120\n683.8662\n\n\n4\nsmoothness_mean\n569.0\n0.0964\n0.0141\n0.0006\n0.0952\n0.0975\n\n\n5\ncompactness_mean\n569.0\n0.1043\n0.0528\n0.0022\n0.1000\n0.1087\n\n\n6\nconcavity_mean\n569.0\n0.0888\n0.0797\n0.0033\n0.0822\n0.0954\n\n\n7\nconcave points_mean\n569.0\n0.0489\n0.0388\n0.0016\n0.0457\n0.0521\n\n\n8\nsymmetry_mean\n569.0\n0.1812\n0.0274\n0.0011\n0.1789\n0.1834\n\n\n9\nfractal_dimension_mean\n569.0\n0.0628\n0.0071\n0.0003\n0.0622\n0.0634\n\n\n10\nradius_se\n569.0\n0.4052\n0.2773\n0.0116\n0.3823\n0.4280\n\n\n11\ntexture_se\n569.0\n1.2169\n0.5516\n0.0231\n1.1714\n1.2623\n\n\n12\nperimeter_se\n569.0\n2.8661\n2.0219\n0.0848\n2.6996\n3.0325\n\n\n13\narea_se\n569.0\n40.3371\n45.4910\n1.9071\n36.5913\n44.0829\n\n\n14\nsmoothness_se\n569.0\n0.0070\n0.0030\n0.0001\n0.0068\n0.0073\n\n\n15\ncompactness_se\n569.0\n0.0255\n0.0179\n0.0008\n0.0240\n0.0270\n\n\n16\nconcavity_se\n569.0\n0.0319\n0.0302\n0.0013\n0.0294\n0.0344\n\n\n17\nconcave points_se\n569.0\n0.0118\n0.0062\n0.0003\n0.0113\n0.0123\n\n\n18\nsymmetry_se\n569.0\n0.0205\n0.0083\n0.0003\n0.0199\n0.0212\n\n\n19\nfractal_dimension_se\n569.0\n0.0038\n0.0026\n0.0001\n0.0036\n0.0040\n\n\n20\nradius_worst\n569.0\n16.2692\n4.8332\n0.2026\n15.8712\n16.6672\n\n\n21\ntexture_worst\n569.0\n25.6772\n6.1463\n0.2577\n25.1711\n26.1833\n\n\n22\nperimeter_worst\n569.0\n107.2612\n33.6025\n1.4087\n104.4943\n110.0281\n\n\n23\narea_worst\n569.0\n880.5831\n569.3570\n23.8687\n833.7015\n927.4648\n\n\n24\nsmoothness_worst\n569.0\n0.1324\n0.0228\n0.0010\n0.1305\n0.1342\n\n\n25\ncompactness_worst\n569.0\n0.2543\n0.1573\n0.0066\n0.2413\n0.2672\n\n\n26\nconcavity_worst\n569.0\n0.2722\n0.2086\n0.0087\n0.2550\n0.2894\n\n\n27\nconcave points_worst\n569.0\n0.1146\n0.0657\n0.0028\n0.1092\n0.1200\n\n\n28\nsymmetry_worst\n569.0\n0.2901\n0.0619\n0.0026\n0.2850\n0.2952\n\n\n29\nfractal_dimension_worst\n569.0\n0.0839\n0.0181\n0.0008\n0.0825\n0.0854\n\n\n\n\n\n\n\n\n# select categorical data \ncat_cols = data.select_dtypes(include = 'object')\ncat_cols.head() \n\n\n\n\n\n\n\n\ndiagnosis\n\n\n\n\n0\nM\n\n\n1\nM\n\n\n2\nM\n\n\n3\nM\n\n\n4\nM\n\n\n\n\n\n\n\n\ncat_cols.columns\n\nIndex(['diagnosis'], dtype='object')\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(cat_cols['diagnosis'])\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\ndiagnosis\nB\n357\n62.74\n\n\n1\n\nM\n212\n37.26"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#correlations-between-variables",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#correlations-between-variables",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_10504\\427603040.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.corr(method='pearson')\n\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\nradius_mean\n1.000000\n0.323782\n0.997855\n0.987357\n0.170581\n0.506124\n0.676764\n0.822529\n0.147741\n-0.311631\n...\n0.969539\n0.297008\n0.965137\n0.941082\n0.119616\n0.413463\n0.526911\n0.744214\n0.163953\n0.007066\n\n\ntexture_mean\n0.323782\n1.000000\n0.329533\n0.321086\n-0.023389\n0.236702\n0.302418\n0.293464\n0.071401\n-0.076437\n...\n0.352573\n0.912045\n0.358040\n0.343546\n0.077503\n0.277830\n0.301025\n0.295316\n0.105008\n0.119205\n\n\nperimeter_mean\n0.997855\n0.329533\n1.000000\n0.986507\n0.207278\n0.556936\n0.716136\n0.850977\n0.183027\n-0.261477\n...\n0.969476\n0.303038\n0.970387\n0.941550\n0.150549\n0.455774\n0.563879\n0.771241\n0.189115\n0.051019\n\n\narea_mean\n0.987357\n0.321086\n0.986507\n1.000000\n0.177028\n0.498502\n0.685983\n0.823269\n0.151293\n-0.283110\n...\n0.962746\n0.287489\n0.959120\n0.959213\n0.123523\n0.390410\n0.512606\n0.722017\n0.143570\n0.003738\n\n\nsmoothness_mean\n0.170581\n-0.023389\n0.207278\n0.177028\n1.000000\n0.659123\n0.521984\n0.553695\n0.557775\n0.584792\n...\n0.213120\n0.036072\n0.238853\n0.206718\n0.805324\n0.472468\n0.434926\n0.503053\n0.394309\n0.499316\n\n\ncompactness_mean\n0.506124\n0.236702\n0.556936\n0.498502\n0.659123\n1.000000\n0.883121\n0.831135\n0.602641\n0.565369\n...\n0.535315\n0.248133\n0.590210\n0.509604\n0.565541\n0.865809\n0.816275\n0.815573\n0.510223\n0.687382\n\n\nconcavity_mean\n0.676764\n0.302418\n0.716136\n0.685983\n0.521984\n0.883121\n1.000000\n0.921391\n0.500667\n0.336783\n...\n0.688236\n0.299879\n0.729565\n0.675987\n0.448822\n0.754968\n0.884103\n0.861323\n0.409464\n0.514930\n\n\nconcave points_mean\n0.822529\n0.293464\n0.850977\n0.823269\n0.553695\n0.831135\n0.921391\n1.000000\n0.462497\n0.166917\n...\n0.830318\n0.292752\n0.855923\n0.809630\n0.452753\n0.667454\n0.752399\n0.910155\n0.375744\n0.368661\n\n\nsymmetry_mean\n0.147741\n0.071401\n0.183027\n0.151293\n0.557775\n0.602641\n0.500667\n0.462497\n1.000000\n0.479921\n...\n0.185728\n0.090651\n0.219169\n0.177193\n0.426675\n0.473200\n0.433721\n0.430297\n0.699826\n0.438413\n\n\nfractal_dimension_mean\n-0.311631\n-0.076437\n-0.261477\n-0.283110\n0.584792\n0.565369\n0.336783\n0.166917\n0.479921\n1.000000\n...\n-0.253691\n-0.051269\n-0.205151\n-0.231854\n0.504942\n0.458798\n0.346234\n0.175325\n0.334019\n0.767297\n\n\nradius_se\n0.679090\n0.275869\n0.691765\n0.732562\n0.301467\n0.497473\n0.631925\n0.698050\n0.303379\n0.000111\n...\n0.715065\n0.194799\n0.719684\n0.751548\n0.141919\n0.287103\n0.380585\n0.531062\n0.094543\n0.049559\n\n\ntexture_se\n-0.097317\n0.386358\n-0.086761\n-0.066280\n0.068406\n0.046205\n0.076218\n0.021480\n0.128053\n0.164174\n...\n-0.111690\n0.409003\n-0.102242\n-0.083195\n-0.073658\n-0.092439\n-0.068956\n-0.119638\n-0.128215\n-0.045655\n\n\nperimeter_se\n0.674172\n0.281673\n0.693135\n0.726628\n0.296092\n0.548905\n0.660391\n0.710650\n0.313893\n0.039830\n...\n0.697201\n0.200371\n0.721031\n0.730713\n0.130054\n0.341919\n0.418899\n0.554897\n0.109930\n0.085433\n\n\narea_se\n0.735864\n0.259845\n0.744983\n0.800086\n0.246552\n0.455653\n0.617427\n0.690299\n0.223970\n-0.090170\n...\n0.757373\n0.196497\n0.761213\n0.811408\n0.125389\n0.283257\n0.385100\n0.538166\n0.074126\n0.017539\n\n\nsmoothness_se\n-0.222600\n0.006614\n-0.202694\n-0.166777\n0.332375\n0.135299\n0.098564\n0.027653\n0.187321\n0.401964\n...\n-0.230691\n-0.074743\n-0.217304\n-0.182195\n0.314457\n-0.055558\n-0.058298\n-0.102007\n-0.107342\n0.101480\n\n\ncompactness_se\n0.206000\n0.191975\n0.250744\n0.212583\n0.318943\n0.738722\n0.670279\n0.490424\n0.421659\n0.559837\n...\n0.204607\n0.143003\n0.260516\n0.199371\n0.227394\n0.678780\n0.639147\n0.483208\n0.277878\n0.590973\n\n\nconcavity_se\n0.194204\n0.143293\n0.228082\n0.207660\n0.248396\n0.570517\n0.691270\n0.439167\n0.342627\n0.446630\n...\n0.186904\n0.100241\n0.226680\n0.188353\n0.168481\n0.484858\n0.662564\n0.440472\n0.197788\n0.439329\n\n\nconcave points_se\n0.376169\n0.163851\n0.407217\n0.372320\n0.380676\n0.642262\n0.683260\n0.615634\n0.393298\n0.341198\n...\n0.358127\n0.086741\n0.394999\n0.342271\n0.215351\n0.452888\n0.549592\n0.602450\n0.143116\n0.310655\n\n\nsymmetry_se\n-0.104321\n0.009127\n-0.081629\n-0.072497\n0.200774\n0.229977\n0.178009\n0.095351\n0.449137\n0.345007\n...\n-0.128121\n-0.077473\n-0.103753\n-0.110343\n-0.012662\n0.060255\n0.037119\n-0.030413\n0.389402\n0.078079\n\n\nfractal_dimension_se\n-0.042641\n0.054458\n-0.005523\n-0.019887\n0.283607\n0.507318\n0.449301\n0.257584\n0.331786\n0.688132\n...\n-0.037488\n-0.003195\n-0.001000\n-0.022736\n0.170568\n0.390159\n0.379975\n0.215204\n0.111094\n0.591328\n\n\nradius_worst\n0.969539\n0.352573\n0.969476\n0.962746\n0.213120\n0.535315\n0.688236\n0.830318\n0.185728\n-0.253691\n...\n1.000000\n0.359921\n0.993708\n0.984015\n0.216574\n0.475820\n0.573975\n0.787424\n0.243529\n0.093492\n\n\ntexture_worst\n0.297008\n0.912045\n0.303038\n0.287489\n0.036072\n0.248133\n0.299879\n0.292752\n0.090651\n-0.051269\n...\n0.359921\n1.000000\n0.365098\n0.345842\n0.225429\n0.360832\n0.368366\n0.359755\n0.233027\n0.219122\n\n\nperimeter_worst\n0.965137\n0.358040\n0.970387\n0.959120\n0.238853\n0.590210\n0.729565\n0.855923\n0.219169\n-0.205151\n...\n0.993708\n0.365098\n1.000000\n0.977578\n0.236775\n0.529408\n0.618344\n0.816322\n0.269493\n0.138957\n\n\narea_worst\n0.941082\n0.343546\n0.941550\n0.959213\n0.206718\n0.509604\n0.675987\n0.809630\n0.177193\n-0.231854\n...\n0.984015\n0.345842\n0.977578\n1.000000\n0.209145\n0.438296\n0.543331\n0.747419\n0.209146\n0.079647\n\n\nsmoothness_worst\n0.119616\n0.077503\n0.150549\n0.123523\n0.805324\n0.565541\n0.448822\n0.452753\n0.426675\n0.504942\n...\n0.216574\n0.225429\n0.236775\n0.209145\n1.000000\n0.568187\n0.518523\n0.547691\n0.493838\n0.617624\n\n\ncompactness_worst\n0.413463\n0.277830\n0.455774\n0.390410\n0.472468\n0.865809\n0.754968\n0.667454\n0.473200\n0.458798\n...\n0.475820\n0.360832\n0.529408\n0.438296\n0.568187\n1.000000\n0.892261\n0.801080\n0.614441\n0.810455\n\n\nconcavity_worst\n0.526911\n0.301025\n0.563879\n0.512606\n0.434926\n0.816275\n0.884103\n0.752399\n0.433721\n0.346234\n...\n0.573975\n0.368366\n0.618344\n0.543331\n0.518523\n0.892261\n1.000000\n0.855434\n0.532520\n0.686511\n\n\nconcave points_worst\n0.744214\n0.295316\n0.771241\n0.722017\n0.503053\n0.815573\n0.861323\n0.910155\n0.430297\n0.175325\n...\n0.787424\n0.359755\n0.816322\n0.747419\n0.547691\n0.801080\n0.855434\n1.000000\n0.502528\n0.511114\n\n\nsymmetry_worst\n0.163953\n0.105008\n0.189115\n0.143570\n0.394309\n0.510223\n0.409464\n0.375744\n0.699826\n0.334019\n...\n0.243529\n0.233027\n0.269493\n0.209146\n0.493838\n0.614441\n0.532520\n0.502528\n1.000000\n0.537848\n\n\nfractal_dimension_worst\n0.007066\n0.119205\n0.051019\n0.003738\n0.499316\n0.687382\n0.514930\n0.368661\n0.438413\n0.767297\n...\n0.093492\n0.219122\n0.138957\n0.079647\n0.617624\n0.810455\n0.686511\n0.511114\n0.537848\n1.000000\n\n\n\n\n30 rows × 30 columns"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#skewness",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#skewness",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_10504\\942340472.py:2: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.skew()\n\n\nradius_mean                0.942380\ntexture_mean               0.650450\nperimeter_mean             0.990650\narea_mean                  1.645732\nsmoothness_mean            0.456324\ncompactness_mean           1.190123\nconcavity_mean             1.401180\nconcave points_mean        1.171180\nsymmetry_mean              0.725609\nfractal_dimension_mean     1.304489\nradius_se                  3.088612\ntexture_se                 1.646444\nperimeter_se               3.443615\narea_se                    5.447186\nsmoothness_se              2.314450\ncompactness_se             1.902221\nconcavity_se               5.110463\nconcave points_se          1.444678\nsymmetry_se                2.195133\nfractal_dimension_se       3.923969\nradius_worst               1.103115\ntexture_worst              0.498321\nperimeter_worst            1.128164\narea_worst                 1.859373\nsmoothness_worst           0.415426\ncompactness_worst          1.473555\nconcavity_worst            1.150237\nconcave points_worst       0.492616\nsymmetry_worst             1.433928\nfractal_dimension_worst    1.662579\ndtype: float64"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#data-visualizations",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#data-visualizations",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(6,5))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(6,5))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(20,6))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_10504\\4023993825.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr = data.corr()"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#setup",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#setup",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\ndiagnosis\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\nM\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\nM\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\nM\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\nM\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\nM\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.classification import *\nsetup(data, target = 'diagnosis', session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\ndiagnosis\n\n\n2\nTarget type\nBinary\n\n\n3\nTarget mapping\nB: 0, M: 1\n\n\n4\nOriginal data shape\n(569, 31)\n\n\n5\nTransformed data shape\n(569, 31)\n\n\n6\nTransformed train set shape\n(398, 31)\n\n\n7\nTransformed test set shape\n(171, 31)\n\n\n8\nNumeric features\n30\n\n\n9\nPreprocess\nTrue\n\n\n10\nImputation type\nsimple\n\n\n11\nNumeric imputation\nmean\n\n\n12\nCategorical imputation\nmode\n\n\n13\nFold Generator\nStratifiedKFold\n\n\n14\nFold Number\n10\n\n\n15\nCPU Jobs\n-1\n\n\n16\nUse GPU\nFalse\n\n\n17\nLog Experiment\nFalse\n\n\n18\nExperiment Name\nclf-default-name\n\n\n19\nUSI\n7d04\n\n\n\n\n\n&lt;pycaret.classification.oop.ClassificationExperiment at 0x2821cc82200&gt;"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#compare-models",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#compare-models",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.9673\n0.9961\n0.9329\n0.9795\n0.9539\n0.9287\n0.9313\n0.4020\n\n\nada\nAda Boost Classifier\n0.9622\n0.9911\n0.9395\n0.9621\n0.9484\n0.9187\n0.9215\n0.3980\n\n\nlda\nLinear Discriminant Analysis\n0.9599\n0.9920\n0.8924\n1.0000\n0.9412\n0.9112\n0.9165\n0.3920\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.9599\n0.9926\n0.9262\n0.9665\n0.9437\n0.9127\n0.9156\n0.4900\n\n\nqda\nQuadratic Discriminant Analysis\n0.9571\n0.9952\n0.9390\n0.9478\n0.9420\n0.9080\n0.9097\n0.3730\n\n\ngbc\nGradient Boosting Classifier\n0.9547\n0.9921\n0.9390\n0.9453\n0.9393\n0.9033\n0.9068\n0.4250\n\n\nrf\nRandom Forest Classifier\n0.9546\n0.9939\n0.9390\n0.9460\n0.9397\n0.9034\n0.9070\n0.4410\n\n\nridge\nRidge Classifier\n0.9497\n0.0000\n0.8852\n0.9788\n0.9268\n0.8890\n0.8944\n0.3750\n\n\nlr\nLogistic Regression\n0.9473\n0.9923\n0.9129\n0.9514\n0.9271\n0.8861\n0.8916\n0.8610\n\n\nnb\nNaive Bayes\n0.9346\n0.9880\n0.8790\n0.9470\n0.9076\n0.8574\n0.8630\n0.3870\n\n\ndt\nDecision Tree Classifier\n0.9271\n0.9229\n0.9057\n0.9077\n0.9037\n0.8452\n0.8489\n0.3810\n\n\nknn\nK Neighbors Classifier\n0.9197\n0.9496\n0.8586\n0.9288\n0.8866\n0.8249\n0.8328\n0.4330\n\n\nsvm\nSVM - Linear Kernel\n0.8382\n0.0000\n0.8867\n0.8007\n0.8132\n0.6815\n0.7172\n0.4060\n\n\ndummy\nDummy Classifier\n0.6282\n0.5000\n0.0000\n0.0000\n0.0000\n0.0000\n0.0000\n0.3720"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#create-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#create-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \net = create_model('et')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n1\n0.9500\n0.9853\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n2\n0.9250\n1.0000\n0.8000\n1.0000\n0.8889\n0.8333\n0.8452\n\n\n3\n0.9500\n1.0000\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n4\n0.9750\n1.0000\n0.9333\n1.0000\n0.9655\n0.9459\n0.9473\n\n\n5\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n0.9500\n0.9960\n0.9333\n0.9333\n0.9333\n0.8933\n0.8933\n\n\n7\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.9487\n0.9800\n0.9286\n0.9286\n0.9286\n0.8886\n0.8886\n\n\n9\n0.9744\n1.0000\n1.0000\n0.9333\n0.9655\n0.9451\n0.9466\n\n\nMean\n0.9673\n0.9961\n0.9329\n0.9795\n0.9539\n0.9287\n0.9313\n\n\nStd\n0.0252\n0.0069\n0.0667\n0.0313\n0.0364\n0.0554\n0.0528\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(et)\n\nExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='sqrt',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_samples_leaf=1,\n                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n                     n_estimators=100, n_jobs=-1, oob_score=False,\n                     random_state=123, verbose=0, warm_start=False)"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#tune-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#tune-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of rf\ntuned_et = tune_model(et)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9500\n1.0000\n1.0000\n0.8824\n0.9375\n0.8961\n0.9010\n\n\n1\n0.9500\n0.9733\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n2\n0.9750\n1.0000\n0.9333\n1.0000\n0.9655\n0.9459\n0.9473\n\n\n3\n0.9500\n0.9973\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n4\n0.9750\n1.0000\n0.9333\n1.0000\n0.9655\n0.9459\n0.9473\n\n\n5\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n0.9750\n0.9973\n1.0000\n0.9375\n0.9677\n0.9474\n0.9487\n\n\n7\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.8974\n0.9800\n0.9286\n0.8125\n0.8667\n0.7839\n0.7885\n\n\n9\n0.9744\n1.0000\n1.0000\n0.9333\n0.9655\n0.9451\n0.9466\n\n\nMean\n0.9647\n0.9948\n0.9529\n0.9566\n0.9526\n0.9245\n0.9271\n\n\nStd\n0.0284\n0.0092\n0.0523\n0.0620\n0.0374\n0.0601\n0.0584\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_et, tuner = tune_model(et, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9500\n1.0000\n1.0000\n0.8824\n0.9375\n0.8961\n0.9010\n\n\n1\n0.9500\n0.9733\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n2\n0.9750\n1.0000\n0.9333\n1.0000\n0.9655\n0.9459\n0.9473\n\n\n3\n0.9500\n0.9973\n0.8667\n1.0000\n0.9286\n0.8904\n0.8958\n\n\n4\n0.9750\n1.0000\n0.9333\n1.0000\n0.9655\n0.9459\n0.9473\n\n\n5\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n6\n0.9750\n0.9973\n1.0000\n0.9375\n0.9677\n0.9474\n0.9487\n\n\n7\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n8\n0.8974\n0.9800\n0.9286\n0.8125\n0.8667\n0.7839\n0.7885\n\n\n9\n0.9744\n1.0000\n1.0000\n0.9333\n0.9655\n0.9451\n0.9466\n\n\nMean\n0.9647\n0.9948\n0.9529\n0.9566\n0.9526\n0.9245\n0.9271\n\n\nStd\n0.0284\n0.0092\n0.0523\n0.0620\n0.0374\n0.0601\n0.0584\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_et\n\nExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='sqrt',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_samples_leaf=1,\n                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n                     n_estimators=100, n_jobs=-1, oob_score=False,\n                     random_state=123, verbose=0, warm_start=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExtraTreesClassifierExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='gini', max_depth=None, max_features='sqrt',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_samples_leaf=1,\n                     min_samples_split=2, min_weight_fraction_leaf=0.0,\n                     n_estimators=100, n_jobs=-1, oob_score=False,\n                     random_state=123, verbose=0, warm_start=False)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['radius_mean', 'texture_mean',\n                                             'perimeter_mean', 'area_mean',\n                                             'smoothness_mean',\n                                             'compactness_mean',\n                                             'c...\n                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                      class_weight=None, criterion='gini',\n                                      max_depth=None, max_features='sqrt',\n                                      max_leaf_nodes=None, max_samples=None,\n                                      min_impurity_decrease=0.0,\n                                      min_samples_leaf=1, min_samples_split=2,\n                                      min_weight_fraction_leaf=0.0,\n                                      n_estimators=100, n_jobs=-1,\n                                      oob_score=False, random_state=123,\n                                      verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(include=['radius_mean', 'texture_mean', 'perimeter_mean',\n                            'area_mean', 'smoothness_mean', 'compactness_mean',\n                            'concavity_mean', 'concave points_mean',\n                            'symmetry_mean', 'fractal_dimension_mean',\n                            'radius_se', 'texture_se', 'perimeter_se',\n                            'area_se', 'smoothness_se', 'compactness_se',\n                            'concavity_se', 'concave points_se', 'symmetry_se',\n                            'fractal_dimension_se', 'radius_worst',\n                            'texture_worst', 'perimeter_worst', 'area_worst',\n                            'smoothness_worst', 'compactness_worst',\n                            'concavity_worst', 'concave points_worst',\n                            'symmetry_worst', 'fractal_dimension_worst'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=[],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')clean_column_names: TransformerWrapperTransformerWrapper(transformer=CleanColumnNames())transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()ExtraTreesClassifierExtraTreesClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#analyze-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#analyze-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# plot confusion matrix\nplot_model(et, plot = 'confusion_matrix')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot AUC\nplot_model(et, plot = 'auc')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot class report\nplot_model(et, plot = 'class_report')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(et, plot = 'feature')"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#evaluate-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#evaluate-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(et)"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#finalize-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#finalize-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(et)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['radius_mean', 'texture_mean',\n                                             'perimeter_mean', 'area_mean',\n                                             'smoothness_mean',\n                                             'compactness_mean',\n                                             'c...\n                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                      class_weight=None, criterion='gini',\n                                      max_depth=None, max_features='sqrt',\n                                      max_leaf_nodes=None, max_samples=None,\n                                      min_impurity_decrease=0.0,\n                                      min_samples_leaf=1, min_samples_split=2,\n                                      min_weight_fraction_leaf=0.0,\n                                      n_estimators=100, n_jobs=-1,\n                                      oob_score=False, random_state=123,\n                                      verbose=0, warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['radius_mean', 'texture_mean',\n                                             'perimeter_mean', 'area_mean',\n                                             'smoothness_mean',\n                                             'compactness_mean',\n                                             'c...\n                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                      class_weight=None, criterion='gini',\n                                      max_depth=None, max_features='sqrt',\n                                      max_leaf_nodes=None, max_samples=None,\n                                      min_impurity_decrease=0.0,\n                                      min_samples_leaf=1, min_samples_split=2,\n                                      min_weight_fraction_leaf=0.0,\n                                      n_estimators=100, n_jobs=-1,\n                                      oob_score=False, random_state=123,\n                                      verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['radius_mean', 'texture_mean', 'perimeter_mean',\n                            'area_mean', 'smoothness_mean', 'compactness_mean',\n                            'concavity_mean', 'concave points_mean',\n                            'symmetry_mean', 'fractal_dimension_mean',\n                            'radius_se', 'texture_se', 'perimeter_se',\n                            'area_se', 'smoothness_se', 'compactness_se',\n                            'concavity_se', 'concave points_se', 'symmetry_se',\n                            'fra...nsion_se', 'radius_worst',\n                            'texture_worst', 'perimeter_worst', 'area_worst',\n                            'smoothness_worst', 'compactness_worst',\n                            'concavity_worst', 'concave points_worst',\n                            'symmetry_worst', 'fractal_dimension_worst'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=[],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')clean_column_names: TransformerWrapperTransformerWrapper(exclude=None, include=None,\n                   transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()ExtraTreesClassifierExtraTreesClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#prediction",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#prediction",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(et)\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\nExtra Trees Classifier\n0.9649\n0.9918\n0.9688\n0.9394\n0.9538\n0.9256\n0.9258\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\ndiagnosis\nprediction_label\nprediction_score\n\n\n\n\n115\n11.930000\n21.530001\n76.529999\n438.600006\n0.09768\n0.07849\n0.03328\n0.02008\n0.1688\n0.06194\n...\n583.000000\n0.15000\n0.2399\n0.15030\n0.07247\n0.2438\n0.08541\n0\nB\n0.99\n\n\n21\n9.504000\n12.440000\n60.340000\n273.899994\n0.10240\n0.06492\n0.02956\n0.02076\n0.1815\n0.06905\n...\n314.899994\n0.13240\n0.1148\n0.08867\n0.06227\n0.2450\n0.07773\n0\nB\n1.00\n\n\n382\n12.050000\n22.719999\n78.750000\n447.799988\n0.06935\n0.10730\n0.07943\n0.02978\n0.1203\n0.06659\n...\n488.399994\n0.08799\n0.3214\n0.29120\n0.10920\n0.2191\n0.09349\n0\nB\n0.93\n\n\n136\n11.710000\n16.670000\n74.720001\n423.600006\n0.10510\n0.06095\n0.03592\n0.02600\n0.1339\n0.05945\n...\n546.700012\n0.12710\n0.1028\n0.10460\n0.06968\n0.1712\n0.07343\n0\nB\n0.98\n\n\n2\n19.690001\n21.250000\n130.000000\n1203.000000\n0.10960\n0.15990\n0.19740\n0.12790\n0.2069\n0.05999\n...\n1709.000000\n0.14440\n0.4245\n0.45040\n0.24300\n0.3613\n0.08758\n1\nM\n1.00\n\n\n\n\n5 rows × 33 columns\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('diagnosis', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nradius_worst\ntexture_worst\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\n\n\n\n\n0\n17.99\n10.38\n122.80\n1001.0\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n25.38\n17.33\n184.60\n2019.0\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\n\n\n1\n20.57\n17.77\n132.90\n1326.0\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n24.99\n23.41\n158.80\n1956.0\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\n\n\n2\n19.69\n21.25\n130.00\n1203.0\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n23.57\n25.53\n152.50\n1709.0\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\n\n\n3\n11.42\n20.38\n77.58\n386.1\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n14.91\n26.50\n98.87\n567.7\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\n\n\n4\n20.29\n14.34\n135.10\n1297.0\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n22.54\n16.67\n152.20\n1575.0\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\n\n\n\n\n5 rows × 30 columns\n\n\n\n\n# predict model on new_data\npredictions = predict_model(best, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nradius_mean\ntexture_mean\nperimeter_mean\narea_mean\nsmoothness_mean\ncompactness_mean\nconcavity_mean\nconcave points_mean\nsymmetry_mean\nfractal_dimension_mean\n...\nperimeter_worst\narea_worst\nsmoothness_worst\ncompactness_worst\nconcavity_worst\nconcave points_worst\nsymmetry_worst\nfractal_dimension_worst\nprediction_label\nprediction_score\n\n\n\n\n0\n17.990000\n10.380000\n122.800003\n1001.000000\n0.11840\n0.27760\n0.3001\n0.14710\n0.2419\n0.07871\n...\n184.600006\n2019.000000\n0.1622\n0.6656\n0.7119\n0.2654\n0.4601\n0.11890\nM\n1.00\n\n\n1\n20.570000\n17.770000\n132.899994\n1326.000000\n0.08474\n0.07864\n0.0869\n0.07017\n0.1812\n0.05667\n...\n158.800003\n1956.000000\n0.1238\n0.1866\n0.2416\n0.1860\n0.2750\n0.08902\nM\n1.00\n\n\n2\n19.690001\n21.250000\n130.000000\n1203.000000\n0.10960\n0.15990\n0.1974\n0.12790\n0.2069\n0.05999\n...\n152.500000\n1709.000000\n0.1444\n0.4245\n0.4504\n0.2430\n0.3613\n0.08758\nM\n1.00\n\n\n3\n11.420000\n20.379999\n77.580002\n386.100006\n0.14250\n0.28390\n0.2414\n0.10520\n0.2597\n0.09744\n...\n98.870003\n567.700012\n0.2098\n0.8663\n0.6869\n0.2575\n0.6638\n0.17300\nM\n0.85\n\n\n4\n20.290001\n14.340000\n135.100006\n1297.000000\n0.10030\n0.13280\n0.1980\n0.10430\n0.1809\n0.05883\n...\n152.199997\n1575.000000\n0.1374\n0.2050\n0.4000\n0.1625\n0.2364\n0.07678\nM\n1.00\n\n\n\n\n5 rows × 32 columns"
  },
  {
    "objectID": "projects/Breast_Cancer/03_Breast Cancer.html#save-model",
    "href": "projects/Breast_Cancer/03_Breast Cancer.html#save-model",
    "title": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(et, '../models/breast_cancer')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('label_encoding',\n                  TransformerWrapperWithInverse(exclude=None, include=None,\n                                                transformer=LabelEncoder())),\n                 ('numerical_imputer',\n                  TransformerWrapper(exclude=None,\n                                     include=['radius_mean', 'texture_mean',\n                                              'perimeter_mean', 'area_mean',\n                                              'smoothness_mean',\n                                              'compactness_mean',\n                                              'c...\n                  ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                       class_weight=None, criterion='gini',\n                                       max_depth=None, max_features='sqrt',\n                                       max_leaf_nodes=None, max_samples=None,\n                                       min_impurity_decrease=0.0,\n                                       min_samples_leaf=1, min_samples_split=2,\n                                       min_weight_fraction_leaf=0.0,\n                                       n_estimators=100, n_jobs=-1,\n                                       oob_score=False, random_state=123,\n                                       verbose=0, warm_start=False))],\n          verbose=False),\n '../models/breast_cancer.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/breast_cancer')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['radius_mean', 'texture_mean',\n                                             'perimeter_mean', 'area_mean',\n                                             'smoothness_mean',\n                                             'compactness_mean',\n                                             'c...\n                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                      class_weight=None, criterion='gini',\n                                      max_depth=None, max_features='sqrt',\n                                      max_leaf_nodes=None, max_samples=None,\n                                      min_impurity_decrease=0.0,\n                                      min_samples_leaf=1, min_samples_split=2,\n                                      min_weight_fraction_leaf=0.0,\n                                      n_estimators=100, n_jobs=-1,\n                                      oob_score=False, random_state=123,\n                                      verbose=0, warm_start=False))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['radius_mean', 'texture_mean',\n                                             'perimeter_mean', 'area_mean',\n                                             'smoothness_mean',\n                                             'compactness_mean',\n                                             'c...\n                 ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0,\n                                      class_weight=None, criterion='gini',\n                                      max_depth=None, max_features='sqrt',\n                                      max_leaf_nodes=None, max_samples=None,\n                                      min_impurity_decrease=0.0,\n                                      min_samples_leaf=1, min_samples_split=2,\n                                      min_weight_fraction_leaf=0.0,\n                                      n_estimators=100, n_jobs=-1,\n                                      oob_score=False, random_state=123,\n                                      verbose=0, warm_start=False))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['radius_mean', 'texture_mean', 'perimeter_mean',\n                            'area_mean', 'smoothness_mean', 'compactness_mean',\n                            'concavity_mean', 'concave points_mean',\n                            'symmetry_mean', 'fractal_dimension_mean',\n                            'radius_se', 'texture_se', 'perimeter_se',\n                            'area_se', 'smoothness_se', 'compactness_se',\n                            'concavity_se', 'concave points_se', 'symmetry_se',\n                            'fra...nsion_se', 'radius_worst',\n                            'texture_worst', 'perimeter_worst', 'area_worst',\n                            'smoothness_worst', 'compactness_worst',\n                            'concavity_worst', 'concave points_worst',\n                            'symmetry_worst', 'fractal_dimension_worst'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=[],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')clean_column_names: TransformerWrapperTransformerWrapper(exclude=None, include=None,\n                   transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()ExtraTreesClassifierExtraTreesClassifier(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "teaching/tcga-data-analysis-with-r/index.html",
    "href": "teaching/tcga-data-analysis-with-r/index.html",
    "title": "TCGA Data Analysis with R",
    "section": "",
    "text": "The TCGA Data Analysis with R course is designed to equip participants with the knowledge and skills necessary to effectively analyze and interpret data from The Cancer Genome Atlas (TCGA) using the R programming language. TCGA is a valuable resource for cancer researchers, providing comprehensive genomic and clinical data on various cancer types. This course will cover essential concepts, tools, and techniques for data preprocessing, exploratory data analysis, differential gene expression analysis, survival analysis, and data visualization using R. Participants will gain hands-on experience by working with real TCGA datasets and will learn to derive meaningful insights from complex cancer genomics data."
  },
  {
    "objectID": "teaching/tcga-data-analysis-with-r/index.html#course-description",
    "href": "teaching/tcga-data-analysis-with-r/index.html#course-description",
    "title": "TCGA Data Analysis with R",
    "section": "",
    "text": "The TCGA Data Analysis with R course is designed to equip participants with the knowledge and skills necessary to effectively analyze and interpret data from The Cancer Genome Atlas (TCGA) using the R programming language. TCGA is a valuable resource for cancer researchers, providing comprehensive genomic and clinical data on various cancer types. This course will cover essential concepts, tools, and techniques for data preprocessing, exploratory data analysis, differential gene expression analysis, survival analysis, and data visualization using R. Participants will gain hands-on experience by working with real TCGA datasets and will learn to derive meaningful insights from complex cancer genomics data."
  },
  {
    "objectID": "teaching/tcga-data-analysis-with-r/index.html#learning-objectives",
    "href": "teaching/tcga-data-analysis-with-r/index.html#learning-objectives",
    "title": "TCGA Data Analysis with R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduce you to the TCGA (The Cancer Genome Atlas) data available at the NCI’s Genomic Data Commons (GDC).\nDemonstrate how to access and import TCGA data using the R/Bioconductor package TCGAbiolinks (Colaprico et al. 2015).\nProvide instruction to visualize copy number alteration and mutation data using the R/Bioconductor package maftools."
  },
  {
    "objectID": "teaching/tcga-data-analysis-with-r/index.html#pre-requisites",
    "href": "teaching/tcga-data-analysis-with-r/index.html#pre-requisites",
    "title": "TCGA Data Analysis with R",
    "section": "Pre-requisites",
    "text": "Pre-requisites\n\nBasic knowledge of R syntax\nUnderstand the pipe operator (“%&gt;%”) (help material https://r4ds.had.co.nz/pipes.html)\nUnderstand the SummarizedExperiment data structure (help material http://bioconductor.org/packages/SummarizedExperiment/)"
  },
  {
    "objectID": "teaching/tcga-data-analysis-with-r/index.html#course-format",
    "href": "teaching/tcga-data-analysis-with-r/index.html#course-format",
    "title": "TCGA Data Analysis with R",
    "section": "Course Format",
    "text": "Course Format\nThe course will be delivered through a combination of lectures, hands-on practical sessions, and interactive discussions. Participants will have access to real TCGA datasets and will be guided through step-by-step analysis using R. Additionally, participants will be provided with relevant learning resources, including code examples and data repositories, to support their learning outside the course hours.\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/single-cell-seq-analysis-with-r/index.html",
    "href": "teaching/single-cell-seq-analysis-with-r/index.html",
    "title": "Single-Cell RNA-Seq Analysis with R",
    "section": "",
    "text": "The Single-Cell RNA-Seq Analysis with R course is designed for researchers and bioinformaticians interested in unlocking insights from single-cell transcriptomics data using the R programming language. Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular heterogeneity and gene expression dynamics. This course provides a comprehensive overview of scRNA-seq data analysis, covering preprocessing, quality control, dimensionality reduction, cell clustering, differential expression analysis, and visualization techniques."
  },
  {
    "objectID": "teaching/single-cell-seq-analysis-with-r/index.html#course-description",
    "href": "teaching/single-cell-seq-analysis-with-r/index.html#course-description",
    "title": "Single-Cell RNA-Seq Analysis with R",
    "section": "",
    "text": "The Single-Cell RNA-Seq Analysis with R course is designed for researchers and bioinformaticians interested in unlocking insights from single-cell transcriptomics data using the R programming language. Single-cell RNA sequencing (scRNA-seq) has revolutionized our understanding of cellular heterogeneity and gene expression dynamics. This course provides a comprehensive overview of scRNA-seq data analysis, covering preprocessing, quality control, dimensionality reduction, cell clustering, differential expression analysis, and visualization techniques."
  },
  {
    "objectID": "teaching/single-cell-seq-analysis-with-r/index.html#learning-objectives",
    "href": "teaching/single-cell-seq-analysis-with-r/index.html#learning-objectives",
    "title": "Single-Cell RNA-Seq Analysis with R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of the Single-Cell RNA-Seq Analysis with R course, participants will be able to:\n\nIntroduction to Single-Cell RNA-Seq\nPreprocessing and Quality Control\nDimensionality Reduction Techniques\nCell Clustering and Annotation\nDifferential Expression Analysis\nTrajectory Analysis and Pseudotime\nFunctional Enrichment Analysis\nData Visualization\nIntegration with Other Omics Data\nInterpreting Results and Reporting\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/r-for-research/index.html",
    "href": "teaching/r-for-research/index.html",
    "title": "R for Research",
    "section": "",
    "text": "The purpose of this course is to provide a practical introduction to the programming language R for researchers in any field. We are convinced that R is a powerful tool that can ultimately make your life easier by enabling efficient solutions to your data analytical problems. Also, R is completely free. Who doesn’t like that?\nThis course aims to facilitate the participant’s first steps in R and equip them with the tools and understanding to expand their technical know-how according to the needs of their specific research. It is neither a computer science nor a methodological course, but aimed at the practical needs of empirically working researchers.\n\n\nBy the end of the course students you shall be confident and equipped with all the knowledge required to perform analytical activities in R. Specifically,\n\nUnderstand the fundamental syntax of R through readings, practice exercises, demonstrations, and writing R code.\nApply critical programming language concepts such as data types, iteration, control structures, functions, and boolean operators by writing R programs and through examples\nImport a variety of data formats into R using RStudio\nPrepare or tidy data for in preparation for analysis\nQuery data using SQL and R\nAnalyze a data set in R and present findings using the appropriate R packages\nVisualize data attributes using ggplot2 and other R packages.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/r-for-research/index.html#learning-outcomes",
    "href": "teaching/r-for-research/index.html#learning-outcomes",
    "title": "R for Research",
    "section": "",
    "text": "By the end of the course students you shall be confident and equipped with all the knowledge required to perform analytical activities in R. Specifically,\n\nUnderstand the fundamental syntax of R through readings, practice exercises, demonstrations, and writing R code.\nApply critical programming language concepts such as data types, iteration, control structures, functions, and boolean operators by writing R programs and through examples\nImport a variety of data formats into R using RStudio\nPrepare or tidy data for in preparation for analysis\nQuery data using SQL and R\nAnalyze a data set in R and present findings using the appropriate R packages\nVisualize data attributes using ggplot2 and other R packages.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/python-for-health-data-analysis/index.html",
    "href": "teaching/python-for-health-data-analysis/index.html",
    "title": "Python for Health Data Analysis",
    "section": "",
    "text": "This comprehensive 12-week course was designed to equip participants with essential skills to analyze health data using Python. Health data analysis is a critical aspect of modern healthcare, enabling evidence- based decision making and insights for improving patient outcomes and public health initiatives.\nThroughout the course, students will learn how to work with real-world health datasets, implement data analysis techniques, and visualize results using powerful Python data science libraries. The course is designed for both beginners in Python and individuals with prior programming experience in other languages."
  },
  {
    "objectID": "teaching/python-for-health-data-analysis/index.html#overview",
    "href": "teaching/python-for-health-data-analysis/index.html#overview",
    "title": "Python for Health Data Analysis",
    "section": "",
    "text": "This comprehensive 12-week course was designed to equip participants with essential skills to analyze health data using Python. Health data analysis is a critical aspect of modern healthcare, enabling evidence- based decision making and insights for improving patient outcomes and public health initiatives.\nThroughout the course, students will learn how to work with real-world health datasets, implement data analysis techniques, and visualize results using powerful Python data science libraries. The course is designed for both beginners in Python and individuals with prior programming experience in other languages."
  },
  {
    "objectID": "teaching/python-for-health-data-analysis/index.html#learning-objectives",
    "href": "teaching/python-for-health-data-analysis/index.html#learning-objectives",
    "title": "Python for Health Data Analysis",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this course, participants will be able to:\n\nUnderstanding the importance of health data analysis and its role in healthcare decision making\nPython libraries (Pandas, NumPy, and Matplotlib) were used to import, manipulate, and visualize health-related datasets.\nExploratory data analysis techniques were applied to gain insights and identify patterns in health data.\nPreprocess health data, handle missing values, and ensure data quality for machine learning applications.\nImplementation of basic machine-learning algorithms for predictive modeling.\nEvaluate machine learning model performance using evaluation metrics\nPerform model optimization and hyperparameter tuning to improve the machine learning model performance.\nExplore ethical considerations and challenges related to machine learning in the healthcare domain."
  },
  {
    "objectID": "teaching/python-for-health-data-analysis/index.html#prerequisites",
    "href": "teaching/python-for-health-data-analysis/index.html#prerequisites",
    "title": "Python for Health Data Analysis",
    "section": "Prerequisites",
    "text": "Prerequisites\nThere are no specific prerequisites for this course. However, basic familiarity with programming concepts and Python syntax is beneficial. Participants with no prior Python experience were encouraged to review the fundamentals of Python programming prior to starting the course."
  },
  {
    "objectID": "teaching/python-for-health-data-analysis/index.html#organizer",
    "href": "teaching/python-for-health-data-analysis/index.html#organizer",
    "title": "Python for Health Data Analysis",
    "section": "Organizer",
    "text": "Organizer\nThe Center for Bioinformatics Learning Advancement and Systematics Training(cBLAST) under the authority of the University of Dhaka was established in October, 2014. The Center was established in view of the importance of ‘Bioinformatics’ for solving biological problems encountered during pursuit of most of the Life Sciences subjects. The applicability of Bioinformatics for managing biological data was appreciated and the Center was created as a platform for understanding and learning it. Dr. Zeba I. Seraj was appointed as the Director of the Center to serve a term of 3 years from January 1, 2015 and further re-appointed as the Director of the Center from January 1, 2018 for last 3 years. The center is managed by an Administrative Council of 13 members, chaired by the Honorable Vice Chancellor of the University of Dhaka. Positional members are the Dean of Biological Sciences and the Chair of the BMB Department. In addition, there are 5 more members from BMBDU, one from GEBDU, one from ISRT, DU, one from IIT, DU and one from industry.\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/ml-for-bioinformatics/index.html",
    "href": "teaching/ml-for-bioinformatics/index.html",
    "title": "Machine Learning for Bioinformatics with Python",
    "section": "",
    "text": "The “Machine Learning for Bioinformatics with Python” course is designed to provide participants with a solid foundation in applying machine learning techniques to solve complex problems in bioinformatics. Through hands-on exercises and practical examples, participants will learn how to leverage Python’s powerful libraries for machine learning to analyze biological data, make predictions, and extract meaningful insights. This course bridges the gap between bioinformatics and machine learning, enabling participants to develop skills at the intersection of these fields."
  },
  {
    "objectID": "teaching/ml-for-bioinformatics/index.html#overview",
    "href": "teaching/ml-for-bioinformatics/index.html#overview",
    "title": "Machine Learning for Bioinformatics with Python",
    "section": "",
    "text": "The “Machine Learning for Bioinformatics with Python” course is designed to provide participants with a solid foundation in applying machine learning techniques to solve complex problems in bioinformatics. Through hands-on exercises and practical examples, participants will learn how to leverage Python’s powerful libraries for machine learning to analyze biological data, make predictions, and extract meaningful insights. This course bridges the gap between bioinformatics and machine learning, enabling participants to develop skills at the intersection of these fields."
  },
  {
    "objectID": "teaching/ml-for-bioinformatics/index.html#learning-objectives",
    "href": "teaching/ml-for-bioinformatics/index.html#learning-objectives",
    "title": "Machine Learning for Bioinformatics with Python",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAt the end of the tutorial, participants will be able to:\n\nUnderstand the Basics of Machine Learning in Bioinformatics\nPreprocess Biological Data for Machine Learning\nExplore Feature Selection and Dimensionality Reduction\nImplement Classification Algorithms\nPerform Regression Analysis on Biological Data\nUse Clustering Techniques for Pattern Discovery\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/data-analysis-with-r/index.html",
    "href": "teaching/data-analysis-with-r/index.html",
    "title": "Data Analysis with R",
    "section": "",
    "text": "The purpose of this course is to provide a practical introduction to the programming language R for researchers in any field. We are convinced that R is a powerful tool that can ultimately make your life easier by enabling efficient solutions to your data analytical problems. Also, R is completely free. Who doesn’t like that?\nThis course aims to facilitate the participant’s first steps in R and equip them with the tools and understanding to expand their technical know-how according to the needs of their specific research. It is neither a computer science nor a methodological course, but aimed at the practical needs of empirically working researchers.\nYou might think of R as an overly complicated status symbol for ambitious quantitative researchers. This is not quite true anymore as R has become much more user-friendly over the last years. Think of R is your friend and helper if you are interested in doing: insightful graphs, regression analysis, QCA, quantitative text analysis, web-scraping, data visualization, systematic hand-coding of documents, formal modelling, etc.\nWhile this course is most likely to attract people with solid statistical training who usually use Stata or SPSS, it can easily be attended by a broader audience. All you really need to know is [I] the structure of a typical dataset in the social sciences (rows are observations, columns are variables) and [II] the logic of dependent and independent variables.\nYou will learn how to use R to handle your data, describe it, analyse it (regressions), and transform it into beautiful graphs. At the end, you will have an idea of what R can do, and the resources to teach yourself more where needed."
  },
  {
    "objectID": "teaching/data-analysis-with-r/index.html#overview",
    "href": "teaching/data-analysis-with-r/index.html#overview",
    "title": "Data Analysis with R",
    "section": "",
    "text": "The purpose of this course is to provide a practical introduction to the programming language R for researchers in any field. We are convinced that R is a powerful tool that can ultimately make your life easier by enabling efficient solutions to your data analytical problems. Also, R is completely free. Who doesn’t like that?\nThis course aims to facilitate the participant’s first steps in R and equip them with the tools and understanding to expand their technical know-how according to the needs of their specific research. It is neither a computer science nor a methodological course, but aimed at the practical needs of empirically working researchers.\nYou might think of R as an overly complicated status symbol for ambitious quantitative researchers. This is not quite true anymore as R has become much more user-friendly over the last years. Think of R is your friend and helper if you are interested in doing: insightful graphs, regression analysis, QCA, quantitative text analysis, web-scraping, data visualization, systematic hand-coding of documents, formal modelling, etc.\nWhile this course is most likely to attract people with solid statistical training who usually use Stata or SPSS, it can easily be attended by a broader audience. All you really need to know is [I] the structure of a typical dataset in the social sciences (rows are observations, columns are variables) and [II] the logic of dependent and independent variables.\nYou will learn how to use R to handle your data, describe it, analyse it (regressions), and transform it into beautiful graphs. At the end, you will have an idea of what R can do, and the resources to teach yourself more where needed."
  },
  {
    "objectID": "teaching/data-analysis-with-r/index.html#learning-outcomes",
    "href": "teaching/data-analysis-with-r/index.html#learning-outcomes",
    "title": "Data Analysis with R",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nBy the end of the course students you shall be confident and equipped with all the knowledge required to perform analytical activities in R. Specifically,\n\nUnderstand the fundamental syntax of R through readings, practice exercises, demonstrations, and writing R code.\nApply critical programming language concepts such as data types, iteration, control structures, functions, and boolean operators by writing R programs and through examples\nImport a variety of data formats into R using RStudio\nPrepare or tidy datas for in preparation for analysis\nQuery data using SQL and R\nAnalyze a data set in R and present findings using the appropriate R packages\nVisualize data attributes using ggplot2 and other R packages.\n\nWebsite\n\nApply to the Program"
  },
  {
    "objectID": "teaching/bioinformatics-bootcamp/index.html",
    "href": "teaching/bioinformatics-bootcamp/index.html",
    "title": "Child Health Research Foundation (CHRF)",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/bioinformatics-bootcamp/index.html#bootcamp-description",
    "href": "teaching/bioinformatics-bootcamp/index.html#bootcamp-description",
    "title": "Child Health Research Foundation (CHRF)",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/bioinformatics-bootcamp/index.html#learning-objectives",
    "href": "teaching/bioinformatics-bootcamp/index.html#learning-objectives",
    "title": "Child Health Research Foundation (CHRF)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of the Bioinformatics BootCamp, participants will be able to:\n\nUnderstand Bioinformatics Fundamentals\nPerform Sequence Analysis\nExplore Structural Bioinformatics\nUtilize Bioinformatics Databases and Resources\nApply Data Visualization Techniques\nConduct Statistical Analysis in Bioinformatics\nSolve Bioinformatics Challenges\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "",
    "section": "",
    "text": "AI in Public Health Workshop\n\n\n\n\n\nTeaching Assistant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBacterial Genomics & Antimicrobial Resistance Workshop\n\n\n\n\n\nTeaching Assistant\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBioinformatics Division, National Institute of Biotechnology (NIB)\n\n\n\n\n\nTeaching Assistant\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/state-the-art-of-microbial-genome-analysi/index.html",
    "href": "talks/state-the-art-of-microbial-genome-analysi/index.html",
    "title": "Industrial Training (Academic)",
    "section": "",
    "text": "Genomics is the study of whole genomes of organisms, and incorporates elements from ge- netics. Genomics uses a combination of recombinant DNA, DNA sequencing methods, and bioinformatics to sequence, assemble, and analyse the structure and function of genomes. Genomics have become an inter-disciplinary(Computer Science, Statistics, Biology) sci- ence. The genomic data analysis steps typically include data collection, quality check and cleaning, processing, modeling, visualization and reporting.\n\nNational Forensic DNA Profiling Laboratory (NFDPL) - Dhaka\nBangladesh Council of Scientific and Industrial Research(BCSIR) - Dhaka\nDelta Pharma Ltd. Kishoreganj, Bangladesh"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "(Academic) career related essays\nA collection of resources on (scientific) career planning from the Nature Publishing Group.\nA resource from the NIH toward Broadening Experiences in Scientific Training\nA good list of books on ‘learning how to learn’\n\n\n\n\n\nmyIDP from AAAS. Free IDP focused on scientific career planning.\n\n\n\n\n\nThoughts on doing a postdoc or not\n\n\n\n\n\nInsight Data Science arranges short internships with some of the big names.\n\n\n\n\n\nThis blog post provides a nice description how applications for faculty positions are reviewed. From my own experience having been on and chaired several faculty searches, I think it’s a fairly accurate description.\n\n\n\n\n\nTips to have a productive PhD"
  },
  {
    "objectID": "resources.html#general",
    "href": "resources.html#general",
    "title": "Resources",
    "section": "",
    "text": "(Academic) career related essays\nA collection of resources on (scientific) career planning from the Nature Publishing Group.\nA resource from the NIH toward Broadening Experiences in Scientific Training\nA good list of books on ‘learning how to learn’"
  },
  {
    "objectID": "resources.html#individual-development-plan-idp",
    "href": "resources.html#individual-development-plan-idp",
    "title": "Resources",
    "section": "",
    "text": "myIDP from AAAS. Free IDP focused on scientific career planning."
  },
  {
    "objectID": "resources.html#postdoc-related",
    "href": "resources.html#postdoc-related",
    "title": "Resources",
    "section": "",
    "text": "Thoughts on doing a postdoc or not"
  },
  {
    "objectID": "resources.html#data-science-internships",
    "href": "resources.html#data-science-internships",
    "title": "Resources",
    "section": "",
    "text": "Insight Data Science arranges short internships with some of the big names."
  },
  {
    "objectID": "resources.html#academic-job-searchhiring",
    "href": "resources.html#academic-job-searchhiring",
    "title": "Resources",
    "section": "",
    "text": "This blog post provides a nice description how applications for faculty positions are reviewed. From my own experience having been on and chaired several faculty searches, I think it’s a fairly accurate description."
  },
  {
    "objectID": "resources.html#academic-career",
    "href": "resources.html#academic-career",
    "title": "Resources",
    "section": "",
    "text": "Tips to have a productive PhD"
  },
  {
    "objectID": "resources.html#software",
    "href": "resources.html#software",
    "title": "Resources",
    "section": "Software",
    "text": "Software\n\nlearnr - R package that allows development of interactive web-based R tutorials.\nFeedback at scale - tutorial for using learnr and gradethis as teaching tools."
  },
  {
    "objectID": "resources.html#books",
    "href": "resources.html#books",
    "title": "Resources",
    "section": "Books",
    "text": "Books\n\nrstudio4edu - A Handbook for Teaching and Learning with R and RStudio - still very much a draft."
  },
  {
    "objectID": "resources.html#online-resources",
    "href": "resources.html#online-resources",
    "title": "Resources",
    "section": "Online Resources",
    "text": "Online Resources\n\nTeaching Statistics and Data Science Online - materials for several teacher workshops taught by Mine Çetinkaya-Rundel."
  },
  {
    "objectID": "resources.html#other",
    "href": "resources.html#other",
    "title": "Resources",
    "section": "Other",
    "text": "Other\n\nBioicons - a collection of free drawings and diagrams on biological topics, which can be used in teaching (or research) presentations."
  },
  {
    "objectID": "resources.html#r-programming",
    "href": "resources.html#r-programming",
    "title": "Resources",
    "section": "R Programming",
    "text": "R Programming"
  },
  {
    "objectID": "resources.html#lists-by-others",
    "href": "resources.html#lists-by-others",
    "title": "Resources",
    "section": "Lists by others",
    "text": "Lists by others\n\n[A list of the best R tools] (https://github.com/qinwf/awesome-R)"
  },
  {
    "objectID": "resources.html#online-courses-for-r",
    "href": "resources.html#online-courses-for-r",
    "title": "Resources",
    "section": "Online Courses for R",
    "text": "Online Courses for R\n\nSwirl - An R package that teaches R interactively inside R. Not as polished as DataCamp, but good interactivity, great motivating feedback/messages, beginner and intermediate levels. Completely free.\nTry R, a free course at codeschool.com"
  },
  {
    "objectID": "resources.html#r-books-and-tutorials",
    "href": "resources.html#r-books-and-tutorials",
    "title": "Resources",
    "section": "R books and Tutorials",
    "text": "R books and Tutorials\n\nYaRrr! The Pirate’s Guide to R: A nice R introduction with a piratey bend\nComputerworld’s R beginner guide\n‘R Meta book’ - a list of resources on specific topics:\nSpringer has a series of books called ‘Use R’ - A long and growing series of books on a lot of different R topics. From beginner to advanced. Many university libraries have a subscription to Springer Online, which allows you to get the books for free.\nRoger Peng’s “R programming for data science” book on Leanpub. Name your own price.\n“Advanced R” by Hadley Wickham for more advanced R coding"
  },
  {
    "objectID": "resources.html#general-r-online-resources",
    "href": "resources.html#general-r-online-resources",
    "title": "Resources",
    "section": "General R Online Resources",
    "text": "General R Online Resources\n\nQuick R - a great resource for many ‘how do I’ questions of common data analysis and graphics tasks.\nCookbook for R - another useful collection of ‘how do I do X’ information.\nR bloggers an aggregation of blogs about all things R\nConvenient reference/cheat sheets for some important R tasks\nResource collection to do useful data analysis related tasks in R\nR Documentation: - a webpage and R package that tries to enhance the help/documentation files for R and its packages.\nThe Task View sorting R packages into categories is often useful"
  },
  {
    "objectID": "resources.html#r-and-shiny",
    "href": "resources.html#r-and-shiny",
    "title": "Resources",
    "section": "R and Shiny",
    "text": "R and Shiny\n\nOfficial shiny webpage\nCollection of Shiny tips and tricks, see also here and here\nEngineering Production-Grade Shiny Apps\nOutstanding User Interfaces with Shiny - a book covering advanced Shiny material, covering advanced styling, mobile development, integration with other web technologies, etc."
  },
  {
    "objectID": "resources.html#r-markdown-and-related",
    "href": "resources.html#r-markdown-and-related",
    "title": "Resources",
    "section": "R Markdown and related",
    "text": "R Markdown and related\n\nCollection of R Markdown Tips and Tricks"
  },
  {
    "objectID": "resources.html#r-package-development",
    "href": "resources.html#r-package-development",
    "title": "Resources",
    "section": "R package development",
    "text": "R package development\n\n“R packages” by Hadley Wickham. This (free online) book alone is enough to get a package done. Should definitely be the first (and maybe even only) source to read/use.\nHow to quickly write an R package\nA series of online videos talking about package development\ndevtools, roxygen2, etc. - almost required for development. See Hadley’s book.\navailable - tests if a specific name is still available\nusethis - helps with a lot of setup tasks for packages (and other R coding tasks)"
  },
  {
    "objectID": "resources.html#other-lists",
    "href": "resources.html#other-lists",
    "title": "Resources",
    "section": "Other lists",
    "text": "Other lists\n\nAwesome data science - Someone else’s list of resources related to data science.\nMachine learning book list - A list of free online books concerning all things machine learning and data analysis.\nA machine learning list of terms and definitions."
  },
  {
    "objectID": "resources.html#data-cleaning-and-checking",
    "href": "resources.html#data-cleaning-and-checking",
    "title": "Resources",
    "section": "Data cleaning and checking",
    "text": "Data cleaning and checking\n\nCheck out the R packages tidyr, plyr, dplyr, validate, vtreat (you can find good documentation for all of them)\nData Analysis and Visualization Using R\nA nice series of blog posts describing common data manipulations for dplyr"
  },
  {
    "objectID": "resources.html#visualization",
    "href": "resources.html#visualization",
    "title": "Resources",
    "section": "Visualization",
    "text": "Visualization\n\nR Graph Gallery: Has lots of examples of graphs and plots in R, including the underlying code that produced the graph. Great place to find a graph similar to one you’d like to make, take the code and adjust it."
  },
  {
    "objectID": "resources.html#missing-dataimputation",
    "href": "resources.html#missing-dataimputation",
    "title": "Resources",
    "section": "Missing Data/Imputation",
    "text": "Missing Data/Imputation\n\nCheck out R packages mi, mice, Amelia, missforest, imputation\nLittle & Rubin “Statistical Analysis with Missing Data, 2nd Edition”"
  },
  {
    "objectID": "resources.html#p-values",
    "href": "resources.html#p-values",
    "title": "Resources",
    "section": "p-values",
    "text": "p-values\n\nNice discussion of p-values\nAnother stab at p-values\nAnother good discussion of the problem with p-values"
  },
  {
    "objectID": "resources.html#epidemiology-specific-topics",
    "href": "resources.html#epidemiology-specific-topics",
    "title": "Resources",
    "section": "Epidemiology specific topics",
    "text": "Epidemiology specific topics\n\ntableone - A package that allows for convenient creation of a descriptive table based on a data frame.\ncompareGroups - A package that allows for convenient creation of result tables from linear and logistic (and related) models.\nThe R survival package - Lots of functionality for survival analysis."
  },
  {
    "objectID": "resources.html#time-series-analysis",
    "href": "resources.html#time-series-analysis",
    "title": "Resources",
    "section": "Time-series analysis",
    "text": "Time-series analysis\n\nThe R survival package - Lots of functionality for survival analysis.\nForecasting: principles and practice"
  },
  {
    "objectID": "resources.html#general-data-related-resources",
    "href": "resources.html#general-data-related-resources",
    "title": "Resources",
    "section": "General Data Related Resources",
    "text": "General Data Related Resources\n\nThe Project Open Data Dashboard gives overview statistics of available government data from various agencies.\nGuide to Open Data Publishing & Analytics - A good article describing best practices for publishing data openly. Is also a good read for those who want to analyze other’s data.\nA short list of data related R packages - packages that either access data or include data\nGoogle’s Data Search Engine"
  },
  {
    "objectID": "resources.html#some-data-sources",
    "href": "resources.html#some-data-sources",
    "title": "Resources",
    "section": "Some data Sources",
    "text": "Some data Sources\n(listed in no particular order)\n\nKaggle Data - A growing number of datasets used in Kaggle data analysis contests and available for any other use.\nNasdaq Data Link - mainly finance related data\nNHANES - longstanding and thorough survey done by CDC\nSEER - Cancer data\nCDC WONDER - list of mainly CDC online databases\nHealthy People Website - contains among other things links to various data sources\nHCUP - collection of health related databases, focusing on US wide and state-specific samples of ER and hospital visits. Not free, but not too expensive.\nClinical Study Data Request - a way to get (tedious) access to clinical trial data\nEMA Clinical Data Portal - looks like a way to get access to some clinical trial data for EMA registered studies.\nMIMIC - a free and open database of critical care patient visits to a Boston hospital.\nData.gov - federak government data platform.\nAnalyze Survey Data for Free - Step by Step Instructions to Explore Public Microdata from an Easy to Type Website\nVaccine Safety Datalink (VSD)\nGlobal Health Data Exchange / IHME\nVaccine Adverse Event Reporting System (VAERS)\nNational EMS Data (NEMSIS)\nData from the UN\nInter-university Consortium for Political and Social Research (ICPSR) - access to various social and behavioral sciences data.\nFederal Reserve in Atlanta Data Center\nDatabase to search health related government data\nA list hosted by Microsoft with links to various data sources"
  },
  {
    "objectID": "resources.html#infectious-disease-specific",
    "href": "resources.html#infectious-disease-specific",
    "title": "Resources",
    "section": "Infectious Disease Specific",
    "text": "Infectious Disease Specific\n\nGeneral\n\nProject Tycho - infectious disease data\nhttp://www.viprbrc.org\nhttp://eupathdb.org\nClinEpiDB - a database of (a few) clinical epidemiology studies, focusing on infectious diseases.\nImmPort\n\n\n\nInfluenza\n\nhttp://www.fludb.org\nWHO Flunet\ngisaid.org\nhttp://www.cdc.gov/flu/index.htm\nhttp://www.ncbi.nlm.nih.gov/genomes/FLU/FLU.html\nSystemsInfluenza.org\n\n\n\nTB\n\nOTIS on CDC WONDER: http://wonder.cdc.gov/tb.html"
  },
  {
    "objectID": "resources.html#data",
    "href": "resources.html#data",
    "title": "Resources",
    "section": "Data",
    "text": "Data\n\nThe Cancer Genome Atlas (TCGA): TCGA is a comprehensive collection of multi-dimensional cancer genomics data covering multiple cancer types.\nInternational Cancer Genome Consortium (ICGC): Description: ICGC provides high-quality genomic and clinical data from various cancer projects worldwide.\nGene Expression Omnibus (GEO): GEO is a public repository hosted by the National Center for Biotechnology Information (NCBI) containing a vast collection of gene expression data, including cancer datasets.\nEuropean Genome-phenome Archive (EGA): Description: EGA is a repository for secure storage and sharing of human genetic and phenotypic data, including cancer datasets.\nNational Cancer Institute (NCI) Genomic Data Commons (GDC): Description: GDC is an open-access data portal providing access to a wide range of cancer genomics datasets.\nOncoLnc: Description: OncoLnc is a web resource that provides survival analysis and expression correlation for genes of interest across multiple cancer datasets.\nUCSC Cancer Genomics Browser: The UCSC Cancer Genomics Browser offers a comprehensive collection of cancer genomics data integrated with genomic annotations.\nGREIN : GEO RNA-seq Experiments Interactive Navigator: GREIN is an interactive web platform that provides user-friendly options to explore and analyze GEO RNA-seq data. GREIN is powered by the back-end computational pipeline for uniform processing of RNA-seq data and the large number (&gt;6,000) of already processed datasets. These datasets were retrieved from GEO and reprocessed consistently by the back-end GEO RNA-seq experiments processing pipeline (GREP2)."
  },
  {
    "objectID": "resources.html#tools",
    "href": "resources.html#tools",
    "title": "Resources",
    "section": "Tools",
    "text": "Tools\n\nGEPIA2: GEPIA2 is a web-based tool for analyzing gene expression data in cancer. It stands for Gene Expression Profiling Interactive Analysis 2 and is an updated version of the original GEPIA tool. GEPIA2 allows users to explore gene expression patterns, perform survival analyses, and visualize gene expression data across various cancer types.\nUALCAN: UALCAN is a web-based platform that provides interactive and comprehensive analysis of cancer transcriptome data. It enables users to explore gene expression patterns, perform survival analyses, and compare gene expression between tumor and normal samples across different cancer types. UALCAN utilizes data from The Cancer Genome Atlas (TCGA) to facilitate cancer research and provide insights into tumor biology.\ncBioPortal for Cancer Genomics:: cBioPortal hosts a large collection of cancer genomics datasets, allowing users to explore and visualize the data.\nONCOMINE: ONCOMINE is a powerful web-based platform for the analysis and visualization of cancer transcriptomic data. It provides researchers with access to a vast collection of publicly available gene expression datasets derived from cancer studies. ONCOMINE allows users to explore gene expression patterns, identify potential biomarkers, and compare gene expression between different cancer types or subtypes."
  },
  {
    "objectID": "resources.html#guideline-for-bioconductor-users",
    "href": "resources.html#guideline-for-bioconductor-users",
    "title": "Resources",
    "section": "Guideline for Bioconductor Users",
    "text": "Guideline for Bioconductor Users\nBioconductor is an open-source and open-development software project that provides a comprehensive collection of bioinformatics and computational biology tools in the R programming language. It focuses on the analysis and comprehension of high-throughput genomic data, including DNA sequencing, RNA sequencing, microarray analysis, proteomics, and more."
  },
  {
    "objectID": "resources.html#required-software",
    "href": "resources.html#required-software",
    "title": "Resources",
    "section": "Required software",
    "text": "Required software\n\nR: http://www.r-project.org/ (FREE)\nRStudio (additional libraries required): http://www.rstudio.com/ (FREE)"
  },
  {
    "objectID": "resources.html#prework",
    "href": "resources.html#prework",
    "title": "Resources",
    "section": "Prework",
    "text": "Prework\nBefore attending the any workshop please have the following installed and configured on your machine. - Recent version of R - Recent version of RStudio - Most recent release of the Bioconductor and other packages used in courses\nInstall the latest release of R, then get the latest version of Bioconductor by starting R and entering the commands.\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install(version = \"3.16\")\n\n\nEnsure you can knit R markdown documents\n\nOpen RStudio and create a new Rmarkdown document\nSave the document and check you are able to knit it."
  },
  {
    "objectID": "resources.html#install-bioconductor-packages",
    "href": "resources.html#install-bioconductor-packages",
    "title": "Resources",
    "section": "Install Bioconductor Packages",
    "text": "Install Bioconductor Packages\n\nif (!require(\"BiocManager\", quietly = TRUE))\n    install.packages(\"BiocManager\")\nBiocManager::install()\n\nInstall specific packages, e.g., “GenomicFeatures” and “AnnotationDbi”, with\n\nBiocManager::install(c(\"GenomicFeatures\", \"AnnotationDbi\"))\n\nThe install() function (in the BiocManager package) has arguments that change its default behavior; type ?install for further help."
  },
  {
    "objectID": "resources.html#r-packages-rnaseq-and-single-cell-rna-seq-analysis",
    "href": "resources.html#r-packages-rnaseq-and-single-cell-rna-seq-analysis",
    "title": "Resources",
    "section": "R Packages RNASeq and Single-cell RNA-seq Analysis",
    "text": "R Packages RNASeq and Single-cell RNA-seq Analysis\n\nDESeq2: DESeq2 is a widely used package for differential gene expression analysis in RNA-seq data.\nedgeR: edgeR is another popular package for differential gene expression analysis in RNA-seq data.\nlimma: limma is a package commonly used for the analysis of microarray and RNA-seq data, particularly for differential expression analysis.\nBallgown: Ballgown is a package for differential expression analysis and visualization of transcriptome assembly data.\nDEXSeq: DEXSeq is specifically designed for the detection of differential exon usage in RNA-seq data.\nNOISeq: NOISeq is a package for non-parametric analysis of differential expression in RNA-seq data.\nclusterProfiler: clusterProfiler is a package for functional enrichment analysis of gene clusters derived from RNA-seq data.\nGenomicFeatures: GenomicFeatures provides tools for working with genomic features, such as gene models, and is useful for annotating RNA-seq data.\nSeurat: Seurat is a package for single-cell RNA-seq data analysis, allowing exploration and visualization of cellular heterogeneity."
  },
  {
    "objectID": "resources.html#blogs-for-r-programming-statistics-and-data-analyis",
    "href": "resources.html#blogs-for-r-programming-statistics-and-data-analyis",
    "title": "Resources",
    "section": "Blogs for R Programming, Statistics, and Data Analyis",
    "text": "Blogs for R Programming, Statistics, and Data Analyis\n\nProgramiz - https://www.datamentor.io/r-programming/\nPennState STAT 484 - https://online.stat.psu.edu/stat484/\nPennState Topics in R Statistical Language - https://online.stat.psu.edu/stat484/\nSimply Statistics - https://simplystatistics.org/\nTutorialPoint - https://www.tutorialspoint.com/r/index.htm\nR for Biologists - https://www.rforbiologists.org/\nComputational Genomics with R - https://compgenomr.github.io/book/\nStat and R - https://statsandr.com/\nRafa Lab - https://rafalab.github.io/pages/harvardx.html\nUniversity of Florida - https://bolt.mph.ufl.edu/software/r-phc-6055/"
  },
  {
    "objectID": "resources.html#videos",
    "href": "resources.html#videos",
    "title": "Resources",
    "section": "Videos",
    "text": "Videos\n\nJuly 9, 2023: I’m thrilled to announce that the highly anticipated videos from our workshop on “R for Research: Fundamentals of R - Part 1” are now available for viewing. Whether you missed the live event or want to revisit the valuable insights shared during the session, these videos are your gateway to mastering the basics of R programming language for data analysis and research. Join us as we explore the foundations of R and learn essential skills to enhance your data analysis capabilities. Don’t wait any longer; dive into the videos today and take your research to new heights! Check it out: \nApril 12, 2023: Watch this informative 2-hour workshop on how NASA Earth Observing Data can help improve public health. Discover how we can use these data to monitor our environment and identify potential health risks. Learn about the different ways NASA Earth Observing Data can benefit our communities and keep us safe. Check it out:"
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_03.html",
    "href": "publications/working-papers/shahnaj2023_03.html",
    "title": "In Silico Structural and Functional Prediction of Phaseolus Vulgaris Hypothetical Protein PHA VU_004G136400g",
    "section": "",
    "text": "In bacterial genomes, there are open reading frames (ORFs) that encode protein sequences without known functions or homologies to other proteins. These ORFs are often annotated as hypothetical proteins or conserved hypothetical proteins. These designations indicate that their functions are not yet understood or that their predicted functions are based on computational analysis rather than experimental evidence. Helicobacter pylori has been shown to induce chronic active gastritis and peptic ulcer and may contribute to the development of duodenal ulcer. Helicobacter pylori (H. pylori) is a Gram-negative bacterium that colonizes the human stomach. This bacterium is associated with various gastrointestinal disorders, including gastritis, peptic ulcers, gastric adenocarcinoma (stomach cancer), and gastric mucosa-associated lymphoid tissue (MALT) lymphoma. This study aims to investigate the structural and functional annotation of hypothetical protein MPF87_06725, which is basically a CagE domain containing protein encode a type IV transporter secretion system essential for pathogenesis in H. pylori induced gastritis and peptic ulceration. The in-silico approach included methods including physiochemical properties, secondary structure prediction, homology modeling, quality assessment of the 3D structure, protein-protein interaction, functional annotation and comparative genomics."
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_03.html#abstract",
    "href": "publications/working-papers/shahnaj2023_03.html#abstract",
    "title": "In Silico Structural and Functional Prediction of Phaseolus Vulgaris Hypothetical Protein PHA VU_004G136400g",
    "section": "",
    "text": "In bacterial genomes, there are open reading frames (ORFs) that encode protein sequences without known functions or homologies to other proteins. These ORFs are often annotated as hypothetical proteins or conserved hypothetical proteins. These designations indicate that their functions are not yet understood or that their predicted functions are based on computational analysis rather than experimental evidence. Helicobacter pylori has been shown to induce chronic active gastritis and peptic ulcer and may contribute to the development of duodenal ulcer. Helicobacter pylori (H. pylori) is a Gram-negative bacterium that colonizes the human stomach. This bacterium is associated with various gastrointestinal disorders, including gastritis, peptic ulcers, gastric adenocarcinoma (stomach cancer), and gastric mucosa-associated lymphoid tissue (MALT) lymphoma. This study aims to investigate the structural and functional annotation of hypothetical protein MPF87_06725, which is basically a CagE domain containing protein encode a type IV transporter secretion system essential for pathogenesis in H. pylori induced gastritis and peptic ulceration. The in-silico approach included methods including physiochemical properties, secondary structure prediction, homology modeling, quality assessment of the 3D structure, protein-protein interaction, functional annotation and comparative genomics."
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_01.html",
    "href": "publications/working-papers/shahnaj2023_01.html",
    "title": "In-Silico Structural and Functional Annotation of Hypothetical Glycosyltransferase Protein",
    "section": "",
    "text": "Glycosyltransferases (GTs) are a diverse group of enzymes involved in the biosynthesis of glycoconjugates, which play critical roles in cellular processes and disease mechanisms. However, a significant number of GTs remain uncharacterized, including many hypothetical proteins with putative GT activity. In this study, we employed in-silico methods for the structural and functional annotation of a hypothetical glycosyltransferase protein.\nUsing sequence homology-based approaches, we identified potential protein templates for comparative modeling. The three-dimensional structure of the hypothetical GT protein was predicted through homology modeling using Modeller software. The generated model was refined and validated using various assessment tools, including Ramachandran plots and ProSA-web analysis.\nFunctional annotation of the hypothetical GT protein was performed by analyzing conserved domains and motifs using InterProScan, Pfam, and PROSITE databases. In addition, we used sequence alignment tools to identify functionally important residues and predict potential donor and acceptor substrate binding sites.\nFurthermore, molecular docking simulations were carried out to predict the potential interactions between the modeled GT protein and potential substrates. Docking studies provided insights into the putative binding modes and key residues involved in substrate recognition and catalytic activity."
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_01.html#abstract",
    "href": "publications/working-papers/shahnaj2023_01.html#abstract",
    "title": "In-Silico Structural and Functional Annotation of Hypothetical Glycosyltransferase Protein",
    "section": "",
    "text": "Glycosyltransferases (GTs) are a diverse group of enzymes involved in the biosynthesis of glycoconjugates, which play critical roles in cellular processes and disease mechanisms. However, a significant number of GTs remain uncharacterized, including many hypothetical proteins with putative GT activity. In this study, we employed in-silico methods for the structural and functional annotation of a hypothetical glycosyltransferase protein.\nUsing sequence homology-based approaches, we identified potential protein templates for comparative modeling. The three-dimensional structure of the hypothetical GT protein was predicted through homology modeling using Modeller software. The generated model was refined and validated using various assessment tools, including Ramachandran plots and ProSA-web analysis.\nFunctional annotation of the hypothetical GT protein was performed by analyzing conserved domains and motifs using InterProScan, Pfam, and PROSITE databases. In addition, we used sequence alignment tools to identify functionally important residues and predict potential donor and acceptor substrate binding sites.\nFurthermore, molecular docking simulations were carried out to predict the potential interactions between the modeled GT protein and potential substrates. Docking studies provided insights into the putative binding modes and key residues involved in substrate recognition and catalytic activity."
  },
  {
    "objectID": "publications/under-review/AkterMM2023.html",
    "href": "publications/under-review/AkterMM2023.html",
    "title": "Food consumption patterns and sedentary behaviours among the university students: a cross-sectional study",
    "section": "",
    "text": "This cross-sectional study, conducted in Dhaka City, Bangladesh, aimed to examine the eating habits and physical inactivity levels of 444 randomly selected university students. Descriptive statistics, Student’s t-test, Wilcoxon rank-sum test, and chi-square test were used for data analysis. Approximately 44% of the students had irregular breakfast consumption, with a slightly higher proportion of males (46%) than females (42%). Additionally, approximately 23% of the students exhibited irregular dinner habits, with a significant difference between males (9.6%) and females (31%) (p &lt; 0.001). The prevalence of smoking was 13%, with a higher percentage of male smokers (22%) than of female smokers (7.6%) (p &lt; 0.001). Only 19% of both male and female students consumed fruit daily. Rice and meat were the preferred food choices for 57% of students, with slightly higher among males (59%) than females (56%). The majority of students (70%) consumed their favourite food once or twice daily, with females (81%) showing a higher prevalence than males (70%) (p &lt; 0.013). Junk was favoured by 55% of the students, with similar preferences among males (53%) and females (56%). Traditional cooking methods were preferred by 71% of the students. 66% of the students had a normal weight, with a slightly higher proportion of males (67%) than females (65%). Overweight and obesity were observed in 13% and 7.4% of the patients, respectively. Additionally, most females (87%) spent less than two hours watching television daily compared to 80% of males (p &lt; 0.035). Furthermore, physical inactivity was more prevalent among female students (44%) than male students (p &lt; 0.026). This study highlights unhealthy habits among Bangladeshi university students, stressing the need for intervention programs to promote healthier lifestyles."
  },
  {
    "objectID": "publications/under-review/AkterMM2023.html#abstract",
    "href": "publications/under-review/AkterMM2023.html#abstract",
    "title": "Food consumption patterns and sedentary behaviours among the university students: a cross-sectional study",
    "section": "",
    "text": "This cross-sectional study, conducted in Dhaka City, Bangladesh, aimed to examine the eating habits and physical inactivity levels of 444 randomly selected university students. Descriptive statistics, Student’s t-test, Wilcoxon rank-sum test, and chi-square test were used for data analysis. Approximately 44% of the students had irregular breakfast consumption, with a slightly higher proportion of males (46%) than females (42%). Additionally, approximately 23% of the students exhibited irregular dinner habits, with a significant difference between males (9.6%) and females (31%) (p &lt; 0.001). The prevalence of smoking was 13%, with a higher percentage of male smokers (22%) than of female smokers (7.6%) (p &lt; 0.001). Only 19% of both male and female students consumed fruit daily. Rice and meat were the preferred food choices for 57% of students, with slightly higher among males (59%) than females (56%). The majority of students (70%) consumed their favourite food once or twice daily, with females (81%) showing a higher prevalence than males (70%) (p &lt; 0.013). Junk was favoured by 55% of the students, with similar preferences among males (53%) and females (56%). Traditional cooking methods were preferred by 71% of the students. 66% of the students had a normal weight, with a slightly higher proportion of males (67%) than females (65%). Overweight and obesity were observed in 13% and 7.4% of the patients, respectively. Additionally, most females (87%) spent less than two hours watching television daily compared to 80% of males (p &lt; 0.035). Furthermore, physical inactivity was more prevalent among female students (44%) than male students (p &lt; 0.026). This study highlights unhealthy habits among Bangladeshi university students, stressing the need for intervention programs to promote healthier lifestyles."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2023_thalas_qol.html",
    "href": "publications/journal-articles/hossainmj2023_thalas_qol.html",
    "title": "Health‑related quality of life among thalassemia patients in Bangladesh using the SF‑36 questionnaire",
    "section": "",
    "text": "Thalassemia is one of the most common autosomal recessive hereditary blood disorders worldwide, especially in developing countries, including Bangladesh. Thus, this study aimed to determine HRQoL and its determinants of thalassemia patients (TP) in Bangladesh. A cross-sectional survey was performed on 356 randomly selected thalassemia patients. Participants were invited to face-to-face interviews. Descriptive statistics (frequencies and percentages), independent t-test, ANOVA, and multivariate (linear and logistic regression) analysis was performed to analyze the data. Our demographic data showed that among 356 patients, 54% and 46% were male and female, respectively, with an average age of 19.75 (SD = 8.02) years. Most were transfusion-dependent (91%), 26% had comorbidities, and 52% were from low-income families. In the case of HRQoL, male patients showed significantly higher scores of bodily pains and physical health summaries than female patients. Lower income, high blood transfusion status, disease severity, comorbidities, and medical expenses (p &lt; 0.05; CI 95%) are significantly associated with lower SF-36 scores. This study found an association between lower income, blood transfusion, disease severity, comorbidities, as well as medical expenses, and the deterioration of HRQoL among TP. Male patients experienced poorer HRQoL than females. National action plans are required to guarantee the holistic welfare of thalassemia patients."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2023_thalas_qol.html#abstract",
    "href": "publications/journal-articles/hossainmj2023_thalas_qol.html#abstract",
    "title": "Health‑related quality of life among thalassemia patients in Bangladesh using the SF‑36 questionnaire",
    "section": "",
    "text": "Thalassemia is one of the most common autosomal recessive hereditary blood disorders worldwide, especially in developing countries, including Bangladesh. Thus, this study aimed to determine HRQoL and its determinants of thalassemia patients (TP) in Bangladesh. A cross-sectional survey was performed on 356 randomly selected thalassemia patients. Participants were invited to face-to-face interviews. Descriptive statistics (frequencies and percentages), independent t-test, ANOVA, and multivariate (linear and logistic regression) analysis was performed to analyze the data. Our demographic data showed that among 356 patients, 54% and 46% were male and female, respectively, with an average age of 19.75 (SD = 8.02) years. Most were transfusion-dependent (91%), 26% had comorbidities, and 52% were from low-income families. In the case of HRQoL, male patients showed significantly higher scores of bodily pains and physical health summaries than female patients. Lower income, high blood transfusion status, disease severity, comorbidities, and medical expenses (p &lt; 0.05; CI 95%) are significantly associated with lower SF-36 scores. This study found an association between lower income, blood transfusion, disease severity, comorbidities, as well as medical expenses, and the deterioration of HRQoL among TP. Male patients experienced poorer HRQoL than females. National action plans are required to guarantee the holistic welfare of thalassemia patients."
  },
  {
    "objectID": "publications/conference/BSM23_TowhidST.html",
    "href": "publications/conference/BSM23_TowhidST.html",
    "title": "Quantitative Microbial Risk Assessment from Vancomycin‑resistant Enterococcus faecalis and Enterococcus faecium from a specific neighbourhood in Dhaka City,Bangladesh",
    "section": "",
    "text": "With the rise of rapid evolution of therapeutic antibiotic resistance among human pathogens, low-middle income countries are recommended to evaluate the burden of antimicrobial resistance (AMR) among critical pathogens to prioritize disinfection and prescription of effective antibiotics. In this study, we present the level of risk factors among vancomycin resistant Enteroocci (VRE) from urine and blood samples from one neighborhood in Dhaka South area in order to quantify the risk to public health from this particular pathogen. Clinical samples were cultured on bile esculin agar to identify enterococci and then molecular identification was done targeting ddle gene using conventional PCR. The PCR-confirmed isolates of enterococci were quantified for major virulence genes (asa, gelE, esp, hylA, cyt) in qPCR and were categorically classified into risk levels (hyper-virulent &gt; virulent &gt; quasi-virulent &gt; non-virulent) depending on the number and degree of virulence factors present. The PCR-confirmed isolates were also analyzed with qPCR for presence of AMR associated genetic elements (vanA, vanB, vanC/vanR) and categorically classified into AMR risk levels (high-risk AMR &gt; moderate risk AMR &gt; low-risk AMR &gt; intermediate-responsive) on the degree of antimicrobial resistance genetic elements expressed. A risk matrix was developed to calculate the cumulative risk score for each islate. This risk matrix consists of a summation of virulence factors, antimicrobial resistance genes and clonality scores (burden of community-associated vs hospital associated strains). Out of 132 isolates presumed to be pathogenic enterococci on Bile esculin agar, only 15 were PCR-confirmed (positive for ddle gene) with Enterococcus faecalis ATCC 51299 with control strain. 13% of the isolates were hyper-virulent (positive for all 5 virulence genes asa, gelE, esp, hylA, cyt) and moderate-risk AMR (vancomycin-resistant in disc-diffusion and positive for vanB and vanC/vanR), 40% were virulent (positive for asa, gelE, hylA) and moderate-risk AMR (vancomycin-resistant in disc-diffusion and positive for vanB only) and 47% were quasivirulent (positive for at least 2 virulence genes) and low-risk AMR (vancomycin-intermediate resistance in disc-diffusion test and low-level expression of vanB gene). 54% of the isolates were hospital-associated and the rest showed inconclusive results in clonality analysis (RFLP of adk, gydl and pst gene cluster). Taken together, the densely-populated neighborhood of interest in Dhaka South shows mild-to-moderate risk of hospital-associated infection from vancomycin-resistant Enterococcal pathogens."
  },
  {
    "objectID": "publications/conference/BSM23_TowhidST.html#abstract",
    "href": "publications/conference/BSM23_TowhidST.html#abstract",
    "title": "Quantitative Microbial Risk Assessment from Vancomycin‑resistant Enterococcus faecalis and Enterococcus faecium from a specific neighbourhood in Dhaka City,Bangladesh",
    "section": "",
    "text": "With the rise of rapid evolution of therapeutic antibiotic resistance among human pathogens, low-middle income countries are recommended to evaluate the burden of antimicrobial resistance (AMR) among critical pathogens to prioritize disinfection and prescription of effective antibiotics. In this study, we present the level of risk factors among vancomycin resistant Enteroocci (VRE) from urine and blood samples from one neighborhood in Dhaka South area in order to quantify the risk to public health from this particular pathogen. Clinical samples were cultured on bile esculin agar to identify enterococci and then molecular identification was done targeting ddle gene using conventional PCR. The PCR-confirmed isolates of enterococci were quantified for major virulence genes (asa, gelE, esp, hylA, cyt) in qPCR and were categorically classified into risk levels (hyper-virulent &gt; virulent &gt; quasi-virulent &gt; non-virulent) depending on the number and degree of virulence factors present. The PCR-confirmed isolates were also analyzed with qPCR for presence of AMR associated genetic elements (vanA, vanB, vanC/vanR) and categorically classified into AMR risk levels (high-risk AMR &gt; moderate risk AMR &gt; low-risk AMR &gt; intermediate-responsive) on the degree of antimicrobial resistance genetic elements expressed. A risk matrix was developed to calculate the cumulative risk score for each islate. This risk matrix consists of a summation of virulence factors, antimicrobial resistance genes and clonality scores (burden of community-associated vs hospital associated strains). Out of 132 isolates presumed to be pathogenic enterococci on Bile esculin agar, only 15 were PCR-confirmed (positive for ddle gene) with Enterococcus faecalis ATCC 51299 with control strain. 13% of the isolates were hyper-virulent (positive for all 5 virulence genes asa, gelE, esp, hylA, cyt) and moderate-risk AMR (vancomycin-resistant in disc-diffusion and positive for vanB and vanC/vanR), 40% were virulent (positive for asa, gelE, hylA) and moderate-risk AMR (vancomycin-resistant in disc-diffusion and positive for vanB only) and 47% were quasivirulent (positive for at least 2 virulence genes) and low-risk AMR (vancomycin-intermediate resistance in disc-diffusion test and low-level expression of vanB gene). 54% of the isolates were hospital-associated and the rest showed inconclusive results in clonality analysis (RFLP of adk, gydl and pst gene cluster). Taken together, the densely-populated neighborhood of interest in Dhaka South shows mild-to-moderate risk of hospital-associated infection from vancomycin-resistant Enterococcal pathogens."
  },
  {
    "objectID": "publications/conference/BSM23_DasM.html",
    "href": "publications/conference/BSM23_DasM.html",
    "title": "Exploration of Streptococcus core genome to reveal druggable targets and novel thera peutics against S. pneumoniae",
    "section": "",
    "text": "Streptococcus pneumoniae (S. pneumoniae), the major etiological agent of community-acquired pneumonia (CAP) contributes significantly to the global burden of infectious diseases which is getting resistant day by day. Nearly 30% of the S. pneumoniae genomes encode hypothetical proteins (HPs), and better understandings of these HPs in virulence and pathogenicity plausibly decipher new treatments. Some of the HPs are present across many Streptococcus species, systematic assessment of these unexplored HPs will disclose prospective drug targets. In this study, through a stringent bioinformatics analysis of the core genome and proteome of S. pneumoniae PCS8235, we identified and analyzed 28 HPs that are common in many Streptococcus species and might have a potential role in the virulence or pathogenesis of the bacteria. Functional annotations of the proteins were conducted based on the physicochemical properties, subcellular localization, virulence prediction, protein-protein interactions, and identification of essential genes, to find potentially druggable proteins among 28 HPs. The majority of the HPs are involved in bacterial transcription and translation. Besides, some of them were homologs of enzymes, binding proteins, transporters, and regulators. Protein-protein interactions revealed HP PCS8235_RS05845 made the highest interactions with other HPs and also has TRP structural motif along with virulent and pathogenic properties indicating it has critical cellular functions and might go under unconventional protein secretions. The second highest interacting protein HP PCS8235_RS02595 interacts with the Regulator of chromosomal segregation (RocS) which participates in chromosome segregation and nucleoid protection in S. pneumoniae. In this interacting network, 54% of protein members have virulent properties and 40% contain pathogenic properties. Among them, most of these proteins circulate in the cytoplasmic area and have hydrophilic properties. Finally, molecular docking and dynamics simulation demonstrated that the antimalarial drug Artenimol can act as a drug repurposing candidate against HP PCS8235_RS 04650 of S. pneumoniae. Hence, the present study could aid in drugs against S. pneumoniae."
  },
  {
    "objectID": "publications/conference/BSM23_DasM.html#abstract",
    "href": "publications/conference/BSM23_DasM.html#abstract",
    "title": "Exploration of Streptococcus core genome to reveal druggable targets and novel thera peutics against S. pneumoniae",
    "section": "",
    "text": "Streptococcus pneumoniae (S. pneumoniae), the major etiological agent of community-acquired pneumonia (CAP) contributes significantly to the global burden of infectious diseases which is getting resistant day by day. Nearly 30% of the S. pneumoniae genomes encode hypothetical proteins (HPs), and better understandings of these HPs in virulence and pathogenicity plausibly decipher new treatments. Some of the HPs are present across many Streptococcus species, systematic assessment of these unexplored HPs will disclose prospective drug targets. In this study, through a stringent bioinformatics analysis of the core genome and proteome of S. pneumoniae PCS8235, we identified and analyzed 28 HPs that are common in many Streptococcus species and might have a potential role in the virulence or pathogenesis of the bacteria. Functional annotations of the proteins were conducted based on the physicochemical properties, subcellular localization, virulence prediction, protein-protein interactions, and identification of essential genes, to find potentially druggable proteins among 28 HPs. The majority of the HPs are involved in bacterial transcription and translation. Besides, some of them were homologs of enzymes, binding proteins, transporters, and regulators. Protein-protein interactions revealed HP PCS8235_RS05845 made the highest interactions with other HPs and also has TRP structural motif along with virulent and pathogenic properties indicating it has critical cellular functions and might go under unconventional protein secretions. The second highest interacting protein HP PCS8235_RS02595 interacts with the Regulator of chromosomal segregation (RocS) which participates in chromosome segregation and nucleoid protection in S. pneumoniae. In this interacting network, 54% of protein members have virulent properties and 40% contain pathogenic properties. Among them, most of these proteins circulate in the cytoplasmic area and have hydrophilic properties. Finally, molecular docking and dynamics simulation demonstrated that the antimalarial drug Artenimol can act as a drug repurposing candidate against HP PCS8235_RS 04650 of S. pneumoniae. Hence, the present study could aid in drugs against S. pneumoniae."
  },
  {
    "objectID": "publications.html",
    "href": "publications.html",
    "title": "Publications",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n         \n          Publication\n        \n         \n          Year\n        \n     \n  \n    \n      \n      \n    \n\n\n\n  \n    Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh\n    Shuborno Islam, Rathindranath Kabiraj, Himadree Sarkar, Preonath Chondrow Dev, Afroza Akter Tanni, Deb Purna Keya, Apurba R Malaker, Arif Mohammad Tanmoy, Prof. Samir K Saha, Dr. Yogesh Hooda, Dr. Senjuti Saha (Corresponding Author)\n    ASM Journals\n    (2024)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n  \n\n  \n    Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach\n    J.P. Debnath, K. Hossen, M.S. Khandaker, S.B. Sayed, Preonath Chondrow Dev*, S. Sarker*, T. Hossain*\n    Scientific Reports\n    (2024)\n    \n      Details\n    \n    \n    \n    \n  \n\n  \n    Exploration of Streptococcus core genome to reveal druggable targets and novel therapeutics against S. pneumoniae\n    Zeshan Mahmud Chowdhury, Arittra Bhattacharjee, Ishtiaque Ahammad, Mohammad Uzzal Hossain, Abdullah All Jaber, Anisur Rahman, Preonath Chondrow Dev, Md. Salimullah, Chaman Ara Keya\n    PLOS ONE\n    (2022)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients\n    Mohammad Abul Hasnat, Farhan Rahman Niloy, Arafat Islam Ashik, Mahedi Hasan, Preonath Chondrow Dev, Sharmin Sultana Panna, Md. Waseque Mia, Zafrul Hasan\n    Journal of Advanced Biotechnology and Experimental Therapeutics\n    (2022)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n  \n    VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment\n    Rajan Saha Raju, Abdullah Al Nahid, Preonath Chondrow Dev, Rashedul Islam\n    Genomics\n    (2022)\n    \n      Details\n    \n    \n    \n      DOI\n    \n    \n    \n    \n    \n       Materials\n    \n    \n  \n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/monkeypox-outbreak-insights/index.html",
    "href": "projects/monkeypox-outbreak-insights/index.html",
    "title": "Monkeypox Outbreak Insights",
    "section": "",
    "text": "Live Dashboard"
  },
  {
    "objectID": "projects/dengue-situation-dashboard/index.html",
    "href": "projects/dengue-situation-dashboard/index.html",
    "title": "Dengue Situation Dashboard",
    "section": "",
    "text": "Live Dashboard"
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nA Global Pediatric Cell Atlas of Nasal and Oral Mucosa\n\n\n\nSingle Cell RNA Seq Analysis\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nExploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages\n\n\n\nGenomics\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNipah Virus Transmission in Bangladesh\n\n\nNipah virus transmission in Bangladesh primarily occurs through direct contact with infected bats, their excretions, or consumption of contaminated fruits or raw date palm…\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPaediatric pneumonia detection from X‑ray images using deep learning\n\n\n\nAI For Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Disease Spread of Dengue using LSTM\n\n\n\nAI For Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN\n\n\n\nAI For Tools\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nRSV Vaccine Impact Monte Carlo Simulation\n\n\n\nPublic Health\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "blog/welcome/index.html",
    "href": "blog/welcome/index.html",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "",
    "text": "Welcome! Ahoy, fellow data adventurers! 🌊🔍 Our voyage through the vast ocean of data has been nothing short of exhilarating. We’ve navigated the twists and turns of data-driven decision-making, explored the tools that power our data collection, and unraveled the mysteries behind data analysis and visualization. We’ve braved the challenges that come with utilizing data and discovered how it can empower businesses, content creators, and even influence SEO strategies.\nHey there, data enthusiasts and curious minds! 📊🔍 Are you ready to embark on a thrilling expedition into the world of data? In today’s digital age, data isn’t just a buzzword; it’s the compass guiding successful businesses, creative endeavors, and innovative solutions. So, grab your virtual backpacks as we dive into the exciting realm of data-driven decision-making and exploration. Join me on this exhilarating journey, and together, we’ll uncover the boundless power that data holds!"
  },
  {
    "objectID": "blog/welcome/index.html#why-data-matters",
    "href": "blog/welcome/index.html#why-data-matters",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Why Data Matters?",
    "text": "Why Data Matters?\nPicture this: a business owner armed with accurate insights that steer their company toward unprecedented growth. Data is the magic wand that makes it happen. Companies big and small are tapping into the goldmine of data to fine-tune their strategies, enhance customer experiences, and predict market trends. Remember the time you received a tailored recommendation on an online platform? Thank data for that! Let’s dive into how data transforms the ordinary into the extraordinary."
  },
  {
    "objectID": "blog/welcome/index.html#the-data-journey-begins",
    "href": "blog/welcome/index.html#the-data-journey-begins",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "The Data Journey Begins",
    "text": "The Data Journey Begins\nData comes in various shapes and sizes—structured and unstructured. Think of structured data as well-organized puzzle pieces, while unstructured data is like the pieces of a jigsaw puzzle scattered across the table. From customer preferences to social media interactions, data is collected through various channels. But wait, collecting raw data is just the first step of our journey."
  },
  {
    "objectID": "blog/welcome/index.html#tools-for-data-collection",
    "href": "blog/welcome/index.html#tools-for-data-collection",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Tools for Data Collection",
    "text": "Tools for Data Collection\nImagine gathering pieces of a puzzle without missing a single one. That’s the level of accuracy data collection tools provide. Meet the heroes behind the scenes: Google Analytics, surveys, social media insights, and more. But remember, accuracy is the name of the game. Garbage in, garbage out, they say. Inaccurate data can lead you down the wrong path. So, gear up with reliable tools to collect gems, not gravel."
  },
  {
    "objectID": "blog/welcome/index.html#making-sense-of-the-data",
    "href": "blog/welcome/index.html#making-sense-of-the-data",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Making Sense of the Data",
    "text": "Making Sense of the Data\nGathering data is like mining, and now it’s time to extract the precious gems hidden within. Data analysis is our refining process. Whether it’s identifying trends, detecting anomalies, or uncovering correlations, data analysis tools are the chisels and magnifying glasses of our journey. From basic spreadsheets to complex algorithms, these tools unveil the stories buried beneath the numbers."
  },
  {
    "objectID": "blog/welcome/index.html#visualizing-data-for-impact",
    "href": "blog/welcome/index.html#visualizing-data-for-impact",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Visualizing Data for Impact",
    "text": "Visualizing Data for Impact\nData, in its raw form, can resemble a dense fog. That’s where data visualization swoops in. Imagine turning complex data sets into captivating visuals—charts, graphs, and infographics that tell a story at a glance. It’s like transforming fog into a clear roadmap, guiding your decisions with precision. After all, a picture is worth a thousand rows of data!"
  },
  {
    "objectID": "blog/welcome/index.html#challenges-in-data-utilization",
    "href": "blog/welcome/index.html#challenges-in-data-utilization",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Challenges in Data Utilization",
    "text": "Challenges in Data Utilization\nAhoy! Rough waters ahead. Navigating the sea of data isn’t without challenges. Data privacy concerns, incomplete or inconsistent data, and the noise of irrelevant information can make our voyage tricky. But fear not! With the right strategies, you can steer clear of these obstacles and set sail towards data-driven success."
  },
  {
    "objectID": "blog/welcome/index.html#empowering-your-business-with-data",
    "href": "blog/welcome/index.html#empowering-your-business-with-data",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Empowering Your Business with Data",
    "text": "Empowering Your Business with Data\nAvast, business owners! Data isn’t just a treasure for the brave; it’s a compass for the wise. Imagine adjusting your business strategies based on real-time insights. From product development to customer service, data-driven decisions can launch your business to new heights. Ready to hoist the sails of success?"
  },
  {
    "objectID": "blog/welcome/index.html#the-human-element-in-data",
    "href": "blog/welcome/index.html#the-human-element-in-data",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "The Human Element in Data",
    "text": "The Human Element in Data\nData is our guide, but human judgment is our compass. While data analysis offers invaluable insights, remember that it’s humans who make the final call. Just like a ship’s captain relies on the stars while steering the ship, human intuition guides the course of action, ensuring a harmonious blend of data and wisdom."
  },
  {
    "objectID": "blog/welcome/index.html#join-me-on-my-blogging-journey",
    "href": "blog/welcome/index.html#join-me-on-my-blogging-journey",
    "title": "Unlocking the Power of Data: Join the Journey on My Blog!",
    "section": "Join Me on My Blogging Journey",
    "text": "Join Me on My Blogging Journey\nBut wait, our adventure doesn’t end here! There’s a treasure trove of insights waiting for you on my blog. Dive deeper into the world of data-driven strategies, technology trends, and real-life success stories. Unearth the secrets that can propel your journey to the next level. Stay tuned"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "",
    "section": "",
    "text": "Multi-omics Data Integration for Cancer Research\n\n\n\n\n\n\nMulti-omics Data Integration\n\n\nCancer Research\n\n\nMachine Learning\n\n\nOmics Data Analysis\n\n\n\nExplore the power of multi-omics data integration in cancer research. Learn from real-world case studies and groundbreaking discoveries. Find out more about this cutting-edge approach.\n\n\n\n\n\nSep 2, 2023\n\n\nMd. Preonath\n\n\n\n\n\n\n\n\n\n\n\n\nUnlocking the Power of Data: Join the Journey on My Blog!\n\n\n\n\n\n\nNews\n\n\n\n\n\n\n\n\n\nOct 22, 2022\n\n\nMd. Preonath\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html#research-experience",
    "href": "about.html#research-experience",
    "title": "Preonath Chondrow Dev",
    "section": "Research Experience",
    "text": "Research Experience\nNational Gene Bank, National Institute of Biotechnology | Dhaka, Bangladesh | (5 Sept 2021 ‑ 30 June 2022)\n\nThesis Project: Development of a disease detection tool from metagenomic sequence data utilizing k‑mer frequency based algorithms: A gut microbiome study\nRevealing the Disease complexity and Population Structure of Human Gut Microbiota from Diverse Cohort of Bangladesh Population\nDeep learning model based tools development, metagenomic analysis, vaccine trial in the mouse model\nRNA seq data analysis and molecular dynamic simulation\nWet Lab: Sample preparation, and DNA extraction‑related activities\n\nChild Health Research Foundation  | Dhaka, Bangladesh | (01 Aug 2022 ‑ Present)\n\nDevelop and apply bioinformatics tools to process, analyze, and interpret high throughput datasets, including bacterial genome sequencing, viral genome sequencing, and unbiased metagenomic sequencing\nApply biological and statistical knowledge to draw rigorous, actionable conclusions from complex data sets. Discuss findings with colleagues and supervisors\nCollaborate with groups across the institution to support research projects\nParticipate in the preparation of written reports and oral presentations summarizing data analysis results, including a detailed description of bioinformatics methods, analysis of results, and synthesis of conclusions from summary tables.\nTake the initiative to acquire new bioinformatics, statistical, or programming skills as needed."
  },
  {
    "objectID": "about.html#teaching-experience",
    "href": "about.html#teaching-experience",
    "title": "Preonath Chondrow Dev",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nInstractor  [Bioinformatics Division, National Institute of Biotechnology (NIB)] | Dhaka, Bangladesh | (2021)\n\nDelivered training on Bioinformatics for Biotechnology Research\nTopics covered included Linux OS, NGS Data Analysis, and Python.\n\nTeaching Assistant  [Child Health Research Foundation (CHRF)] | Dhaka, Bangladesh | (2021)\n\nAdvanced Next Generation Sequencing, Topics Covered in the Training: Linux OS, NGS Data Analysis: Quality checking, Quality filter, Genome Assembly, Annotation Using Pathogenwatch, Building Consensus using CZID, Sample Sheet Preparation for NextSeq2000; Total 25 Batches\nBacterial Genomics Antimicrobial Resistance, Topics Covered in the Training: Linux OS, Bash Scripting Genome Assembly, Genome Mapping, Genome Annotation, Genome Submission, MLST Prediction at Total 2 Batches\nAI in Public Health, This workshop support from the Bill and Melinda Gates foundation, through the ’Democratizing Public Health Modeling Using AI‑based Tools’; Topics Covered in the Training: Data cleaning and preprocessing, Dengue prediction in Puerto Rico using LSTM and ARIMA, Predicting Pneumonia in X‑rays using CNN"
  },
  {
    "objectID": "about.html#volunteering",
    "href": "about.html#volunteering",
    "title": "Preonath Chondrow Dev",
    "section": "Volunteering",
    "text": "Volunteering\nAcademic Team Member  [Bangladesh Mathematical Olympiad] | Dhaka, Bangladesh | (2021)\nEngaged in evaluating examination papers and overseeing exam hall activities to ensure a smooth conduct of the Olympiad.\nAcademic Team Member  [Bangladesh Physics Olympiad] | Dhaka, Bangladesh | (2021)\nPlayed a crucial role in monitoring examination procedures and providing assistance to participants throughout the event.\nAcademic Team Member  [Bangladesh Physics Olympiad] | Dhaka, Bangladesh | (2021)\nTasked with the assessment of solutions and maintaining order among candidates during the Olympiad competitions.\nEnzyme  Bangladesh Biology Olympiad] | Dhaka, Bangladesh | (2021)\nAssisted in the evaluation process and managed the exam hall logistics to facilitate a fair and efficient Olympiad.\nMentor  [Bioinformatics School] | Dhaka, Bangladesh | (2021)\nProvided guidance and solved various bioinformatics-related problems for the group members.\nSolve the different kinds of bioinformatics related problem of the group members"
  },
  {
    "objectID": "about_template.html",
    "href": "about_template.html",
    "title": "About this template",
    "section": "",
    "text": "About this template\n\n\n\nThis page contains some elaborated background information about your workshop, or the instructors.\n\n\nThis is to demonstrate how to make a drop-down page from the navigation bar."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html",
    "href": "blog/multi-omics-data-integration/index.html",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "",
    "text": "To understand multi-omics data integration, we must first grasp the concept of omics. It’s like exploring different dimensions of cancer. We have genomics (genes), transcriptomics (gene expression), proteomics (proteins), metabolomics (metabolites), and more. Each of these dimensions provides a unique perspective on the disease, akin to different layers of a complex painting.\n\n\n\nCancer affects millions worldwide, making it a pressing global health concern. Research in this field has the potential to save countless lives, making it a moral imperative. Harnessing multi-omics data could be the key to unlocking breakthroughs in cancer diagnosis, treatment, and prevention."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#multi-omics-data-integration-for-cancer-research",
    "href": "blog/multi-omics-data-integration/index.html#multi-omics-data-integration-for-cancer-research",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "",
    "text": "To understand multi-omics data integration, we must first grasp the concept of omics. It’s like exploring different dimensions of cancer. We have genomics (genes), transcriptomics (gene expression), proteomics (proteins), metabolomics (metabolites), and more. Each of these dimensions provides a unique perspective on the disease, akin to different layers of a complex painting.\n\n\n\nCancer affects millions worldwide, making it a pressing global health concern. Research in this field has the potential to save countless lives, making it a moral imperative. Harnessing multi-omics data could be the key to unlocking breakthroughs in cancer diagnosis, treatment, and prevention."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#challenges-in-cancer-research",
    "href": "blog/multi-omics-data-integration/index.html#challenges-in-cancer-research",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Challenges in Cancer Research",
    "text": "Challenges in Cancer Research\n\nHeterogeneity of Cancer\nCancer isn’t one disease; it’s a family of diseases with countless variations. Each patient’s cancer is as unique as a fingerprint. This inherent heterogeneity poses a significant challenge in understanding and treating cancer effectively.\n\n\nData Overload\nThe explosion of data in biomedical research is both a blessing and a curse. While we have access to vast amounts of omics data, making sense of it all is like searching for a needle in a haystack. This data overload can overwhelm researchers and slow progress."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#multi-omics-data-integration",
    "href": "blog/multi-omics-data-integration/index.html#multi-omics-data-integration",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Multi-omics Data Integration",
    "text": "Multi-omics Data Integration\n\nWhat is Multi-omics?\nMulti-omics data integration involves combining information from various omics sources, like genomics, proteomics, and metabolomics, to get a comprehensive view of a patient’s cancer. It’s like assembling a jigsaw puzzle with pieces from different boxes to see the entire picture.\n\n\nTypes of Omics Data\nGenomic data reveals a person’s genetic makeup, while transcriptomics tells us which genes are active. Proteomics showcases the proteins at work, and metabolomics gives insights into metabolic processes. Integrating these omics layers can unveil hidden patterns.\n\n\nBenefits of Integration\nBy merging these omics datasets, researchers can identify key molecular events driving cancer. This holistic view can lead to more accurate diagnoses, personalized treatment plans, and a deeper understanding of the disease’s complexity."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#tools-and-techniques",
    "href": "blog/multi-omics-data-integration/index.html#tools-and-techniques",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Tools and Techniques",
    "text": "Tools and Techniques\n\nBioinformatics Tools\nBioinformatics plays a pivotal role in multi-omics data integration. Tools like R, Python, and specialized software enable researchers to process and analyze vast datasets efficiently.\n\n\nMachine Learning Algorithms\nMachine learning algorithms like random forests and deep learning models help identify biomarkers and predict treatment responses based on multi-omics data. These algorithms are like detectives deciphering clues in a mystery novel.\n\n\nCase Studies\nReal-world examples, such as The Cancer Genome Atlas (TCGA) project, showcase how multi-omics data integration has led to groundbreaking discoveries. It’s like a detective story with unexpected twists and turns.\n\n\nPrevious Studies\nMulti-omics data integration is a critical component of cancer research. It involves the combination of various omics data, such as genomics, transcriptomics, proteomics, and metabolomics, to gain a comprehensive understanding of cancer biology. Here are some key insights and approaches.\nIntegrative Multi-Omics Approaches at NCBI explore how multiple omics datasets are integrated to identify relevant clinical features in cancer research. This article likely provides in-depth insights into the methods and benefits of multi-omics integration in cancer studies [1].\nMachine Learning for Multi-Omics Data Integration is discussed in the ScienceDirect article. It highlights the role of machine learning in processing and interpreting multi-omics data, which is essential for cancer molecular biology research [2].\nA Nature article describes how multi-omics data integration and modeling unveil new mechanisms in pancreatic cancer and improve prognostic prediction. This suggests the potential clinical applications of multi-omics integration [3].\nThe ResearchGate publication emphasizes the importance of multi-omics data analysis in cancer molecular biology. It mentions groundbreaking discoveries resulting from these efforts and may provide case studies or examples [4].\nFrontiers in Genetics discusses schemes for integrating multi-omics dimensions to stratify cancer patients and predict biomarkers. This can be crucial for personalized medicine and targeted therapies in cancer treatment [5].\nIn Frontiers in Oncology, the article reviews an algorithm called Amaretto, which integrates multiple omics profiles across different cancer types. This algorithm’s use cases and outcomes in oncology research could be valuable [6]."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#applications",
    "href": "blog/multi-omics-data-integration/index.html#applications",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Applications",
    "text": "Applications\n\nEarly Detection\nEarly detection of cancer is crucial for improving survival rates. Multi-omics data can identify subtle changes that occur at the molecular level, allowing for earlier and more accurate diagnosis.\n\n\nPersonalized Treatment\nNo two cancers are alike. Multi-omics data can help tailor treatment plans to individual patients, optimizing their chances of successful outcomes.\n\n\nDrug Discovery\nDiscovering new cancer drugs is like searching for treasure. Multi-omics data integration can identify potential drug targets and pathways, accelerating drug development."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#future-directions",
    "href": "blog/multi-omics-data-integration/index.html#future-directions",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Future Directions",
    "text": "Future Directions\n\nPrecision Medicine\nThe future of cancer care lies in precision medicine. Multi-omics data will play a central role in tailoring treatments to each patient’s unique genetic makeup and disease profile."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#ethical-considerations",
    "href": "blog/multi-omics-data-integration/index.html#ethical-considerations",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Ethical Considerations",
    "text": "Ethical Considerations\nWhile multi-omics data offers immense potential, it also raises ethical concerns about data privacy, consent, and equitable access. Balancing innovation with ethical responsibility is crucial."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#conclusion",
    "href": "blog/multi-omics-data-integration/index.html#conclusion",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "Conclusion",
    "text": "Conclusion\nIn the intricate world of cancer research, multi-omics data integration shines as a beacon of hope. It’s a tool that allows us to dissect the complexity of cancer, offering insights that were once elusive. As we continue this journey, let us remember that every piece of data, like a puzzle piece, brings us one step closer to solving the cancer puzzle."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#faqs",
    "href": "blog/multi-omics-data-integration/index.html#faqs",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "FAQs",
    "text": "FAQs\n\nWhat is multi-omics data integration?\nMulti-omics data integration involves combining information from various omics sources, such as genomics, proteomics, and metabolomics, to gain a comprehensive understanding of a patient’s cancer.\n\n\nHow does multi-omics data benefit cancer research?\nMulti-omics data integration enables researchers to uncover hidden patterns, leading to more accurate diagnoses, personalized treatment plans, and a deeper understanding of the disease’s complexity.\n\n\nWhat tools are commonly used for data integration?\nBioinformatics tools like R and Python, along with machine learning algorithms, are commonly used for multi-omics data integration in cancer research.\n\n\nCan multi-omics data help in early cancer detection?\nYes, multi-omics data can identify subtle molecular changes, enabling earlier and more accurate cancer diagnosis.\n\n\nWhat are the ethical concerns in multi-omics research?\nEthical concerns in multi-omics research revolve around data privacy, consent, and equitable access to the benefits of research findings. Balancing innovation with ethical responsibility is crucial in this field."
  },
  {
    "objectID": "blog/multi-omics-data-integration/index.html#references",
    "href": "blog/multi-omics-data-integration/index.html#references",
    "title": "Multi-omics Data Integration for Cancer Research",
    "section": "References",
    "text": "References\n\nNCBI - Integrative Multi-Omics Approaches in Cancer Research\nScienceDirect - Machine learning for multi-omics data integration in cancer\nNature - Multi-omics data integration and modeling unravels new mechanisms\nResearchGate - Machine learning for multi-omics data integration in cancer\nFrontiers in Genetics - Integration of Online Omics-Data Resources for Cancer\nFrontiers in Oncology - Integrated Multi-Omics Analyses in Oncology"
  },
  {
    "objectID": "dashboards.html",
    "href": "dashboards.html",
    "title": "Dashboards",
    "section": "",
    "text": "The dashboard typically includes various features and components to help stakeholders, such as public health authorities, researchers, and the general public, understand and analyze the dengue situation."
  },
  {
    "objectID": "dashboards.html#dengue-situation-monitoring-dashboard",
    "href": "dashboards.html#dengue-situation-monitoring-dashboard",
    "title": "Dashboards",
    "section": "",
    "text": "The dashboard typically includes various features and components to help stakeholders, such as public health authorities, researchers, and the general public, understand and analyze the dengue situation."
  },
  {
    "objectID": "dashboards.html#nipah-virus-transmission-in-bangladesh",
    "href": "dashboards.html#nipah-virus-transmission-in-bangladesh",
    "title": "Dashboards",
    "section": "Nipah Virus Transmission in Bangladesh",
    "text": "Nipah Virus Transmission in Bangladesh\nNipah virus transmission in Bangladesh primarily occurs through direct contact with infected bats, their excretions, or consumption of contaminated fruits or raw date palm sap. This zoonotic virus can also be transmitted from person to person through close contact, making it a potential threat in healthcare settings. Public health measures, such as avoiding contact with sick individuals and practicing good hygiene, are essential to prevent its spread in Bangladesh and other affected regions."
  },
  {
    "objectID": "dashboards.html#monkeypox-outbreak-insights",
    "href": "dashboards.html#monkeypox-outbreak-insights",
    "title": "Dashboards",
    "section": "Monkeypox Outbreak Insights",
    "text": "Monkeypox Outbreak Insights\nVisualizing the data produced by the World Health Organization on the 2022 monkeypox outbreak."
  },
  {
    "objectID": "news.html",
    "href": "news.html",
    "title": "",
    "section": "",
    "text": "No matching items"
  },
  {
    "objectID": "projects/nipah-virus/index.html",
    "href": "projects/nipah-virus/index.html",
    "title": "Nipah Virus Transmission in Bangladesh",
    "section": "",
    "text": "Live Dashboard"
  },
  {
    "objectID": "publications/conference/BSM23_AkterS.html",
    "href": "publications/conference/BSM23_AkterS.html",
    "title": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment",
    "section": "",
    "text": "Classification of viruses into their taxonomic ranks (e.g., order, family, genus) provides a framework to organize an abundant population of viruses. Next-generation metagenomic sequencing technologies lead to a rapid increase in generating sequencing data of viruses which require bioinformatics tools to analyze the taxonomy. Many metagenomic taxonomy classifiers have been developed to study microbiomes, but it is particularly challenging to assign the taxonomy of diverse virus sequences and there is a growing need for dedicated methods to be developed that are optimized to classify virus sequences into their taxa. VirusTaxo, developed using diverse (e.g., 402 DNA and 280 RNA) genera of viruses, has an average accuracy of 93% at genus level prediction in DNA and RNA viruses. VirusTaxo outperformed existing taxonomic classifiers by assigning taxonomy to a larger fraction of metagenomic contigs compared to other methods. Benchmarking of VirusTaxo on a collection of SARS-CoV-2 sequencing libraries and metavirome datasets suggests that VirusTaxo can characterize virus taxonomy from highly diverse contigs and provide a reliable decision on the taxonomy of viruses.\nKeywords: Virus Taxonomy, Hierarchical Classification, k-mer, Genome"
  },
  {
    "objectID": "publications/conference/BSM23_AkterS.html#abstract",
    "href": "publications/conference/BSM23_AkterS.html#abstract",
    "title": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment",
    "section": "",
    "text": "Classification of viruses into their taxonomic ranks (e.g., order, family, genus) provides a framework to organize an abundant population of viruses. Next-generation metagenomic sequencing technologies lead to a rapid increase in generating sequencing data of viruses which require bioinformatics tools to analyze the taxonomy. Many metagenomic taxonomy classifiers have been developed to study microbiomes, but it is particularly challenging to assign the taxonomy of diverse virus sequences and there is a growing need for dedicated methods to be developed that are optimized to classify virus sequences into their taxa. VirusTaxo, developed using diverse (e.g., 402 DNA and 280 RNA) genera of viruses, has an average accuracy of 93% at genus level prediction in DNA and RNA viruses. VirusTaxo outperformed existing taxonomic classifiers by assigning taxonomy to a larger fraction of metagenomic contigs compared to other methods. Benchmarking of VirusTaxo on a collection of SARS-CoV-2 sequencing libraries and metavirome datasets suggests that VirusTaxo can characterize virus taxonomy from highly diverse contigs and provide a reliable decision on the taxonomy of viruses.\nKeywords: Virus Taxonomy, Hierarchical Classification, k-mer, Genome"
  },
  {
    "objectID": "publications/conference/BSM23_ShahidMSB.html",
    "href": "publications/conference/BSM23_ShahidMSB.html",
    "title": "Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients",
    "section": "",
    "text": "Patients with type-2 diabetes mellitus (T2DM) are known to suffer from hyperlipidemia. How hyperlipidemia is associated with controlled and uncontrolled T2DM patients in Bangladesh remained to be addressed. In this cross-sectional study, 211 participants were enrolled who have been suffering from T2DM for more than 4 years from the northeastern part of Bangladesh. Controlled and uncontrolled patients were defined with their plasma glycated hemoglobin (HbA1c) levels. Among them, 39% and 61% were in the diabetic-controlled and uncontrolled groups. Indeed, the diabetic uncontrolled group showed a higher frequency of hypercholesterolemia, hypertriglyceridemia, hyper LDL-cholesterolemia, and hypo HDLcholesterolemia compare to the diabetic controlled group. Lipid profiling analysis revealed significantly elevated (p&lt;0.0001) levels of cholesterol, triglyceride, and low-density lipoprotein (LDL) in uncontrolled than the controlled group, while high-density lipoprotein (HDL) was significantly (p&lt;0.0001) lower in uncontrolled diabetics patients. Interestingly, significantly (p&lt;0.05) higher dyslipidemia was also observed in individuals with controlled diabetic population, who have been suffering from T2DM for more than 7 years. Therefore, these results highlight that not only the diabetic uncontrolled but also the controlled group patients have a high risk of developing hyperlipidemia after a certain period of diabetes."
  },
  {
    "objectID": "publications/conference/BSM23_ShahidMSB.html#abstract",
    "href": "publications/conference/BSM23_ShahidMSB.html#abstract",
    "title": "Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients",
    "section": "",
    "text": "Patients with type-2 diabetes mellitus (T2DM) are known to suffer from hyperlipidemia. How hyperlipidemia is associated with controlled and uncontrolled T2DM patients in Bangladesh remained to be addressed. In this cross-sectional study, 211 participants were enrolled who have been suffering from T2DM for more than 4 years from the northeastern part of Bangladesh. Controlled and uncontrolled patients were defined with their plasma glycated hemoglobin (HbA1c) levels. Among them, 39% and 61% were in the diabetic-controlled and uncontrolled groups. Indeed, the diabetic uncontrolled group showed a higher frequency of hypercholesterolemia, hypertriglyceridemia, hyper LDL-cholesterolemia, and hypo HDLcholesterolemia compare to the diabetic controlled group. Lipid profiling analysis revealed significantly elevated (p&lt;0.0001) levels of cholesterol, triglyceride, and low-density lipoprotein (LDL) in uncontrolled than the controlled group, while high-density lipoprotein (HDL) was significantly (p&lt;0.0001) lower in uncontrolled diabetics patients. Interestingly, significantly (p&lt;0.05) higher dyslipidemia was also observed in individuals with controlled diabetic population, who have been suffering from T2DM for more than 7 years. Therefore, these results highlight that not only the diabetic uncontrolled but also the controlled group patients have a high risk of developing hyperlipidemia after a certain period of diabetes."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2022_antibiotics.html",
    "href": "publications/journal-articles/hossainmj2022_antibiotics.html",
    "title": "Perception of Students on Antibiotic Resistance and Prevention: An Online, Community‑Based Case Study from Dhaka, Bangladesh",
    "section": "",
    "text": "Aims: The study aimed to assess perception of the students about antibiotic consumption and the rise of antibiotic resistance with the view to developing an effective community engagement strategy for antimicrobial stewardship.\nMethods: A cross-sectional study was carried out from July 2020 to November 2020. The students from Dhaka City, Bangladesh were approached through social media to complete an online questionnaire containing self-identification data, knowledge about antibiotics, attitudes toward antibiotics, and antibiotic consumption. Descriptive statistics were used to analyze the data.\nResults: Out of the 472 survey participants, 24.6% of participants identified antibiotic side effects as the worst outcome, while 74.2% were unaware of the antibiotics’ long-term impact on global health. Participants believe antimicrobial stewardship and legal disciplinary action against indiscriminate use of antibiotics are the best strategies for countering the spread of antimicrobial resistance.\nConclusions: This study has identified knowledge gaps and misconceptions about antibiotic resistance and prevention. In these situations, academics must play a role in training students about the dangers of antibiotic misuse, gathering public opinion in support of effective policy making, and enforcing guidelines and regulations across the healthcare, pharmaceutical, and agricultural industries to prevent the spread of superbugs."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2022_antibiotics.html#abstract",
    "href": "publications/journal-articles/hossainmj2022_antibiotics.html#abstract",
    "title": "Perception of Students on Antibiotic Resistance and Prevention: An Online, Community‑Based Case Study from Dhaka, Bangladesh",
    "section": "",
    "text": "Aims: The study aimed to assess perception of the students about antibiotic consumption and the rise of antibiotic resistance with the view to developing an effective community engagement strategy for antimicrobial stewardship.\nMethods: A cross-sectional study was carried out from July 2020 to November 2020. The students from Dhaka City, Bangladesh were approached through social media to complete an online questionnaire containing self-identification data, knowledge about antibiotics, attitudes toward antibiotics, and antibiotic consumption. Descriptive statistics were used to analyze the data.\nResults: Out of the 472 survey participants, 24.6% of participants identified antibiotic side effects as the worst outcome, while 74.2% were unaware of the antibiotics’ long-term impact on global health. Participants believe antimicrobial stewardship and legal disciplinary action against indiscriminate use of antibiotics are the best strategies for countering the spread of antimicrobial resistance.\nConclusions: This study has identified knowledge gaps and misconceptions about antibiotic resistance and prevention. In these situations, academics must play a role in training students about the dangers of antibiotic misuse, gathering public opinion in support of effective policy making, and enforcing guidelines and regulations across the healthcare, pharmaceutical, and agricultural industries to prevent the spread of superbugs."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2023_thals_kap.html",
    "href": "publications/journal-articles/hossainmj2023_thals_kap.html",
    "title": "Knowledge and Attitudes towards Thalassemia among Public University Students in Bangladesh",
    "section": "",
    "text": "Background: Thalassemia is the most common congenital single-gene condition globally, characterized by a lack of or reduced synthesis of either the α- or β-globin chains and passed down from parents to offspring. This study aimed to determine the level of knowledge of thalassemia among Bangladeshi students from universities and their opinion on prevention.\nMethods: A cross-sectional descriptive online survey was conducted on public university students in Bangladesh using a structured questionnaire between June and November 2020, completed by students online. Demographic characteristics, knowledge, and attitude of the participants were assayed with 10 questions on each topic. The data were analyzed using Python. Descriptive statistics methods such as frequencies and percentages were used to present data.\nResult: A total of 681 students participated in the online survey. The average age of the respondents in this study was 21.97 years, with a standard deviation of 2.9. Most of the students, 611(89.72%), had heard about Thalassemia, yet only 248 (36.42%) students had good knowledge of Thalassemia. The knowledge level of students had no significant relationship with sex (p-value = 0.0819), marital status (p-value = 0.2281), and year of study (p-value = 0.4619), but had a considerable difference with the field of study (0.0042). However, 478 (78.36%) participants showed a positive attitude toward “Premarital Screening” to prevent Thalassemia.\nConclusions: Long-term and target-based preventive initiatives are recommended for university students and the general population of Bangladesh, where the rate of consanguineous marriage is high. These initiatives would provide crucial information and increase awareness of premarital screening and genetic counseling to prevent thalassemia.\nKeywords: Thalassemia, Awareness, Student community, Bangladesh."
  },
  {
    "objectID": "publications/journal-articles/hossainmj2023_thals_kap.html#abstract",
    "href": "publications/journal-articles/hossainmj2023_thals_kap.html#abstract",
    "title": "Knowledge and Attitudes towards Thalassemia among Public University Students in Bangladesh",
    "section": "",
    "text": "Background: Thalassemia is the most common congenital single-gene condition globally, characterized by a lack of or reduced synthesis of either the α- or β-globin chains and passed down from parents to offspring. This study aimed to determine the level of knowledge of thalassemia among Bangladeshi students from universities and their opinion on prevention.\nMethods: A cross-sectional descriptive online survey was conducted on public university students in Bangladesh using a structured questionnaire between June and November 2020, completed by students online. Demographic characteristics, knowledge, and attitude of the participants were assayed with 10 questions on each topic. The data were analyzed using Python. Descriptive statistics methods such as frequencies and percentages were used to present data.\nResult: A total of 681 students participated in the online survey. The average age of the respondents in this study was 21.97 years, with a standard deviation of 2.9. Most of the students, 611(89.72%), had heard about Thalassemia, yet only 248 (36.42%) students had good knowledge of Thalassemia. The knowledge level of students had no significant relationship with sex (p-value = 0.0819), marital status (p-value = 0.2281), and year of study (p-value = 0.4619), but had a considerable difference with the field of study (0.0042). However, 478 (78.36%) participants showed a positive attitude toward “Premarital Screening” to prevent Thalassemia.\nConclusions: Long-term and target-based preventive initiatives are recommended for university students and the general population of Bangladesh, where the rate of consanguineous marriage is high. These initiatives would provide crucial information and increase awareness of premarital screening and genetic counseling to prevent thalassemia.\nKeywords: Thalassemia, Awareness, Student community, Bangladesh."
  },
  {
    "objectID": "publications/under-review/HossainMJ2023.html",
    "href": "publications/under-review/HossainMJ2023.html",
    "title": "Prevalence, Antibiotic Resistance Pattern for Bacteriuria from Patients with Urinary Tract Infections",
    "section": "",
    "text": "Background: Antibiotic resistance (AR) in urinary tract infections (UTIs) is a global public health concern, particularly in developing countries. This study aimed to analyze the antimicrobial susceptibility patterns of UTI-causing bacteria in patients at Mughda Medical College Hospital in Dhaka, Bangladesh.\nMethods: A one-year retrospective study was conducted at the Mugdha Medical College and Hospital in Dhaka (January 2019 to December 2020). The clinical information and laboratory findings of individuals with a positive urine culture (105 CFU/mL urine) were used. Patients were divided into four age groups: infants (1-18 years), young adults (18-33 years), late adults (33-50 years), and the elderly (50-95 years). The antibiotic sensitivity assay followed the standard Kirby-Bauer method for the 28 most common antibiotics.\nResults: In this study, 243 urine samples were analyzed, and Escherichia coli was the most prevalent uropathogen (65.84%) in UTIs, followed by Klebsiella spp. (12.34%) and Enterococcus spp. (8.23%), and Streptococcus spp. (6.58%) and Proteus spp. (2.88%) and Pseudomonas spp. (2.46%), and Staphylococcus spp. (1.64%). Proteus spp. were the most prevalent in infants and adolescents (86%), while Pseudomonas spp. were the most prevalent in the elderly (83%). Most UTIs occur in females (82%), with the highest prevalence among older people (34%). Antibiotic resistance was observed in E. coli, Klebsiella spp., Enterococcus spp., Streptococcus spp., Proteus spp., Pseudomonas spp., and Staphylococcus spp., all of which were resistant to varying numbers of antibiotics. Pseudomonas spp. showed maximum resistance (100) to ten antibiotics. Appropriate antibiotic selection is crucial for effective treatment of UTIs.\nConclusion: It can be concluded that older adults are the most vulnerable age group, and E. coli is the most prevalent causative organism of UTIs in the Mugdha Medical College Hospital area. Our results suggest that appropriate antibiotic selection is essential for effective treatment of UTIs in people of different ages.\nKeywords: Antibiotic Resistance, Susceptibility, Urinary Tract Infections (UTI), Uropathogen, Bangladesh"
  },
  {
    "objectID": "publications/under-review/HossainMJ2023.html#abstract",
    "href": "publications/under-review/HossainMJ2023.html#abstract",
    "title": "Prevalence, Antibiotic Resistance Pattern for Bacteriuria from Patients with Urinary Tract Infections",
    "section": "",
    "text": "Background: Antibiotic resistance (AR) in urinary tract infections (UTIs) is a global public health concern, particularly in developing countries. This study aimed to analyze the antimicrobial susceptibility patterns of UTI-causing bacteria in patients at Mughda Medical College Hospital in Dhaka, Bangladesh.\nMethods: A one-year retrospective study was conducted at the Mugdha Medical College and Hospital in Dhaka (January 2019 to December 2020). The clinical information and laboratory findings of individuals with a positive urine culture (105 CFU/mL urine) were used. Patients were divided into four age groups: infants (1-18 years), young adults (18-33 years), late adults (33-50 years), and the elderly (50-95 years). The antibiotic sensitivity assay followed the standard Kirby-Bauer method for the 28 most common antibiotics.\nResults: In this study, 243 urine samples were analyzed, and Escherichia coli was the most prevalent uropathogen (65.84%) in UTIs, followed by Klebsiella spp. (12.34%) and Enterococcus spp. (8.23%), and Streptococcus spp. (6.58%) and Proteus spp. (2.88%) and Pseudomonas spp. (2.46%), and Staphylococcus spp. (1.64%). Proteus spp. were the most prevalent in infants and adolescents (86%), while Pseudomonas spp. were the most prevalent in the elderly (83%). Most UTIs occur in females (82%), with the highest prevalence among older people (34%). Antibiotic resistance was observed in E. coli, Klebsiella spp., Enterococcus spp., Streptococcus spp., Proteus spp., Pseudomonas spp., and Staphylococcus spp., all of which were resistant to varying numbers of antibiotics. Pseudomonas spp. showed maximum resistance (100) to ten antibiotics. Appropriate antibiotic selection is crucial for effective treatment of UTIs.\nConclusion: It can be concluded that older adults are the most vulnerable age group, and E. coli is the most prevalent causative organism of UTIs in the Mugdha Medical College Hospital area. Our results suggest that appropriate antibiotic selection is essential for effective treatment of UTIs in people of different ages.\nKeywords: Antibiotic Resistance, Susceptibility, Urinary Tract Infections (UTI), Uropathogen, Bangladesh"
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_02.html",
    "href": "publications/working-papers/shahnaj2023_02.html",
    "title": "Prediction of Structure and Function of Helicobacter pylori Hypothetical Protein MPG40_02440: An In-Silico Approach",
    "section": "",
    "text": "Hypothetical proteins (HPs) are the proteins whose occurrence has been predicted, yet in vivo function has not been manufactured up. {Helicobacter pylori (H. pylori) is a Gram-negative helical, microaerophilic bacterium, surviving in stomachs harsh acidic environment through mechanisms of acid resistance and colonization factors.} As H. pylori has been found to play a major etiological and pathogenic role in chronic gastritis, peptic ulcer disease and gastric cancer so it’s important to investigate H. pylori’s protein structures and functions. The current study aims to predict the structure and function of hypothetical protein MPG40_02440 from H. pylori [molybdopterin guanine dinucleotide-containing S/N-oxide reductases] Some server-based Bio-informatics tools are used to get physicochemical properties, secondary structure prediction, homology modeling etc. The study shows it is a stable, soluble and intracellular protein. It belongs to bisC_fam super family. These enzymes function in general to transfer oxygen to or from a physiological molecule."
  },
  {
    "objectID": "publications/working-papers/shahnaj2023_02.html#abstract",
    "href": "publications/working-papers/shahnaj2023_02.html#abstract",
    "title": "Prediction of Structure and Function of Helicobacter pylori Hypothetical Protein MPG40_02440: An In-Silico Approach",
    "section": "",
    "text": "Hypothetical proteins (HPs) are the proteins whose occurrence has been predicted, yet in vivo function has not been manufactured up. {Helicobacter pylori (H. pylori) is a Gram-negative helical, microaerophilic bacterium, surviving in stomachs harsh acidic environment through mechanisms of acid resistance and colonization factors.} As H. pylori has been found to play a major etiological and pathogenic role in chronic gastritis, peptic ulcer disease and gastric cancer so it’s important to investigate H. pylori’s protein structures and functions. The current study aims to predict the structure and function of hypothetical protein MPG40_02440 from H. pylori [molybdopterin guanine dinucleotide-containing S/N-oxide reductases] Some server-based Bio-informatics tools are used to get physicochemical properties, secondary structure prediction, homology modeling etc. The study shows it is a stable, soluble and intracellular protein. It belongs to bisC_fam super family. These enzymes function in general to transfer oxygen to or from a physiological molecule."
  },
  {
    "objectID": "research.html#ai-based-approaches-to-improving-ehealth-literacy-and-combating-infodemic",
    "href": "research.html#ai-based-approaches-to-improving-ehealth-literacy-and-combating-infodemic",
    "title": "Research Projects",
    "section": "AI-Based Approaches to Improving eHealth Literacy and Combating Infodemic",
    "text": "AI-Based Approaches to Improving eHealth Literacy and Combating Infodemic\nTeam: Md. Jubayer Hossain (Team Lead)\n\nAbstract\nThe rapid evolution of digital health platforms and the proliferation of online health information have highlighted the critical importance of eHealth literacy. However, the current digital landscape also gives rise to the spread of misinformation, leading to an infodemic that can have detrimental effects on public health. This paper explores how artificial intelligence (AI) can be harnessed to enhance eHealth literacy and counter the challenges posed by the infodemic.\nThis study employs a comprehensive review of existing literature and case studies, focusing on AI-driven solutions aimed at improving eHealth literacy and mitigating the impact of health-related misinformation. Examples include chatbots that provide accurate health information, AI-powered content verification tools, and personalized eHealth platforms.\nLeveraging AI to enhance eHealth literacy offers a promising avenue for tackling the challenges posed by the infodemic. By tailoring information delivery to users’ needs, AI can foster a better understanding of health information and encourage critical thinking. Moreover, AI’s capability to identify and combat misinformation contributes to a more informed and resilient society. Collaborative efforts among technology developers, healthcare professionals, and policymakers are crucial to realizing the full potential of AI in promoting accurate health information and mitigating the harmful effects of the infodemic.\n\n\nPublic Health Relevance Statement\nIn response to the evolving landscape of digital health information and the challenges posed by the rapid spread of misinformation, innovative AI-based solutions have emerged as powerful tools to enhance eHealth literacy and mitigate the impact of infodemics. By leveraging natural language processing and machine learning algorithms, these approaches can efficiently analyze and categorize health-related content, providing individuals with accurate and tailored information. Through personalized recommendations, interactive educational platforms, and real-time fact-checking, AI-driven interventions hold the potential to empower individuals with the skills and knowledge necessary to make informed decisions about their health, thereby fostering healthier communities and curbing the detrimental effects of rampant misinformation.\nKeywords: eHealth literacy, infodemic, artificial intelligence, misinformation, health information, digital health.\n\n\nKey AI Applications in Public Health\n\nContent Filtering and Fact-Checking: AI algorithms can be used to automatically filter out false or misleading health information from online sources. Natural language processing (NLP) techniques can analyze text for credibility, accuracy, and consistency with established medical knowledge.\nHealth Chatbots and Virtual Assistants: AI-powered chatbots and virtual assistants can provide reliable and personalized health information to users. They can answer questions, offer guidance, and direct individuals to trustworthy sources of information, helping to improve eHealth literacy.\nPersonalized Health Education: AI can analyze user data and preferences to deliver personalized health education content. This ensures that individuals receive information tailored to their specific needs and knowledge levels.\nPredictive Analytics for Disease Outbreaks: AI and machine learning can analyze vast amounts of health data to detect and predict disease outbreaks. Early warnings enable public health authorities to respond quickly and disseminate accurate information to the public, reducing panic and misinformation.\nSentiment Analysis and Social Media Monitoring: AI can monitor social media platforms and news outlets for trending health topics and public sentiment. This helps identify emerging infodemics and enables timely interventions to counteract false information.\nAutomated Multilingual Translation: AI can facilitate the translation of health information into multiple languages, making it more accessible to diverse populations and reducing language-related barriers to eHealth literacy.\nInteractive Learning Platforms: AI-driven interactive platforms can gamify health education, making it engaging and interactive. These platforms can track users’ progress and adapt content to their evolving knowledge levels.\nData Visualization and Infographics: AI can assist in creating visually appealing and easy-to-understand infographics and data visualizations to convey complex health information effectively.\nContent Recommendation Systems: AI algorithms can recommend reliable health websites, articles, and resources to users based on their interests and past behavior. This helps individuals access credible information more easily.\nNatural Language Understanding for Voice Assistants: Voice-activated AI systems can provide spoken explanations and answers to health-related queries, making information accessible to those with limited literacy skills or visual impairments.\nMonitoring and Flagging Misinformation: AI can continuously scan online platforms for the dissemination of false health information and flag or report it for review. This helps in quick content moderation.\nCollaboration with Healthcare Professionals: AI can facilitate communication between healthcare professionals and patients by providing accurate and up-to-date information, aiding in diagnosis, and offering guidance on treatment options.\nUser Education and Training: AI-powered platforms can assess users’ eHealth literacy levels and provide tailored training and educational content to improve their ability to discern credible health information from misinformation."
  },
  {
    "objectID": "research.html#monitoring-water-borne-and-vector-borne-disease-using-nasa-earth-observing-data",
    "href": "research.html#monitoring-water-borne-and-vector-borne-disease-using-nasa-earth-observing-data",
    "title": "Research Projects",
    "section": "Monitoring Water-borne and Vector-borne Disease Using NASA Earth Observing Data",
    "text": "Monitoring Water-borne and Vector-borne Disease Using NASA Earth Observing Data\nTeam: Md. Jubayer Hossain (Team Lead)\n\nAbstract\nThe utilization of NASA Earth Observing Data for monitoring water-borne and vector-borne diseases presents a promising approach to advancing public health surveillance and response systems. This project explores the integration of satellite imagery, climate data, and geographic information systems (GIS) to enhance the detection, prediction, and management of diseases transmitted through water and vectors. By harnessing the power of remote sensing technology, this research highlights the potential to identify disease hotspots, track environmental risk factors, and inform targeted interventions. The interdisciplinary collaboration between space agencies and public health entities opens new avenues for early warning systems and evidence-based decision-making, ultimately contributing to the reduction of disease burdens and the promotion of global health.\n\n\nPublic Health Relevance Statement\nIn the ongoing battle against water-borne and vector-borne diseases, the synergy between NASA Earth Observing Data and public health objectives marks a pivotal advancement. By leveraging satellite imagery and climate information, we can enhance our understanding of disease dynamics, identifying regions vulnerable to outbreaks and improving our preparedness. This integration allows for proactive measures, such as deploying resources for disease prevention, implementing targeted vector control, and disseminating timely health advisories to affected populations. As climate change continues to impact disease transmission patterns, harnessing the insights from remote sensing data becomes essential for public health agencies, aiding in their mission to safeguard communities, reduce illness burdens, and save lives.\nKeyword: Water-borne diseases, factors, vibrio cholerae, public health, health impacts, calamities, water."
  },
  {
    "objectID": "research.html#prevention-and-control-of-thalassemia-in-bangladesh",
    "href": "research.html#prevention-and-control-of-thalassemia-in-bangladesh",
    "title": "Research Projects",
    "section": "Prevention and Control of Thalassemia in Bangladesh",
    "text": "Prevention and Control of Thalassemia in Bangladesh\n\nAbstract\nThalassemia, a hereditary blood disorder characterized by abnormal hemoglobin production, poses a significant public health challenge in Bangladesh. This project outlines a comprehensive initiative aimed at the prevention and control of thalassemia within the country. The multifaceted approach integrates awareness campaigns, genetic screening, accessible healthcare services, and community engagement to mitigate the prevalence and impact of thalassemia.\nThe project’s primary objectives include raising public awareness about thalassemia, educating communities about its genetic nature, and promoting informed family planning decisions. Additionally, the initiative seeks to establish a robust infrastructure for genetic screening, enabling early detection and intervention. This involves collaboration with healthcare institutions, laboratories, and medical professionals to ensure accurate and affordable testing.\nTo ensure equitable access to quality healthcare, the project focuses on enhancing healthcare facilities’ capacity to provide specialized thalassemia care. This involves training healthcare personnel, ensuring the availability of necessary treatments, and creating a supportive environment for patients and their families. Moreover, the project emphasizes the importance of support groups and counseling services to address the psychological and emotional aspects of thalassemia.\n\n\nPublic Health Relevance Statement\nThis project’s importance within the realm of public health cannot be overstated. By focusing on awareness campaigns and educational initiatives, the project aims to dispel misconceptions surrounding thalassemia, empower communities with accurate information, and reduce the stigma attached to the disorder. This proactive approach fosters a culture of informed decision-making, particularly in matters of family planning and genetic counseling."
  },
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAdvance Bioinformatics Training\n\n\n\n\n\n\nTraining\n\n\n\nCHIRAL Bangladesh\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nIndustrial Training (Academic)\n\n\n\n\n\n\nTraining\n\n\n\nIndustrial Training\n\n\n\n\n\nJul 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNIB Advanced Bioinformatics Workshop\n\n\n\n\n\n\nTraining\n\n\n\nNIB Advanced Bioinformatics\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nNasal Swab Dissociation Livestream\n\n\n\n\n\n\nWorkshop\n\n\n\nWellcome Nasal Swab Dissociation Livestream\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\n\n\n\n\n\n\nSanger Institute\n\n\n\n\n\n\nTraining\n\n\n\nWellcome Sanger Institute\n\n\n\n\n\nAug 8, 2022\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/undergraduate-research-importance-benefits-and-challenges/index.html",
    "href": "talks/undergraduate-research-importance-benefits-and-challenges/index.html",
    "title": "Advance Bioinformatics Training",
    "section": "",
    "text": "The Undergraduate Research Training Program(URTP) provides intermediate-level biomedical research training for undergraduates in a supportive environment with supplement educational activities. Furthermore, this course is designed to enhance the undergraduate research experience by focusing on critical communication skills for success in research and broadly transferable professional skills. Students will become part of a community of scholars in health sciences. The course is suitable for students beginning the first semester of undergraduate studies and completing their second or"
  },
  {
    "objectID": "teaching/applied-machine-learning-for-healthcare/index.html",
    "href": "teaching/applied-machine-learning-for-healthcare/index.html",
    "title": "Applied Machine Learning for Healthcare (AML4H)",
    "section": "",
    "text": "Applied Machine Learning for Healthcare (AML4H) is an advanced-level course that focuses on the intersection of machine learning and healthcare. This course is designed to provide students with the knowledge and skills needed to effectively apply machine learning techniques to healthcare data, leading to improved medical diagnostics, treatment plans, and patient outcomes. Through a combination of lectures, hands-on projects, and case studies, students will gain a deep understanding of the challenges and opportunities in this rapidly evolving field."
  },
  {
    "objectID": "teaching/applied-machine-learning-for-healthcare/index.html#overview",
    "href": "teaching/applied-machine-learning-for-healthcare/index.html#overview",
    "title": "Applied Machine Learning for Healthcare (AML4H)",
    "section": "",
    "text": "Applied Machine Learning for Healthcare (AML4H) is an advanced-level course that focuses on the intersection of machine learning and healthcare. This course is designed to provide students with the knowledge and skills needed to effectively apply machine learning techniques to healthcare data, leading to improved medical diagnostics, treatment plans, and patient outcomes. Through a combination of lectures, hands-on projects, and case studies, students will gain a deep understanding of the challenges and opportunities in this rapidly evolving field."
  },
  {
    "objectID": "teaching/applied-machine-learning-for-healthcare/index.html#learning-objectives",
    "href": "teaching/applied-machine-learning-for-healthcare/index.html#learning-objectives",
    "title": "Applied Machine Learning for Healthcare (AML4H)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUpon completion of the course, students will be able to:\n\nUnderstand the unique characteristics and challenges of healthcare data.\nIdentify and preprocess various types of healthcare data, including electronic health records (EHR), medical images, and genomics data.\nApply a wide range of machine learning algorithms to healthcare problems, such as classification, regression, clustering, and sequence analysis.\nEvaluate the performance of machine learning models using appropriate metrics for healthcare tasks.\nInterpret and communicate the results of machine learning models to healthcare professionals and stakeholders.\nExplore ethical considerations and potential biases when applying machine learning in healthcare.\nImplement techniques to handle imbalanced datasets and small sample sizes common in medical applications.\nDevelop predictive models for disease diagnosis, prognosis, and patient risk stratification.\nUtilize deep learning techniques for medical image analysis, such as image segmentation and object detection.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/building-dashboard-with-r/index.html",
    "href": "teaching/building-dashboard-with-r/index.html",
    "title": "Building Dashboard with R",
    "section": "",
    "text": "In this comprehensive course, you will learn how to create interactive and informative dashboards using the R programming language. Dashboards are essential tools for data visualization and communication, and R provides powerful libraries and tools to build them. Whether you are a data scientist, analyst, or anyone interested in presenting data effectively, this course will equip you with the skills to design and develop insightful dashboards."
  },
  {
    "objectID": "teaching/building-dashboard-with-r/index.html#course-description",
    "href": "teaching/building-dashboard-with-r/index.html#course-description",
    "title": "Building Dashboard with R",
    "section": "",
    "text": "In this comprehensive course, you will learn how to create interactive and informative dashboards using the R programming language. Dashboards are essential tools for data visualization and communication, and R provides powerful libraries and tools to build them. Whether you are a data scientist, analyst, or anyone interested in presenting data effectively, this course will equip you with the skills to design and develop insightful dashboards."
  },
  {
    "objectID": "teaching/building-dashboard-with-r/index.html#what-youll-learn-in-this-course",
    "href": "teaching/building-dashboard-with-r/index.html#what-youll-learn-in-this-course",
    "title": "Building Dashboard with R",
    "section": "What you’ll learn in this course?",
    "text": "What you’ll learn in this course?\n\nR Markdown\nCreate your own dashboard\nDashboard components and layout\nCombining flexdashboard with Shiny\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/gtsummary-for-public-health/index.html",
    "href": "teaching/gtsummary-for-public-health/index.html",
    "title": "Public Health Insights: Clinical Reporting Using {gtsummary}",
    "section": "",
    "text": "Join us for an engaging and informative workshop on “Public Health Insights: Clinical Reporting Using {gtsummary}.” This workshop is designed to equip public health professionals, researchers, and analysts with the skills and knowledge to effectively utilize the {gtsummary} package for creating comprehensive clinical reports. {gtsummary} is a powerful tool that streamlines the process of summarizing data, making it an invaluable asset for deriving meaningful insights and driving evidence-based decisions in public health contexts.\nThrough hands-on demonstrations and interactive sessions, participants will learn how to leverage {gtsummary} to generate visually appealing and interpretable clinical reports. The workshop will cover essential concepts, best practices, and practical examples that demonstrate the application of {gtsummary} in real-world public health scenarios. Whether you’re a seasoned data analyst or new to the world of data reporting, this workshop will provide you with the tools and techniques needed to elevate your clinical reporting capabilities.\n\n\nThe gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nA critical part of the work of statisticians, data scientists, and analysts is summarizing data sets and regression models in R and publishing or sharing polished summary tables.\nThe gtsummary package was created to streamline these everyday analysis tasks by allowing users to easily create reproducible summaries of data sets, regression models, survey data, and survival data with a simple interface and very little code.\nThe package follows a tidy framework, making it easy to integrate with standard data workflows, and offers many table customization features through function arguments, helper functions, and custom themes.\n\n\n\nBuild, customize, and report tables often found in medical journals and other research-related publications.\n\n\n\nIf your answer to any of the following questions is “yes”, then this is the right workshop for you.\n\nDo you make summary tables in R (data, survey data, regression models, time-to-event data, adverse event reports)?\nDo you want your workflow to be reproducible?\nAre you often frustrated with the immense amount of code required to create great-looking tables in R?\n\nThe workshop is designed for those with some experience in R. It will be expected that you can perform basic data manipulation. Experience with the {tidyverse} and the %&gt;% operator is a plus, but not required.\n\n\n\nBefore attending the workshop please have the following installed and configured on your machine.\n\nRecent version of R\nRecent version of RStudio\nMost recent release of the gtsummary and other packages used in workshop.\ninstll_pkgs &lt;- \n  c(\"gtsummary\", \"tidyverse\", \"labelled\", \"usethis\", \n    \"causaldata\", \"fs\", \"skimr\", \"car\", \"emmeans\")\ninstall.packages(instll_pkgs)\nEnsure you can knit R markdown documents\n\nOpen RStudio and create a new Rmarkdown document\nSave the document and check you are able to knit it.\n\n\nWorkshop Website\n\nRegister"
  },
  {
    "objectID": "teaching/gtsummary-for-public-health/index.html#what-is-gtsummary",
    "href": "teaching/gtsummary-for-public-health/index.html#what-is-gtsummary",
    "title": "Public Health Insights: Clinical Reporting Using {gtsummary}",
    "section": "",
    "text": "The gtsummary package provides an elegant and flexible way to create publication-ready summary tables in R.\nA critical part of the work of statisticians, data scientists, and analysts is summarizing data sets and regression models in R and publishing or sharing polished summary tables.\nThe gtsummary package was created to streamline these everyday analysis tasks by allowing users to easily create reproducible summaries of data sets, regression models, survey data, and survival data with a simple interface and very little code.\nThe package follows a tidy framework, making it easy to integrate with standard data workflows, and offers many table customization features through function arguments, helper functions, and custom themes."
  },
  {
    "objectID": "teaching/gtsummary-for-public-health/index.html#learning-objectives",
    "href": "teaching/gtsummary-for-public-health/index.html#learning-objectives",
    "title": "Public Health Insights: Clinical Reporting Using {gtsummary}",
    "section": "",
    "text": "Build, customize, and report tables often found in medical journals and other research-related publications."
  },
  {
    "objectID": "teaching/gtsummary-for-public-health/index.html#is-this-course-for-me",
    "href": "teaching/gtsummary-for-public-health/index.html#is-this-course-for-me",
    "title": "Public Health Insights: Clinical Reporting Using {gtsummary}",
    "section": "",
    "text": "If your answer to any of the following questions is “yes”, then this is the right workshop for you.\n\nDo you make summary tables in R (data, survey data, regression models, time-to-event data, adverse event reports)?\nDo you want your workflow to be reproducible?\nAre you often frustrated with the immense amount of code required to create great-looking tables in R?\n\nThe workshop is designed for those with some experience in R. It will be expected that you can perform basic data manipulation. Experience with the {tidyverse} and the %&gt;% operator is a plus, but not required."
  },
  {
    "objectID": "teaching/gtsummary-for-public-health/index.html#prework",
    "href": "teaching/gtsummary-for-public-health/index.html#prework",
    "title": "Public Health Insights: Clinical Reporting Using {gtsummary}",
    "section": "",
    "text": "Before attending the workshop please have the following installed and configured on your machine.\n\nRecent version of R\nRecent version of RStudio\nMost recent release of the gtsummary and other packages used in workshop.\ninstll_pkgs &lt;- \n  c(\"gtsummary\", \"tidyverse\", \"labelled\", \"usethis\", \n    \"causaldata\", \"fs\", \"skimr\", \"car\", \"emmeans\")\ninstall.packages(instll_pkgs)\nEnsure you can knit R markdown documents\n\nOpen RStudio and create a new Rmarkdown document\nSave the document and check you are able to knit it.\n\n\nWorkshop Website\n\nRegister"
  },
  {
    "objectID": "teaching/python-for-geospatial-health-data-science/index.html",
    "href": "teaching/python-for-geospatial-health-data-science/index.html",
    "title": "Python for Geospatial Health Data Science (PY4GHDS)",
    "section": "",
    "text": "The Python for Geospatial Health Data Science (PY4GHDS) course is designed to provide an introduction to using Python for analyzing and visualizing geospatial health data. The course covers the basics of Python programming, geospatial data processing, and statistical analysis of health data.\nThroughout the course, students will have the opportunity to work with real-world health and geospatial data to build their skills in Python programming and data analysis. By the end of the course, students will be able to use Python to perform geospatial analysis and statistical analysis of health data, and to communicate their findings through data visualization."
  },
  {
    "objectID": "teaching/python-for-geospatial-health-data-science/index.html#overview",
    "href": "teaching/python-for-geospatial-health-data-science/index.html#overview",
    "title": "Python for Geospatial Health Data Science (PY4GHDS)",
    "section": "",
    "text": "The Python for Geospatial Health Data Science (PY4GHDS) course is designed to provide an introduction to using Python for analyzing and visualizing geospatial health data. The course covers the basics of Python programming, geospatial data processing, and statistical analysis of health data.\nThroughout the course, students will have the opportunity to work with real-world health and geospatial data to build their skills in Python programming and data analysis. By the end of the course, students will be able to use Python to perform geospatial analysis and statistical analysis of health data, and to communicate their findings through data visualization."
  },
  {
    "objectID": "teaching/python-for-geospatial-health-data-science/index.html#why-geospatial-health-data-science",
    "href": "teaching/python-for-geospatial-health-data-science/index.html#why-geospatial-health-data-science",
    "title": "Python for Geospatial Health Data Science (PY4GHDS)",
    "section": "Why Geospatial Health Data Science?",
    "text": "Why Geospatial Health Data Science?\nGeospatial health data science is an important field because it allows us to better understand the spatial distribution of health outcomes and the factors that contribute to them. By analyzing health data in combination with geospatial data, we can identify patterns and trends that would be difficult to detect using traditional statistical methods.\nGeospatial health data science has numerous applications, such as identifying areas of high disease incidence or prevalence, tracking the spread of infectious diseases, and evaluating the impact of environmental exposures on health outcomes. This information can be used to inform public health policies and interventions, as well as to guide resource allocation and target interventions to those who need them most.\nPython is a powerful and widely used programming language that is well-suited for data analysis and visualization. By learning to use Python for geospatial health data science, students will be able to apply these skills to a wide range of health and environmental datasets, allowing them to contribute to this important field and make meaningful contributions to public health.\n Material\n\nApply to the Program"
  },
  {
    "objectID": "teaching/r-for-bioinformatics/index.html",
    "href": "teaching/r-for-bioinformatics/index.html",
    "title": "R for Bioinformatics",
    "section": "",
    "text": "Welcome to the exciting world of bioinformatics! In this course, we will explore the application of the R programming language in the field of bioinformatics. Whether you are a biologist, a computer scientist, or simply someone interested in the intersection of biology and programming, this book is designed to help you get started with using R for bioinformatics.\nBioinformatics combines biology, computer science, statistics, and math to understand complex biological processes. R is a powerful programming language widely used in bioinformatics for data analysis, visualization, and modeling. It has a rich set of packages and libraries for processing biological data.\nThis course assumes no prior knowledge of R programming, but assumes basic understanding of statistics and bioinformatics. We’ll cover data manipulation, visualization, statistics, and common bioinformatics techniques.\nRemember, learning a new language and field takes time and practice. Stay persistent, ask questions, and engage actively with the material. Let’s embark on this exciting journey into bioinformatics with R. By the end, you’ll have the skills to explore and contribute to bioinformatics research.\nEnjoy your learning experience, and good luck!"
  },
  {
    "objectID": "teaching/r-for-bioinformatics/index.html#overview",
    "href": "teaching/r-for-bioinformatics/index.html#overview",
    "title": "R for Bioinformatics",
    "section": "",
    "text": "Welcome to the exciting world of bioinformatics! In this course, we will explore the application of the R programming language in the field of bioinformatics. Whether you are a biologist, a computer scientist, or simply someone interested in the intersection of biology and programming, this book is designed to help you get started with using R for bioinformatics.\nBioinformatics combines biology, computer science, statistics, and math to understand complex biological processes. R is a powerful programming language widely used in bioinformatics for data analysis, visualization, and modeling. It has a rich set of packages and libraries for processing biological data.\nThis course assumes no prior knowledge of R programming, but assumes basic understanding of statistics and bioinformatics. We’ll cover data manipulation, visualization, statistics, and common bioinformatics techniques.\nRemember, learning a new language and field takes time and practice. Stay persistent, ask questions, and engage actively with the material. Let’s embark on this exciting journey into bioinformatics with R. By the end, you’ll have the skills to explore and contribute to bioinformatics research.\nEnjoy your learning experience, and good luck!"
  },
  {
    "objectID": "teaching/r-for-bioinformatics/index.html#learning-objectives",
    "href": "teaching/r-for-bioinformatics/index.html#learning-objectives",
    "title": "R for Bioinformatics",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the fundamentals of bioinformatics and its application in biological research.\nGain a solid foundation in R programming, including data structures, control flow, and functions.\nLearn data manipulation techniques specific to bioinformatics, such as working with DNA sequences and protein structures.\nGain hands-on experience by working with real-world bioinformatics datasets and applying R to extract valuable information.\nUnderstand the importance of reproducibility and best practices in bioinformatics research using R.\nFeel confident in applying R for basic bioinformatics tasks and be ready to explore more advanced topics in the field.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/rna-seq-data-analysis-with-r/index.html",
    "href": "teaching/rna-seq-data-analysis-with-r/index.html",
    "title": "RNA-seq Data Aanalysis with R",
    "section": "",
    "text": "The RNA-seq Data Analysis with R course is designed to provide participants with the knowledge and skills needed to effectively analyze and interpret RNA sequencing (RNA-seq) data using the R programming language. Participants will learn the entire workflow, from raw data preprocessing to differential gene expression analysis and visualization. This hands-on course is suitable for researchers, biologists, and bioinformaticians interested in unlocking insights from gene expression data."
  },
  {
    "objectID": "teaching/rna-seq-data-analysis-with-r/index.html#course-description",
    "href": "teaching/rna-seq-data-analysis-with-r/index.html#course-description",
    "title": "RNA-seq Data Aanalysis with R",
    "section": "",
    "text": "The RNA-seq Data Analysis with R course is designed to provide participants with the knowledge and skills needed to effectively analyze and interpret RNA sequencing (RNA-seq) data using the R programming language. Participants will learn the entire workflow, from raw data preprocessing to differential gene expression analysis and visualization. This hands-on course is suitable for researchers, biologists, and bioinformaticians interested in unlocking insights from gene expression data."
  },
  {
    "objectID": "teaching/rna-seq-data-analysis-with-r/index.html#learning-objectives",
    "href": "teaching/rna-seq-data-analysis-with-r/index.html#learning-objectives",
    "title": "RNA-seq Data Aanalysis with R",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nUnderstand the principles of RNA-seq technology and its applications in gene expression analysis.\nRecognize the steps involved in the RNA-seq workflow, from sample preparation to data analysis.\nPerform quality control assessments on raw RNA-seq data.\nTrim, filter, and preprocess raw reads to ensure data quality.\nAssess the impact of sequencing quality on downstream analyses.\nPerform differential expression analysis to identify genes with significant expression changes between conditions.\nApply statistical tests (e.g., DESeq2, edgeR) to assess differential expression.\nInterpret the results in terms of fold changes and adjusted p-values.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/spss/index.html",
    "href": "teaching/spss/index.html",
    "title": "Research Data Analysis with SPSS",
    "section": "",
    "text": "The Research Data Analysis with SPSS course is tailored for researchers, analysts, and students who want to acquire a strong foundation in using SPSS (Statistical Package for the Social Sciences) for conducting data analysis in research projects. The course covers a range of statistical techniques, data manipulation, and visualization tools within the SPSS environment. Participants will learn how to effectively prepare, analyze, and interpret research data, enabling evidence-based decision-making."
  },
  {
    "objectID": "teaching/spss/index.html#course-description",
    "href": "teaching/spss/index.html#course-description",
    "title": "Research Data Analysis with SPSS",
    "section": "",
    "text": "The Research Data Analysis with SPSS course is tailored for researchers, analysts, and students who want to acquire a strong foundation in using SPSS (Statistical Package for the Social Sciences) for conducting data analysis in research projects. The course covers a range of statistical techniques, data manipulation, and visualization tools within the SPSS environment. Participants will learn how to effectively prepare, analyze, and interpret research data, enabling evidence-based decision-making."
  },
  {
    "objectID": "teaching/spss/index.html#learning-objectives",
    "href": "teaching/spss/index.html#learning-objectives",
    "title": "Research Data Analysis with SPSS",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of the Research Data Analysis with SPSS course, participants will be able to:\n\nFamiliarize with the SPSS interface, tools, and data management features.\nUnderstand the role of SPSS in research data analysis across various fields.\nImport data from different sources (e.g., Excel, CSV) into SPSS.\nHandle missing data, outliers, and data inconsistencies.\nClean and prepare data for analysis.\nCalculate and interpret descriptive statistics, including measures of central tendency and dispersion.\nExploratory Data Analysis\nParametric Tests\nNon-Parametric Tests\nCorrelation and Regression Analysis\nCategorical Data Analysis\nMultivariate Analysis Techniques\nData Visualization in SPSS\nInterpreting and Presenting Results\nBest Practices and Ethical Considerations\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "use.html#literature-review",
    "href": "use.html#literature-review",
    "title": "What I Use",
    "section": "Literature Review",
    "text": "Literature Review\n\ntypeset.io - Typeset.io is a platform designed to assist researchers, academics, and authors in the process of writing, formatting, and preparing documents for publication.\nscholarcy - The AI-powered article summarizer.\nelicit - AI-based research assistant."
  },
  {
    "objectID": "use.html#journal-finders",
    "href": "use.html#journal-finders",
    "title": "What I Use",
    "section": "Journal Finders",
    "text": "Journal Finders\n\nJournal Finder by Elsevier: This tool allows you to enter your manuscript title and abstract to find journals that are a good match for your research. It also provides information on journal metrics and submission guidelines.\nSpringer Journal Suggester: Springer’s tool helps you find the right journal for your research by analyzing your abstract or keywords and suggesting journals that publish articles in your field.\nPubMed Journal Selector: If your research is in the biomedical or life sciences, PubMed’s Journal Selector can assist you in finding journals that match your keywords and research area.\nJANE (Journal/Author Name Estimator): JANE is a free online tool that helps you find journals and authors based on the text of your article’s title and abstract.\nJournalGuide: This tool provides a comprehensive database of journals in various fields and allows you to search for journals by keywords or browse by subject area.\nScopus Journal Finder: Scopus, a bibliographic database, offers a journal finder feature that helps you find journals related to your research area based on keywords or article titles.\nDOAJ (Directory of Open Access Journals): If you’re interested in open-access journals, DOAJ is a directory of freely available scholarly journals that you can search by subject or keyword.\nScimago Journal & Country Rank: On the Scimago platform, you can find information about journals, their rankings, citation data, and more, which can be useful for researchers looking to identify suitable journals for their research or assess the impact and prestige of journals in their field. It’s a valuable tool for academic research and evaluation."
  },
  {
    "objectID": "use.html#data-wrangling",
    "href": "use.html#data-wrangling",
    "title": "What I Use",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nreadxl for importing data into R.\ndplyr, tidyr, and others from the tidyverse for data preparation."
  },
  {
    "objectID": "use.html#data-visualization",
    "href": "use.html#data-visualization",
    "title": "What I Use",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nggplot2 for the majority of the graphics, together with the hrbrtheme for styling.\npatchwork to combine graphics.\nggraph and igraph for most network-related graphics.\nplotly and other HTML widgets for interactive graphics.\nRColorBrewer, viridis, and colormap for color control in charts.\nggrepel and other ggplot2 extensions for simplifying plotting tasks.\nheatmaply for heatmaps."
  },
  {
    "objectID": "use.html#publication-ready-tables",
    "href": "use.html#publication-ready-tables",
    "title": "What I Use",
    "section": "Publication-ready Tables",
    "text": "Publication-ready Tables\n\ngtsummary for creating publication-ready descriptives and analytical tables.\ngt to customize tables and export as docs or tex."
  },
  {
    "objectID": "use.html#reproducible-research",
    "href": "use.html#reproducible-research",
    "title": "What I Use",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nR Markdown to produce statistical reports.\nQuarto to build websites for courses and more."
  },
  {
    "objectID": "use.html#statistical-modeling",
    "href": "use.html#statistical-modeling",
    "title": "What I Use",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nStatic modeling in SPSS entails building and evaluating models that represent relationships within a dataset at a particular point in time. Common methods include ANOVA, descriptive statistics, and regression analysis. Data preparation, variable selection, model construction, and evaluation are part of the process. Coefficients, p-values, and R-squared help interpret output to determine the significance and strength of relationships. Applications include business analytics, social sciences, and market research."
  },
  {
    "objectID": "use.html#guides",
    "href": "use.html#guides",
    "title": "What I Use",
    "section": "Guides",
    "text": "Guides\n\nThe Project Open Data Dashboard gives overview statistics of available government data from various agencies.\nGuide to Open Data Publishing & Analytics - A good article describing best practices for publishing data openly. Is also a good read for those who want to analyze other’s data.\nA short list of data related R packages - packages that either access data or include data\nGoogle’s Data Search Engine"
  },
  {
    "objectID": "use.html#some-data-sources",
    "href": "use.html#some-data-sources",
    "title": "What I Use",
    "section": "Some Data Sources",
    "text": "Some Data Sources\n\nKaggle Data - A growing number of datasets used in Kaggle data analysis contests and available for any other use."
  },
  {
    "objectID": "use.html#infectious-disease-specific",
    "href": "use.html#infectious-disease-specific",
    "title": "What I Use",
    "section": "Infectious Disease Specific",
    "text": "Infectious Disease Specific\n\nGeneral\n\nProject Tycho - infectious disease data\nhttp://www.viprbrc.org\nhttp://eupathdb.org\nClinEpiDB - a database of (a few) clinical epidemiology studies, focusing on infectious diseases.\nImmPort\n\n\n\nInfluenza\n\nhttp://www.fludb.org\n\n&lt;gisaid.org&gt;\nhttp://www.cdc.gov/flu/index.htm\nhttp://www.ncbi.nlm.nih.gov/genomes/FLU/FLU.html\nhttp://www.SystemsInfluenza.org\n\n\n\nTB\n\nOTIS on CDC WONDER: http://wonder.cdc.gov/tb.html"
  },
  {
    "objectID": "use.html#data-sources",
    "href": "use.html#data-sources",
    "title": "What I Use",
    "section": "Data Sources",
    "text": "Data Sources\n\nThe Cancer Genome Atlas (TCGA): Comprehensive multi-dimensional cancer genomics data.\nInternational Cancer Genome Consortium (ICGC): Genomic and clinical data from cancer projects worldwide.\nGene Expression Omnibus (GEO): Public repository for gene expression data, including cancer datasets.\nEuropean Genome-phenome Archive (EGA): Repository for secure storage of human genetic and phenotypic data.\nNational Cancer Institute (NCI) Genomic Data Commons (GDC): Open-access data portal for cancer genomics datasets."
  },
  {
    "objectID": "use.html#analysis-tools",
    "href": "use.html#analysis-tools",
    "title": "What I Use",
    "section": "Analysis Tools",
    "text": "Analysis Tools\n\nGEPIA2: Web tool for analyzing gene expression data in cancer.\nTIMER2.0: Tool for immune infiltrate analysis in cancer.\nUALCAN: Platform for exploring cancer transcriptome data.\ncBioPortal for Cancer Genomics: Collection of cancer genomics datasets for exploration and visualization.\nGREIN: Interactive platform for exploring and analyzing GEO RNA-seq data.\nOncoLnc: Resource for survival analysis and gene expression correlation.\nUCSC Cancer Genomics Browser: Browser for cancer genomics data.\nONCOMINE: Platform for cancer transcriptomic data analysis."
  },
  {
    "objectID": "use.html#r-packages",
    "href": "use.html#r-packages",
    "title": "What I Use",
    "section": "R Packages",
    "text": "R Packages\n\nTCGAbiolinks: R package for integrative analysis with GDC data.\nmaftools: R package for summarizing, analyzing, and visualizing MAF files.\nSummarizedExperiment: Container for genomic data assays.\nMutationalPatterns: Tool for genome-wide analysis of mutational processes."
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "",
    "text": "The goal of this project is to develop a machine learning model that can accurately predict the number of calories burnt during physical activities. Accurate calorie prediction is crucial for individuals seeking to monitor and manage their fitness and weight loss goals. Existing methods for estimating calorie expenditure often lack precision and rely on generalized formulas that do not consider individual variations. By leveraging machine learning techniques, this project aims to create a more personalized and accurate prediction model that takes into account various factors such as heart rate, duration of activity, body metrics, and other relevant features. The developed model will enable individuals to track their calorie expenditure more effectively, optimize their exercise routines, and make informed decisions for achieving their fitness objectives."
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#problem",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#problem",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "",
    "text": "The goal of this project is to develop a machine learning model that can accurately predict the number of calories burnt during physical activities. Accurate calorie prediction is crucial for individuals seeking to monitor and manage their fitness and weight loss goals. Existing methods for estimating calorie expenditure often lack precision and rely on generalized formulas that do not consider individual variations. By leveraging machine learning techniques, this project aims to create a more personalized and accurate prediction model that takes into account various factors such as heart rate, duration of activity, body metrics, and other relevant features. The developed model will enable individuals to track their calorie expenditure more effectively, optimize their exercise routines, and make informed decisions for achieving their fitness objectives."
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#objective",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#objective",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Objective",
    "text": "Objective\nDevelop a machine learning model to accurately predict calories burnt during physical activities, enabling individuals to track their calorie expenditure and optimize their fitness routines.\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv('../data/calories_burnt.csv')\ndata.head()\n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\n\n\n\n\n0\n14733363\nmale\n68\n190.0\n94.0\n29.0\n105.0\n40.8\n231.0\n\n\n1\n14861698\nfemale\n20\n166.0\n60.0\n14.0\n94.0\n40.3\n66.0\n\n\n2\n11179863\nmale\n69\n179.0\n79.0\n5.0\n88.0\n38.7\n26.0\n\n\n3\n16180408\nfemale\n34\n179.0\n71.0\n13.0\n100.0\n40.5\n71.0\n\n\n4\n17771927\nfemale\n27\n154.0\n58.0\n10.0\n81.0\n39.8\n35.0"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#exploring-data",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#exploring-data",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(15000, 9)\n\n\n\n# dtypes \ndata.dtypes\n\nUser_ID         int64\nGender         object\nAge             int64\nHeight        float64\nWeight        float64\nDuration      float64\nHeart_Rate    float64\nBody_Temp     float64\nCalories      float64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 15000 entries, 0 to 14999\nData columns (total 9 columns):\n #   Column      Non-Null Count  Dtype  \n---  ------      --------------  -----  \n 0   User_ID     15000 non-null  int64  \n 1   Gender      15000 non-null  object \n 2   Age         15000 non-null  int64  \n 3   Height      15000 non-null  float64\n 4   Weight      15000 non-null  float64\n 5   Duration    15000 non-null  float64\n 6   Heart_Rate  15000 non-null  float64\n 7   Body_Temp   15000 non-null  float64\n 8   Calories    15000 non-null  float64\ndtypes: float64(6), int64(2), object(1)\nmemory usage: 1.0+ MB\n\n\n\n# check missing data \ndata.isnull().sum() \n\nUser_ID       0\nGender        0\nAge           0\nHeight        0\nWeight        0\nDuration      0\nHeart_Rate    0\nBody_Temp     0\nCalories      0\ndtype: int64\n\n\n\ndata.columns\n\nIndex(['User_ID', 'Gender', 'Age', 'Height', 'Weight', 'Duration',\n       'Heart_Rate', 'Body_Temp', 'Calories'],\n      dtype='object')"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#descriptive-statistics",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#descriptive-statistics",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# summary statistics of numerical variables \nrp.summary_cont(data[['Age', 'Height', 'Weight', 'Duration','Heart_Rate', 'Body_Temp', 'Calories']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nAge\n15000.0\n42.7898\n16.9803\n0.1386\n42.5180\n43.0616\n\n\n1\nHeight\n15000.0\n174.4651\n14.2581\n0.1164\n174.2369\n174.6933\n\n\n2\nWeight\n15000.0\n74.9669\n15.0357\n0.1228\n74.7262\n75.2075\n\n\n3\nDuration\n15000.0\n15.5306\n8.3192\n0.0679\n15.3975\n15.6637\n\n\n4\nHeart_Rate\n15000.0\n95.5185\n9.5833\n0.0782\n95.3652\n95.6719\n\n\n5\nBody_Temp\n15000.0\n40.0255\n0.7792\n0.0064\n40.0130\n40.0379\n\n\n6\nCalories\n15000.0\n89.5395\n62.4570\n0.5100\n88.5400\n90.5391\n\n\n\n\n\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(data['Gender'])\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\nGender\nfemale\n7553\n50.35\n\n\n1\n\nmale\n7447\n49.65"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#correlations-between-variables",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#correlations-between-variables",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_16664\\427603040.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.corr(method='pearson')\n\n\n\n\n\n\n\n\n\nUser_ID\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\n\n\n\n\nUser_ID\n1.000000\n-0.001827\n-0.013520\n-0.011603\n-0.002751\n-0.000457\n0.000923\n-0.001661\n\n\nAge\n-0.001827\n1.000000\n0.009554\n0.090094\n0.013247\n0.010482\n0.013175\n0.154395\n\n\nHeight\n-0.013520\n0.009554\n1.000000\n0.958451\n-0.004625\n0.000528\n0.001200\n0.017537\n\n\nWeight\n-0.011603\n0.090094\n0.958451\n1.000000\n-0.001884\n0.004311\n0.004095\n0.035481\n\n\nDuration\n-0.002751\n0.013247\n-0.004625\n-0.001884\n1.000000\n0.852869\n0.903167\n0.955421\n\n\nHeart_Rate\n-0.000457\n0.010482\n0.000528\n0.004311\n0.852869\n1.000000\n0.771529\n0.897882\n\n\nBody_Temp\n0.000923\n0.013175\n0.001200\n0.004095\n0.903167\n0.771529\n1.000000\n0.824558\n\n\nCalories\n-0.001661\n0.154395\n0.017537\n0.035481\n0.955421\n0.897882\n0.824558\n1.000000"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#skewness",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#skewness",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_16664\\942340472.py:2: FutureWarning: The default value of numeric_only in DataFrame.skew is deprecated. In a future version, it will default to False. In addition, specifying 'numeric_only=None' is deprecated. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.skew()\n\n\nUser_ID       0.004788\nAge           0.473383\nHeight       -0.006190\nWeight        0.226725\nDuration      0.004751\nHeart_Rate   -0.010704\nBody_Temp    -0.994382\nCalories      0.505371\ndtype: float64"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#data-visualizations",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#data-visualizations",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(3,4))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(3,4))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(10,6))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_16664\\2539154758.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr = data.corr()"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#setup",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#setup",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\n\n\n\n\n0\n14733363\nmale\n68\n190.0\n94.0\n29.0\n105.0\n40.8\n231.0\n\n\n1\n14861698\nfemale\n20\n166.0\n60.0\n14.0\n94.0\n40.3\n66.0\n\n\n2\n11179863\nmale\n69\n179.0\n79.0\n5.0\n88.0\n38.7\n26.0\n\n\n3\n16180408\nfemale\n34\n179.0\n71.0\n13.0\n100.0\n40.5\n71.0\n\n\n4\n17771927\nfemale\n27\n154.0\n58.0\n10.0\n81.0\n39.8\n35.0\n\n\n\n\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.regression import *\nsetup(data, target = 'Calories', train_size = .08, session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nCalories\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(15000, 9)\n\n\n4\nTransformed data shape\n(15000, 9)\n\n\n5\nTransformed train set shape\n(1200, 9)\n\n\n6\nTransformed test set shape\n(13800, 9)\n\n\n7\nOrdinal features\n1\n\n\n8\nNumeric features\n7\n\n\n9\nCategorical features\n1\n\n\n10\nPreprocess\nTrue\n\n\n11\nImputation type\nsimple\n\n\n12\nNumeric imputation\nmean\n\n\n13\nCategorical imputation\nmode\n\n\n14\nMaximum one-hot encoding\n25\n\n\n15\nEncoding method\nNone\n\n\n16\nFold Generator\nKFold\n\n\n17\nFold Number\n10\n\n\n18\nCPU Jobs\n-1\n\n\n19\nUse GPU\nFalse\n\n\n20\nLog Experiment\nFalse\n\n\n21\nExperiment Name\nreg-default-name\n\n\n22\nUSI\nf9a6\n\n\n\n\n\n&lt;pycaret.regression.oop.RegressionExperiment at 0x21553a88af0&gt;"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#compare-models",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#compare-models",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\ngbr\nGradient Boosting Regressor\n3.3280\n23.6355\n4.7654\n0.9941\n0.1121\n0.0703\n0.2560\n\n\net\nExtra Trees Regressor\n3.0345\n25.0997\n4.8829\n0.9938\n0.0766\n0.0514\n0.2770\n\n\nlightgbm\nLight Gradient Boosting Machine\n2.9355\n26.1030\n4.7990\n0.9937\n0.0782\n0.0505\n0.3400\n\n\nrf\nRandom Forest Regressor\n4.1750\n45.4263\n6.5779\n0.9887\n0.0917\n0.0654\n0.2870\n\n\ndt\nDecision Tree Regressor\n7.2508\n114.9292\n10.6882\n0.9703\n0.1401\n0.1057\n0.2130\n\n\nlr\nLinear Regression\n8.2643\n129.8015\n11.3331\n0.9669\n0.3870\n0.2764\n0.7340\n\n\nridge\nRidge Regression\n8.2641\n129.8007\n11.3329\n0.9669\n0.3871\n0.2767\n0.2130\n\n\nbr\nBayesian Ridge\n8.2651\n129.8276\n11.3336\n0.9669\n0.3881\n0.2776\n0.2120\n\n\nlar\nLeast Angle Regression\n8.4840\n136.2552\n11.5893\n0.9652\n0.4087\n0.2885\n0.2120\n\n\nada\nAdaBoost Regressor\n9.0554\n137.0731\n11.6582\n0.9649\n0.3668\n0.3492\n0.2430\n\n\nlasso\nLasso Regression\n8.7100\n140.8683\n11.7934\n0.9642\n0.4010\n0.3198\n0.2380\n\n\nllar\nLasso Least Angle Regression\n8.7100\n140.8702\n11.7935\n0.9642\n0.4010\n0.3198\n0.2150\n\n\nen\nElastic Net\n9.2136\n153.0708\n12.2966\n0.9610\n0.4128\n0.3549\n0.2100\n\n\nhuber\nHuber Regressor\n50.9606\n3848.3586\n61.6837\n0.0098\n0.9556\n1.6508\n0.2140\n\n\ndummy\nDummy Regressor\n53.4677\n3905.7786\n62.3983\n-0.0036\n1.0151\n1.9956\n0.2280\n\n\nomp\nOrthogonal Matching Pursuit\n53.4851\n3916.4631\n62.4829\n-0.0064\n1.0158\n1.9972\n0.2110\n\n\nknn\nK Neighbors Regressor\n57.5917\n4789.6908\n69.1247\n-0.2364\n1.0592\n2.0712\n0.2120\n\n\npar\nPassive Aggressive Regressor\n65.8159\n7180.2013\n84.0894\n-0.8645\n1.3472\n1.2131\n0.2140"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#create-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#create-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \ngbr = create_model('gbr')\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n3.3555\n20.8547\n4.5667\n0.9947\n0.1143\n0.0703\n\n\n1\n3.0126\n18.4520\n4.2956\n0.9955\n0.1406\n0.0836\n\n\n2\n3.1049\n17.2076\n4.1482\n0.9955\n0.1126\n0.0781\n\n\n3\n3.3342\n25.6960\n5.0691\n0.9921\n0.1001\n0.0651\n\n\n4\n3.1462\n19.7783\n4.4473\n0.9944\n0.0764\n0.0507\n\n\n5\n3.2958\n22.8887\n4.7842\n0.9940\n0.1339\n0.0690\n\n\n6\n3.3478\n20.2980\n4.5053\n0.9949\n0.1078\n0.0740\n\n\n7\n2.9264\n15.4323\n3.9284\n0.9955\n0.0850\n0.0525\n\n\n8\n3.2084\n19.3546\n4.3994\n0.9952\n0.1129\n0.0759\n\n\n9\n4.5476\n56.3927\n7.5095\n0.9887\n0.1375\n0.0844\n\n\nMean\n3.3280\n23.6355\n4.7654\n0.9941\n0.1121\n0.0703\n\n\nStd\n0.4298\n11.2490\n0.9627\n0.0020\n0.0203\n0.0110\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(gbr)\n\nGradientBoostingRegressor(random_state=123)"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#tune-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#tune-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of et\ntuned_gbr = tune_model(gbr)\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n2.5783\n21.9401\n4.6840\n0.9944\n0.0578\n0.0419\n\n\n1\n2.8951\n21.5455\n4.6417\n0.9947\n0.0724\n0.0523\n\n\n2\n2.6212\n14.9204\n3.8627\n0.9961\n0.0594\n0.0426\n\n\n3\n2.8785\n19.8920\n4.4600\n0.9939\n0.0661\n0.0463\n\n\n4\n2.7726\n17.0033\n4.1235\n0.9952\n0.0613\n0.0439\n\n\n5\n2.6645\n16.2233\n4.0278\n0.9958\n0.0521\n0.0389\n\n\n6\n2.9591\n21.0932\n4.5927\n0.9947\n0.0672\n0.0457\n\n\n7\n2.7118\n15.4020\n3.9245\n0.9956\n0.0587\n0.0427\n\n\n8\n3.1376\n23.6521\n4.8633\n0.9941\n0.0996\n0.0615\n\n\n9\n4.3489\n71.2167\n8.4390\n0.9858\n0.0816\n0.0582\n\n\nMean\n2.9568\n24.2889\n4.7619\n0.9940\n0.0676\n0.0474\n\n\nStd\n0.4915\n15.9063\n1.2700\n0.0028\n0.0133\n0.0071\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# print tuned model\nprint(tuned_gbr)\n\nGradientBoostingRegressor(random_state=123)\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_gbr, tuner = tune_model(gbr, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n2.5783\n21.9401\n4.6840\n0.9944\n0.0578\n0.0419\n\n\n1\n2.8951\n21.5455\n4.6417\n0.9947\n0.0724\n0.0523\n\n\n2\n2.6212\n14.9204\n3.8627\n0.9961\n0.0594\n0.0426\n\n\n3\n2.8785\n19.8920\n4.4600\n0.9939\n0.0661\n0.0463\n\n\n4\n2.7726\n17.0033\n4.1235\n0.9952\n0.0613\n0.0439\n\n\n5\n2.6645\n16.2233\n4.0278\n0.9958\n0.0521\n0.0389\n\n\n6\n2.9591\n21.0932\n4.5927\n0.9947\n0.0672\n0.0457\n\n\n7\n2.7118\n15.4020\n3.9245\n0.9956\n0.0587\n0.0427\n\n\n8\n3.1376\n23.6521\n4.8633\n0.9941\n0.0996\n0.0615\n\n\n9\n4.3489\n71.2167\n8.4390\n0.9858\n0.0816\n0.0582\n\n\nMean\n2.9568\n24.2889\n4.7619\n0.9940\n0.0676\n0.0474\n\n\nStd\n0.4915\n15.9063\n1.2700\n0.0028\n0.0133\n0.0071\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_gbr\n\nGradientBoostingRegressor(random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(include=['User_ID',\n                                                                          'Age',\n                                                                          'Height',\n                                                                          'Weight',\n                                                                          'Duration',\n                                                                          'Heart_Rate',\n                                                                          'Body_Temp'],\n                                                                 transformer=SimpleImputer())),\n                                             ('categorical_imputer',\n                                              Transf...\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                4,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300],\n                                        'actual_estimator__subsample': [0.2,\n                                                                        0.25,\n                                                                        0.3,\n                                                                        0.35,\n                                                                        0.4,\n                                                                        0.45,\n                                                                        0.5,\n                                                                        0.55,\n                                                                        0.6,\n                                                                        0.65,\n                                                                        0.7,\n                                                                        0.75,\n                                                                        0.8,\n                                                                        0.85,\n                                                                        0.9,\n                                                                        0.95,\n                                                                        1.0]},\n                   random_state=123, refit=False, scoring='r2', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(include=['User_ID',\n                                                                          'Age',\n                                                                          'Height',\n                                                                          'Weight',\n                                                                          'Duration',\n                                                                          'Heart_Rate',\n                                                                          'Body_Temp'],\n                                                                 transformer=SimpleImputer())),\n                                             ('categorical_imputer',\n                                              Transf...\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                4,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300],\n                                        'actual_estimator__subsample': [0.2,\n                                                                        0.25,\n                                                                        0.3,\n                                                                        0.35,\n                                                                        0.4,\n                                                                        0.45,\n                                                                        0.5,\n                                                                        0.55,\n                                                                        0.6,\n                                                                        0.65,\n                                                                        0.7,\n                                                                        0.75,\n                                                                        0.8,\n                                                                        0.85,\n                                                                        0.9,\n                                                                        0.95,\n                                                                        1.0]},\n                   random_state=123, refit=False, scoring='r2', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                             'Weight', 'Duration', 'Heart_Rate',\n                                             'Body_Temp'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('ordinal_encoding',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=OrdinalEncoder(cols=['Gender'],\n                                                               handle_missing='return_nan',\n                                                               mapping=[{'col': 'Gender',\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))),\n                ('actual_estimator',\n                 GradientBoostingRegressor(random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['User_ID', 'Age', 'Height', 'Weight', 'Duration',\n                            'Heart_Rate', 'Body_Temp'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])GradientBoostingRegressorGradientBoostingRegressor(random_state=123)"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#analyze-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#analyze-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# residuals plot \nplot_model(gbr, plot = 'residuals')\n\n\n\n\n\n\n\n\n\n\n\n\n# predicting error plot \nplot_model(gbr, plot = 'error')\n\n\n\n\n\n\n\n\n\n\n\n\n# cooks distance plot \nplot_model(gbr, plot = 'cooks')\n\n\n\n\n\n\n\n\n\n\n\n\n# recursive feature selection\nplot_model(gbr, plot = 'rfe')\n\n\n\n\n\n\n\n\n\n\n\n\n# learning curve \nplot_model(gbr, plot = 'learning')\n\n\n\n\n\n\n\n\n\n\n\n\n# validation curve \nplot_model(gbr, plot = 'vc')\n\n\n\n\n\n\n\n\n\n\n\n\n# manifold learning plot \nplot_model(gbr, plot = 'manifold')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(gbr, plot = 'feature')"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#evaluate-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#evaluate-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(gbr)"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#finalize-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#finalize-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(gbr)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                             'Weight', 'Duration', 'Heart_Rate',\n                                             'Body_Temp'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('ordinal_encoding',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=OrdinalEncoder(cols=['Gender'],\n                                                               handle_missing='return_nan',\n                                                               mapping=[{'col': 'Gender',\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))),\n                ('actual_estimator',\n                 GradientBoostingRegressor(random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                             'Weight', 'Duration', 'Heart_Rate',\n                                             'Body_Temp'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('ordinal_encoding',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=OrdinalEncoder(cols=['Gender'],\n                                                               handle_missing='return_nan',\n                                                               mapping=[{'col': 'Gender',\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))),\n                ('actual_estimator',\n                 GradientBoostingRegressor(random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['User_ID', 'Age', 'Height', 'Weight', 'Duration',\n                            'Heart_Rate', 'Body_Temp'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])GradientBoostingRegressorGradientBoostingRegressor(random_state=123)"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#prediction",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#prediction",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(gbr)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nGradient Boosting Regressor\n3.3690\n23.8653\n4.8852\n0.9939\n0.1179\n0.0679\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\nprediction_label\n\n\n\n\n6958\n15399629\nfemale\n71\n166.0\n58.0\n20.0\n99.0\n40.900002\n123.0\n134.324785\n\n\n7534\n17761176\nmale\n22\n191.0\n92.0\n2.0\n84.0\n38.099998\n5.0\n0.573631\n\n\n2975\n14032506\nfemale\n50\n152.0\n48.0\n13.0\n87.0\n39.900002\n60.0\n59.109736\n\n\n3903\n15190143\nfemale\n76\n170.0\n73.0\n3.0\n74.0\n38.099998\n10.0\n8.083763\n\n\n8437\n18418052\nfemale\n67\n160.0\n61.0\n5.0\n79.0\n38.900002\n20.0\n18.910678\n\n\n\n\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('Calories', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\n\n\n\n\n0\n14733363\nmale\n68\n190.0\n94.0\n29.0\n105.0\n40.8\n\n\n1\n14861698\nfemale\n20\n166.0\n60.0\n14.0\n94.0\n40.3\n\n\n2\n11179863\nmale\n69\n179.0\n79.0\n5.0\n88.0\n38.7\n\n\n3\n16180408\nfemale\n34\n179.0\n71.0\n13.0\n100.0\n40.5\n\n\n4\n17771927\nfemale\n27\n154.0\n58.0\n10.0\n81.0\n39.8\n\n\n\n\n\n\n\n\n# predict model on new_data\npredictions = predict_model(gbr, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nprediction_label\n\n\n\n\n0\n14733363\nmale\n68\n190.0\n94.0\n29.0\n105.0\n40.799999\n216.742066\n\n\n1\n14861698\nfemale\n20\n166.0\n60.0\n14.0\n94.0\n40.299999\n67.646251\n\n\n2\n11179863\nmale\n69\n179.0\n79.0\n5.0\n88.0\n38.700001\n25.152727\n\n\n3\n16180408\nfemale\n34\n179.0\n71.0\n13.0\n100.0\n40.500000\n72.968404\n\n\n4\n17771927\nfemale\n27\n154.0\n58.0\n10.0\n81.0\n39.799999\n36.552712"
  },
  {
    "objectID": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#save-model",
    "href": "projects/Calories_Burnt/02_Calories Burnt Prediction.html#save-model",
    "title": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(gbr, '../models/calories')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                              'Weight', 'Duration', 'Heart_Rate',\n                                              'Body_Temp'],\n                                     transformer=SimpleImputer())),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=['Gender'],\n                                     transformer=SimpleImputer(strategy='most_frequent'))),\n                 ('ordinal_encoding',\n                  TransformerWrapper(include=['Gender'],\n                                     transformer=OrdinalEncoder(cols=['Gender'],\n                                                                handle_missing='return_nan',\n                                                                mapping=[{'col': 'Gender',\n                                                                          'data_type': dtype('O'),\n                                                                          'mapping': female    0\n male      1\n NaN      -1\n dtype: int64}]))),\n                 ('trained_model', GradientBoostingRegressor(random_state=123))]),\n '../models/calories.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/calories')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                             'Weight', 'Duration', 'Heart_Rate',\n                                             'Body_Temp'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('ordinal_encoding',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=OrdinalEncoder(cols=['Gender'],\n                                                               handle_missing='return_nan',\n                                                               mapping=[{'col': 'Gender',\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))),\n                ('trained_model', GradientBoostingRegressor(random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['User_ID', 'Age', 'Height',\n                                             'Weight', 'Duration', 'Heart_Rate',\n                                             'Body_Temp'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('ordinal_encoding',\n                 TransformerWrapper(include=['Gender'],\n                                    transformer=OrdinalEncoder(cols=['Gender'],\n                                                               handle_missing='return_nan',\n                                                               mapping=[{'col': 'Gender',\n                                                                         'data_type': dtype('O'),\n                                                                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))),\n                ('trained_model', GradientBoostingRegressor(random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['User_ID', 'Age', 'Height', 'Weight', 'Duration',\n                            'Heart_Rate', 'Body_Temp'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['Gender'],\n                   transformer=OrdinalEncoder(cols=['Gender'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'Gender',\n                                                        'data_type': dtype('O'),\n                                                        'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['Gender'], handle_missing='return_nan',\n               mapping=[{'col': 'Gender', 'data_type': dtype('O'),\n                         'mapping': female    0\nmale      1\nNaN      -1\ndtype: int64}])GradientBoostingRegressorGradientBoostingRegressor(random_state=123)\n\n\n\n# prediction\nprediction_holdout = predict_model(gbr)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nGradient Boosting Regressor\n3.3690\n23.8653\n4.8852\n0.9939\n0.1179\n0.0679\n\n\n\n\n\n\nprediction_holdout.head()\n\n\n\n\n\n\n\n\nUser_ID\nGender\nAge\nHeight\nWeight\nDuration\nHeart_Rate\nBody_Temp\nCalories\nprediction_label\n\n\n\n\n6958\n15399629\nfemale\n71\n166.0\n58.0\n20.0\n99.0\n40.900002\n123.0\n134.324785\n\n\n7534\n17761176\nmale\n22\n191.0\n92.0\n2.0\n84.0\n38.099998\n5.0\n0.573631\n\n\n2975\n14032506\nfemale\n50\n152.0\n48.0\n13.0\n87.0\n39.900002\n60.0\n59.109736\n\n\n3903\n15190143\nfemale\n76\n170.0\n73.0\n3.0\n74.0\n38.099998\n10.0\n8.083763\n\n\n8437\n18418052\nfemale\n67\n160.0\n61.0\n5.0\n79.0\n38.900002\n20.0\n18.910678"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html",
    "href": "projects/ESOL/01_ESOL.html",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "",
    "text": "This paper describes a simple method for estimating the aqueous solubility (ESOL − Estimated SOLubility) of a compound directly from its structure. The model was derived from a set of 2874 measured solubilities using linear regression against nine molecular properties. The most significant parameter was calculated, followed by molecular weight, proportion of heavy atoms in aromatic systems, and number of rotatable bonds. The model performed consistently well across three validation sets, predicting solubilities within a factor of 5−8 of their measured values, and was competitive with the well-established “General Solubility Equation” for medicinal/agrochemical sized molecules."
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#problem",
    "href": "projects/ESOL/01_ESOL.html#problem",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "",
    "text": "This paper describes a simple method for estimating the aqueous solubility (ESOL − Estimated SOLubility) of a compound directly from its structure. The model was derived from a set of 2874 measured solubilities using linear regression against nine molecular properties. The most significant parameter was calculated, followed by molecular weight, proportion of heavy atoms in aromatic systems, and number of rotatable bonds. The model performed consistently well across three validation sets, predicting solubilities within a factor of 5−8 of their measured values, and was competitive with the well-established “General Solubility Equation” for medicinal/agrochemical sized molecules."
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#goals",
    "href": "projects/ESOL/01_ESOL.html#goals",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Goals",
    "text": "Goals\n\nReproduce the Delaney’s paper using Python\nTo learn more about Computational Drug Discovery"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#references",
    "href": "projects/ESOL/01_ESOL.html#references",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "References",
    "text": "References\n[1] Delaney, John S. (2004). ESOL: estimating aqueous solubility directly from molecular structure. Journal of chemical information and computer sciences.\n[2] Chanin Nantasenamat. How to Use Machine Learning for Drug Discovery. https://towardsdatascience.com/how-to-use-machine-learning-for-drug-discovery-1ccb5fdf81ad\n[3] Deep Learning for the Life Sciences: Applying Deep Learning to Genomics, Microscopy, Drug Discovery, and More 1st Edition.\n[4] Pat Walters. Predicting Aqueous Solubility - It’s Harder Than It Looks. http://practicalcheminformatics.blogspot.com/2018/09/predicting-aqueous-solubility-its.html\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\ndata = pd.read_csv('https://raw.githubusercontent.com/dataprofessor/data/master/delaney_solubility_with_descriptors.csv')\ndata.head()\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nlogS\n\n\n\n\n0\n2.5954\n167.850\n0.0\n0.0\n-2.18\n\n\n1\n2.3765\n133.405\n0.0\n0.0\n-2.00\n\n\n2\n2.5938\n167.850\n1.0\n0.0\n-1.74\n\n\n3\n2.0289\n133.405\n1.0\n0.0\n-1.48\n\n\n4\n2.9189\n187.375\n1.0\n0.0\n-3.04"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#exploring-data",
    "href": "projects/ESOL/01_ESOL.html#exploring-data",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(1144, 5)\n\n\n\n# dtypes \ndata.dtypes\n\nMolLogP               float64\nMolWt                 float64\nNumRotatableBonds     float64\nAromaticProportion    float64\nlogS                  float64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 1144 entries, 0 to 1143\nData columns (total 5 columns):\n #   Column              Non-Null Count  Dtype  \n---  ------              --------------  -----  \n 0   MolLogP             1144 non-null   float64\n 1   MolWt               1144 non-null   float64\n 2   NumRotatableBonds   1144 non-null   float64\n 3   AromaticProportion  1144 non-null   float64\n 4   logS                1144 non-null   float64\ndtypes: float64(5)\nmemory usage: 44.8 KB\n\n\n\n# check missing data \ndata.isnull().sum() \n\nMolLogP               0\nMolWt                 0\nNumRotatableBonds     0\nAromaticProportion    0\nlogS                  0\ndtype: int64"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#descriptive-statistics",
    "href": "projects/ESOL/01_ESOL.html#descriptive-statistics",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# summary statistics of numerical variables \nrp.summary_cont(data[['MolLogP', 'MolWt', 'NumRotatableBonds', 'AromaticProportion', 'logS']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nMolLogP\n1144.0\n2.4491\n1.8660\n0.0552\n2.3409\n2.5574\n\n\n1\nMolWt\n1144.0\n204.6317\n102.6205\n3.0340\n198.6788\n210.5846\n\n\n2\nNumRotatableBonds\n1144.0\n2.1740\n2.6274\n0.0777\n2.0215\n2.3264\n\n\n3\nAromaticProportion\n1144.0\n0.3649\n0.3433\n0.0102\n0.3450\n0.3848\n\n\n4\nlogS\n1144.0\n-3.0580\n2.0965\n0.0620\n-3.1796\n-2.9364"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#correlations-between-variables",
    "href": "projects/ESOL/01_ESOL.html#correlations-between-variables",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nlogS\n\n\n\n\nMolLogP\n1.000000\n0.468097\n0.205607\n0.250977\n-0.827959\n\n\nMolWt\n0.468097\n1.000000\n0.394219\n0.098855\n-0.637324\n\n\nNumRotatableBonds\n0.205607\n0.394219\n1.000000\n-0.296737\n-0.238508\n\n\nAromaticProportion\n0.250977\n0.098855\n-0.296737\n1.000000\n-0.268108\n\n\nlogS\n-0.827959\n-0.637324\n-0.238508\n-0.268108\n1.000000"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#skewness",
    "href": "projects/ESOL/01_ESOL.html#skewness",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nMolLogP              -0.012982\nMolWt                 0.890477\nNumRotatableBonds     2.158178\nAromaticProportion    0.226302\nlogS                 -0.484590\ndtype: float64"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#data-visualizations",
    "href": "projects/ESOL/01_ESOL.html#data-visualizations",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.hist(figsize=(20,10), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.plot(kind='density', subplots=True, sharex=False, figsize=(20,10), layout=(2,3))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.plot(kind='box', subplots=True, sharex=False, figsize=(20,10), layout=(2,3))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(10,6))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#setup",
    "href": "projects/ESOL/01_ESOL.html#setup",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nlogS\n\n\n\n\n0\n2.5954\n167.850\n0.0\n0.0\n-2.18\n\n\n1\n2.3765\n133.405\n0.0\n0.0\n-2.00\n\n\n2\n2.5938\n167.850\n1.0\n0.0\n-1.74\n\n\n3\n2.0289\n133.405\n1.0\n0.0\n-1.48\n\n\n4\n2.9189\n187.375\n1.0\n0.0\n-3.04\n\n\n\n\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.regression import *\nsetup(data, target = 'logS', train_size = .08, session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nlogS\n\n\n2\nTarget type\nRegression\n\n\n3\nOriginal data shape\n(1144, 5)\n\n\n4\nTransformed data shape\n(1144, 5)\n\n\n5\nTransformed train set shape\n(91, 5)\n\n\n6\nTransformed test set shape\n(1053, 5)\n\n\n7\nNumeric features\n4\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nFold Generator\nKFold\n\n\n13\nFold Number\n10\n\n\n14\nCPU Jobs\n-1\n\n\n15\nUse GPU\nFalse\n\n\n16\nLog Experiment\nFalse\n\n\n17\nExperiment Name\nreg-default-name\n\n\n18\nUSI\n939a\n\n\n\n\n\n&lt;pycaret.regression.oop.RegressionExperiment at 0x234b3f52f80&gt;"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#compare-models",
    "href": "projects/ESOL/01_ESOL.html#compare-models",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\nTT (Sec)\n\n\n\n\net\nExtra Trees Regressor\n0.5937\n0.6494\n0.7821\n0.7390\n0.2352\n0.7942\n0.2110\n\n\nada\nAdaBoost Regressor\n0.6485\n0.7106\n0.8192\n0.7194\n0.2357\n0.9273\n0.2070\n\n\nrf\nRandom Forest Regressor\n0.6380\n0.7167\n0.8258\n0.7145\n0.2340\n0.7976\n0.2080\n\n\ngbr\nGradient Boosting Regressor\n0.7261\n0.9255\n0.9299\n0.6609\n0.2792\n0.7807\n0.2070\n\n\ndt\nDecision Tree Regressor\n0.7969\n1.1005\n1.0041\n0.5997\n0.2734\n0.6608\n0.2010\n\n\nbr\nBayesian Ridge\n0.7751\n1.0074\n0.9821\n0.5870\n0.3088\n0.7389\n0.2040\n\n\nridge\nRidge Regression\n0.7766\n1.0127\n0.9846\n0.5803\n0.3134\n0.7068\n0.1980\n\n\nlar\nLeast Angle Regression\n0.7777\n1.0167\n0.9865\n0.5770\n0.3149\n0.6985\n0.2040\n\n\nlr\nLinear Regression\n0.7777\n1.0167\n0.9865\n0.5770\n0.3149\n0.6985\n0.6830\n\n\nhuber\nHuber Regressor\n0.7791\n1.0403\n0.9984\n0.5625\n0.3151\n0.6069\n0.2030\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.8206\n1.1789\n1.0643\n0.5261\n0.2703\n0.8863\n0.3000\n\n\nen\nElastic Net\n0.9021\n1.2814\n1.1006\n0.5089\n0.3079\n1.2158\n0.2000\n\n\nlasso\nLasso Regression\n1.0322\n1.6618\n1.2442\n0.3811\n0.3369\n1.4883\n0.2220\n\n\nllar\nLasso Least Angle Regression\n1.0322\n1.6618\n1.2442\n0.3811\n0.3369\n1.4883\n0.2000\n\n\nomp\nOrthogonal Matching Pursuit\n1.1202\n1.9756\n1.3566\n0.2610\n0.3602\n1.6552\n0.2000\n\n\nknn\nK Neighbors Regressor\n1.2695\n2.6246\n1.5251\n-0.0885\n0.4218\n1.8519\n0.2010\n\n\ndummy\nDummy Regressor\n1.5225\n3.4511\n1.8060\n-0.0962\n0.4773\n2.7146\n0.1990\n\n\npar\nPassive Aggressive Regressor\n2.0607\n7.4149\n2.4425\n-1.6408\n0.5095\n1.7921\n0.2000"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#create-model",
    "href": "projects/ESOL/01_ESOL.html#create-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \net = create_model('et')\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n0.7295\n0.8373\n0.9150\n0.8146\n0.3288\n0.6621\n\n\n1\n0.8157\n1.2440\n1.1153\n0.7836\n0.3303\n4.9049\n\n\n2\n0.5375\n0.5803\n0.7618\n0.6586\n0.2491\n0.1979\n\n\n3\n0.8610\n1.2614\n1.1231\n0.3416\n0.3522\n0.4101\n\n\n4\n0.6183\n0.5499\n0.7415\n0.4500\n0.1991\n0.2389\n\n\n5\n0.4943\n0.3524\n0.5936\n0.8720\n0.2104\n0.5975\n\n\n6\n0.5046\n0.4246\n0.6516\n0.9291\n0.1707\n0.3410\n\n\n7\n0.4515\n0.4548\n0.6744\n0.8305\n0.2036\n0.1969\n\n\n8\n0.4390\n0.2893\n0.5378\n0.9146\n0.1561\n0.2352\n\n\n9\n0.4852\n0.4998\n0.7070\n0.7950\n0.1512\n0.1574\n\n\nMean\n0.5937\n0.6494\n0.7821\n0.7390\n0.2352\n0.7942\n\n\nStd\n0.1472\n0.3327\n0.1940\n0.1876\n0.0722\n1.3801\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(et)\n\nExtraTreesRegressor(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#tune-model",
    "href": "projects/ESOL/01_ESOL.html#tune-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of et\ntuned_et = tune_model(et, n_iter = 50, optimize = \"mae\")\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n0.6717\n0.8797\n0.9379\n0.8052\n0.2155\n0.5385\n\n\n1\n0.8837\n1.4049\n1.1853\n0.7556\n0.3791\n7.2279\n\n\n2\n0.5826\n0.5497\n0.7414\n0.6766\n0.2238\n0.1969\n\n\n3\n0.8595\n1.2030\n1.0968\n0.3721\n0.3471\n0.4514\n\n\n4\n0.9602\n1.2808\n1.1317\n-0.2811\n0.3406\n0.3575\n\n\n5\n0.6770\n0.6198\n0.7873\n0.7748\n0.2719\n0.8553\n\n\n6\n0.7719\n0.8677\n0.9315\n0.8552\n0.2816\n0.6681\n\n\n7\n0.5250\n0.4639\n0.6811\n0.8272\n0.2239\n0.3013\n\n\n8\n0.7181\n0.7877\n0.8876\n0.7675\n0.2306\n0.3511\n\n\n9\n0.6738\n0.7599\n0.8717\n0.6883\n0.1804\n0.2029\n\n\nMean\n0.7323\n0.8817\n0.9252\n0.6241\n0.2694\n1.1151\n\n\nStd\n0.1298\n0.3022\n0.1602\n0.3283\n0.0631\n2.0470\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 50 candidates, totalling 500 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# print tuned model\nprint(tuned_et)\n\nExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_et, tuner = tune_model(et, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\nFold\n \n \n \n \n \n \n\n\n\n\n0\n0.9344\n1.4378\n1.1991\n0.6816\n0.2785\n0.7455\n\n\n1\n1.0625\n2.0448\n1.4300\n0.6442\n0.4682\n9.8673\n\n\n2\n0.5371\n0.4696\n0.6853\n0.7237\n0.1925\n0.1777\n\n\n3\n0.8078\n1.0310\n1.0154\n0.4619\n0.3155\n0.4663\n\n\n4\n0.7954\n0.8386\n0.9157\n0.1612\n0.2565\n0.3240\n\n\n5\n0.7359\n0.8366\n0.9147\n0.6960\n0.2983\n0.9740\n\n\n6\n1.0848\n1.9093\n1.3818\n0.6814\n0.3522\n0.8395\n\n\n7\n0.6123\n0.6466\n0.8041\n0.7591\n0.2308\n0.3274\n\n\n8\n0.8416\n1.0428\n1.0212\n0.6922\n0.2639\n0.4224\n\n\n9\n0.7864\n0.9123\n0.9551\n0.6258\n0.2136\n0.2598\n\n\nMean\n0.8198\n1.1169\n1.0322\n0.6127\n0.2870\n1.4404\n\n\nStd\n0.1657\n0.4943\n0.2268\n0.1686\n0.0755\n2.8202\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_et\n\nExtraTreesRegressor(n_jobs=-1, random_state=123)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.ExtraTreesRegressorExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n\n\ntuner\n\nRandomizedSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(include=['MolLogP',\n                                                                          'MolWt',\n                                                                          'NumRotatableBonds',\n                                                                          'AromaticProportion'],\n                                                                 transformer=SimpleImputer())),\n                                             ('categorical_imputer',\n                                              TransformerWrappe...\n                                                                                    0.0002,\n                                                                                    0.002,\n                                                                                    0.02,\n                                                                                    0.0005,\n                                                                                    0.005,\n                                                                                    0.05,\n                                                                                    0.1,\n                                                                                    0.2,\n                                                                                    0.3,\n                                                                                    0.4,\n                                                                                    0.5],\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   random_state=123, refit=False, scoring='r2', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=KFold(n_splits=10, random_state=None, shuffle=False),\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('numerical_imputer',\n                                              TransformerWrapper(include=['MolLogP',\n                                                                          'MolWt',\n                                                                          'NumRotatableBonds',\n                                                                          'AromaticProportion'],\n                                                                 transformer=SimpleImputer())),\n                                             ('categorical_imputer',\n                                              TransformerWrappe...\n                                                                                    0.0002,\n                                                                                    0.002,\n                                                                                    0.02,\n                                                                                    0.0005,\n                                                                                    0.005,\n                                                                                    0.05,\n                                                                                    0.1,\n                                                                                    0.2,\n                                                                                    0.3,\n                                                                                    0.4,\n                                                                                    0.5],\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10],\n                                        'actual_estimator__n_estimators': [10,\n                                                                           20,\n                                                                           30,\n                                                                           40,\n                                                                           50,\n                                                                           60,\n                                                                           70,\n                                                                           80,\n                                                                           90,\n                                                                           100,\n                                                                           110,\n                                                                           120,\n                                                                           130,\n                                                                           140,\n                                                                           150,\n                                                                           160,\n                                                                           170,\n                                                                           180,\n                                                                           190,\n                                                                           200,\n                                                                           210,\n                                                                           220,\n                                                                           230,\n                                                                           240,\n                                                                           250,\n                                                                           260,\n                                                                           270,\n                                                                           280,\n                                                                           290,\n                                                                           300]},\n                   random_state=123, refit=False, scoring='r2', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['MolLogP', 'MolWt',\n                                             'NumRotatableBonds',\n                                             'AromaticProportion'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=[],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('actual_estimator',\n                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['MolLogP', 'MolWt', 'NumRotatableBonds',\n                            'AromaticProportion'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=[],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ExtraTreesRegressorExtraTreesRegressor(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#analyze-model",
    "href": "projects/ESOL/01_ESOL.html#analyze-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# residuals plot \nplot_model(et, plot = 'residuals')\n\n\n\n\n\n\n\n\n\n\n\n\n# predicting error plot \nplot_model(et, plot = 'error')\n\n\n\n\n\n\n\n\n\n\n\n\n# cooks distance plot \nplot_model(et, plot = 'cooks')\n\n\n\n\n\n\n\n\n\n\n\n\n# recursive feature selection\nplot_model(et, plot = 'rfe')\n\n\n\n\n\n\n\n\n\n\n\n\n# learning curve \nplot_model(et, plot = 'learning')\n\n\n\n\n\n\n\n\n\n\n\n\n# validation curve \nplot_model(et, plot = 'vc')\n\n\n\n\n\n\n\n\n\n\n\n\n# manifold learning plot \nplot_model(et, plot = 'manifold')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(best, plot = 'feature')"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#evaluate-model",
    "href": "projects/ESOL/01_ESOL.html#evaluate-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(et)"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#finalize-model",
    "href": "projects/ESOL/01_ESOL.html#finalize-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(et)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['MolLogP', 'MolWt',\n                                             'NumRotatableBonds',\n                                             'AromaticProportion'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=[],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('actual_estimator',\n                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['MolLogP', 'MolWt',\n                                             'NumRotatableBonds',\n                                             'AromaticProportion'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=[],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('actual_estimator',\n                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['MolLogP', 'MolWt', 'NumRotatableBonds',\n                            'AromaticProportion'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=[],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ExtraTreesRegressorExtraTreesRegressor(n_jobs=-1, random_state=123)"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#prediction",
    "href": "projects/ESOL/01_ESOL.html#prediction",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(et)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nExtra Trees Regressor\n0.7273\n0.8718\n0.9337\n0.8051\n0.2559\n0.7491\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nlogS\nprediction_label\n\n\n\n\n681\n5.26898\n349.429993\n5.0\n0.461538\n-6.025\n-5.71126\n\n\n373\n1.84570\n360.450012\n3.0\n0.000000\n-3.850\n-2.06777\n\n\n541\n2.09160\n128.214996\n0.0\n0.000000\n-1.290\n-1.72016\n\n\n381\n1.84560\n227.337006\n5.0\n0.400000\n-3.040\n-2.31554\n\n\n141\n1.02480\n88.150002\n0.0\n0.000000\n-0.400\n-0.48502\n\n\n\n\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('logS', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\n\n\n\n\n0\n2.5954\n167.850\n0.0\n0.0\n\n\n1\n2.3765\n133.405\n0.0\n0.0\n\n\n2\n2.5938\n167.850\n1.0\n0.0\n\n\n3\n2.0289\n133.405\n1.0\n0.0\n\n\n4\n2.9189\n187.375\n1.0\n0.0\n\n\n\n\n\n\n\n\n# predict model on new_data\npredictions = predict_model(et, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nprediction_label\n\n\n\n\n0\n2.5954\n167.850006\n0.0\n0.0\n-1.7092\n\n\n1\n2.3765\n133.404999\n0.0\n0.0\n-1.9794\n\n\n2\n2.5938\n167.850006\n1.0\n0.0\n-1.7400\n\n\n3\n2.0289\n133.404999\n1.0\n0.0\n-1.4800\n\n\n4\n2.9189\n187.375000\n1.0\n0.0\n-3.7649"
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#interpret-model",
    "href": "projects/ESOL/01_ESOL.html#interpret-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Interpret Model",
    "text": "Interpret Model\n\n# interpret model \ninterpret_model(et)\n\n\n\n\n\n\n\n\n\n# correlation plot\ninterpret_model(et, plot = \"correlation\")\n\n\n\n\n\n\n\n\n\n# reason plot\ninterpret_model(et, plot = \"reason\", observation = 10)\n\n\n\n\n\n\n\n  Visualization omitted, Javascript library not loaded!\n  Have you run `initjs()` in this notebook? If this notebook was from another\n  user you must also trust this notebook (File -&gt; Trust notebook). If you are viewing\n  this notebook on github the Javascript has been stripped for security. If you are using\n  JupyterLab this error is because a JupyterLab extension has not yet been written."
  },
  {
    "objectID": "projects/ESOL/01_ESOL.html#save-model",
    "href": "projects/ESOL/01_ESOL.html#save-model",
    "title": "Predicting Disease Spread of Dengue using LSTM",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(et, '../models/esol')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('numerical_imputer',\n                  TransformerWrapper(include=['MolLogP', 'MolWt',\n                                              'NumRotatableBonds',\n                                              'AromaticProportion'],\n                                     transformer=SimpleImputer())),\n                 ('categorical_imputer',\n                  TransformerWrapper(include=[],\n                                     transformer=SimpleImputer(strategy='most_frequent'))),\n                 ('trained_model',\n                  ExtraTreesRegressor(n_jobs=-1, random_state=123))]),\n '../models/esol.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/esol')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['MolLogP', 'MolWt',\n                                             'NumRotatableBonds',\n                                             'AromaticProportion'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=[],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('trained_model',\n                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('numerical_imputer',\n                 TransformerWrapper(include=['MolLogP', 'MolWt',\n                                             'NumRotatableBonds',\n                                             'AromaticProportion'],\n                                    transformer=SimpleImputer())),\n                ('categorical_imputer',\n                 TransformerWrapper(include=[],\n                                    transformer=SimpleImputer(strategy='most_frequent'))),\n                ('trained_model',\n                 ExtraTreesRegressor(n_jobs=-1, random_state=123))])numerical_imputer: TransformerWrapperTransformerWrapper(include=['MolLogP', 'MolWt', 'NumRotatableBonds',\n                            'AromaticProportion'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=[],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ExtraTreesRegressorExtraTreesRegressor(n_jobs=-1, random_state=123)\n\n\n\n# prediction\nprediction_holdout = predict_model(et)\n\n\n\n\n\n\n \nModel\nMAE\nMSE\nRMSE\nR2\nRMSLE\nMAPE\n\n\n\n\n0\nExtra Trees Regressor\n0.7273\n0.8718\n0.9337\n0.8051\n0.2559\n0.7491\n\n\n\n\n\n\nprediction_holdout.head()\n\n\n\n\n\n\n\n\nMolLogP\nMolWt\nNumRotatableBonds\nAromaticProportion\nlogS\nprediction_label\n\n\n\n\n681\n5.26898\n349.429993\n5.0\n0.461538\n-6.025\n-5.71126\n\n\n373\n1.84570\n360.450012\n3.0\n0.000000\n-3.850\n-2.06777\n\n\n541\n2.09160\n128.214996\n0.0\n0.000000\n-1.290\n-1.72016\n\n\n381\n1.84560\n227.337006\n5.0\n0.400000\n-3.040\n-2.31554\n\n\n141\n1.02480\n88.150002\n0.0\n0.000000\n-0.400\n-0.48502"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html",
    "href": "projects/Hepatitis/02_Hepatitis.html",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "",
    "text": "Hepatitis is a significant global health concern, affecting millions of individuals and posing a considerable burden on healthcare systems. Early detection and accurate prediction of hepatitis can lead to timely interventions, improved patient outcomes, and effective public health strategies. However, traditional diagnostic methods often have limitations in terms of accuracy and efficiency. There is a pressing need for a machine learning-based approach that can effectively predict the risk of hepatitis and aid in early identification of individuals who are likely to develop the disease."
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#problem",
    "href": "projects/Hepatitis/02_Hepatitis.html#problem",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "",
    "text": "Hepatitis is a significant global health concern, affecting millions of individuals and posing a considerable burden on healthcare systems. Early detection and accurate prediction of hepatitis can lead to timely interventions, improved patient outcomes, and effective public health strategies. However, traditional diagnostic methods often have limitations in terms of accuracy and efficiency. There is a pressing need for a machine learning-based approach that can effectively predict the risk of hepatitis and aid in early identification of individuals who are likely to develop the disease."
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#objective",
    "href": "projects/Hepatitis/02_Hepatitis.html#objective",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Objective",
    "text": "Objective\nDevelop a machine learning model to accurately predict the risk of hepatitis by analyzing relevant factors, aiding in early identification and intervention.\n\n# Load libraries\nimport numpy as np \nimport pandas as pd\nimport researchpy as rp \nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n\n# Load dataset\n# Colnames \ncol_names = [\"Class\",\"AGE\",\"SEX\",\"STEROID\",\"ANTIVIRALS\",\"FATIGUE\",\"MALAISE\",\"ANOREXIA\",\"LIVER BIG\",\n             \"LIVER FIRM\",\"SPLEEN PALPABLE\",\"SPIDERS\",\"ASCITES\",\"VARICES\",\"BILIRUBIN\",\"ALK PHOSPHATE\",\n             \"SGOT\",\"ALBUMIN\",\"PROTIME\",\"HISTOLOGY\"]\n\ndata = pd.read_csv('../data//hepatitis.data', names=col_names)\ndata.head() \n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.00\n85\n18\n4.0\n?\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.90\n135\n42\n3.5\n?\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.70\n96\n32\n4.0\n?\n1\n\n\n3\n2\n31\n1\n?\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.70\n46\n52\n4.0\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.00\n?\n200\n4.0\n?\n1"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#exploring-data",
    "href": "projects/Hepatitis/02_Hepatitis.html#exploring-data",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Exploring Data",
    "text": "Exploring Data\n\n# check shape of data \ndata.shape\n\n(155, 20)\n\n\n\n# dtypes \ndata.dtypes\n\nClass               int64\nAGE                 int64\nSEX                 int64\nSTEROID            object\nANTIVIRALS          int64\nFATIGUE            object\nMALAISE            object\nANOREXIA           object\nLIVER BIG          object\nLIVER FIRM         object\nSPLEEN PALPABLE    object\nSPIDERS            object\nASCITES            object\nVARICES            object\nBILIRUBIN          object\nALK PHOSPHATE      object\nSGOT               object\nALBUMIN            object\nPROTIME            object\nHISTOLOGY           int64\ndtype: object\n\n\n\n# info \ndata.info() \n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 155 entries, 0 to 154\nData columns (total 20 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Class            155 non-null    int64 \n 1   AGE              155 non-null    int64 \n 2   SEX              155 non-null    int64 \n 3   STEROID          155 non-null    object\n 4   ANTIVIRALS       155 non-null    int64 \n 5   FATIGUE          155 non-null    object\n 6   MALAISE          155 non-null    object\n 7   ANOREXIA         155 non-null    object\n 8   LIVER BIG        155 non-null    object\n 9   LIVER FIRM       155 non-null    object\n 10  SPLEEN PALPABLE  155 non-null    object\n 11  SPIDERS          155 non-null    object\n 12  ASCITES          155 non-null    object\n 13  VARICES          155 non-null    object\n 14  BILIRUBIN        155 non-null    object\n 15  ALK PHOSPHATE    155 non-null    object\n 16  SGOT             155 non-null    object\n 17  ALBUMIN          155 non-null    object\n 18  PROTIME          155 non-null    object\n 19  HISTOLOGY        155 non-null    int64 \ndtypes: int64(5), object(15)\nmemory usage: 24.3+ KB\n\n\n\n# Replace ? with 0\ndata = data.replace('?',0)\ndata.head()\n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.00\n85\n18\n4.0\n0\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.90\n135\n42\n3.5\n0\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.70\n96\n32\n4.0\n0\n1\n\n\n3\n2\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.70\n46\n52\n4.0\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.00\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\n# Convert from one dt to another\ndata.columns[data.dtypes == 'object']\ndata[['STEROID', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM','SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'BILIRUBIN',\n      'ALK PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME']] =  data[['STEROID', 'FATIGUE', 'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM','SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES', 'BILIRUBIN',\n      'ALK PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME']].astype(float).astype(int)\ndata.head()\n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1\n85\n18\n4\n0\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0\n135\n42\n3\n0\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0\n96\n32\n4\n0\n1\n\n\n3\n2\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0\n46\n52\n4\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1\n0\n200\n4\n0\n1\n\n\n\n\n\n\n\n\ndata[['BILIRUBIN','ALBUMIN']] = data[['BILIRUBIN','ALBUMIN']].astype(float)\ndata.head() \n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n\n\n3\n2\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\ndata[['Class','SEX']] = data[['Class','SEX']].astype(object)\ndata.head() \n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n\n\n3\n2\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\ndata.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 155 entries, 0 to 154\nData columns (total 20 columns):\n #   Column           Non-Null Count  Dtype  \n---  ------           --------------  -----  \n 0   Class            155 non-null    object \n 1   AGE              155 non-null    int64  \n 2   SEX              155 non-null    object \n 3   STEROID          155 non-null    int32  \n 4   ANTIVIRALS       155 non-null    int64  \n 5   FATIGUE          155 non-null    int32  \n 6   MALAISE          155 non-null    int32  \n 7   ANOREXIA         155 non-null    int32  \n 8   LIVER BIG        155 non-null    int32  \n 9   LIVER FIRM       155 non-null    int32  \n 10  SPLEEN PALPABLE  155 non-null    int32  \n 11  SPIDERS          155 non-null    int32  \n 12  ASCITES          155 non-null    int32  \n 13  VARICES          155 non-null    int32  \n 14  BILIRUBIN        155 non-null    float64\n 15  ALK PHOSPHATE    155 non-null    int32  \n 16  SGOT             155 non-null    int32  \n 17  ALBUMIN          155 non-null    float64\n 18  PROTIME          155 non-null    int32  \n 19  HISTOLOGY        155 non-null    int64  \ndtypes: float64(2), int32(13), int64(3), object(2)\nmemory usage: 16.5+ KB\n\n\n\n# check missing data \ndata.isnull().sum() \n\nClass              0\nAGE                0\nSEX                0\nSTEROID            0\nANTIVIRALS         0\nFATIGUE            0\nMALAISE            0\nANOREXIA           0\nLIVER BIG          0\nLIVER FIRM         0\nSPLEEN PALPABLE    0\nSPIDERS            0\nASCITES            0\nVARICES            0\nBILIRUBIN          0\nALK PHOSPHATE      0\nSGOT               0\nALBUMIN            0\nPROTIME            0\nHISTOLOGY          0\ndtype: int64"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#descriptive-statistics",
    "href": "projects/Hepatitis/02_Hepatitis.html#descriptive-statistics",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Descriptive statistics",
    "text": "Descriptive statistics\n\n# select numeric data \nnum_cols = data.select_dtypes(exclude = 'object')\nnum_cols.head() \n\n\n\n\n\n\n\n\nAGE\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n30\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n\n\n1\n50\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n\n\n2\n78\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n\n\n3\n31\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n\n\n4\n34\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\n# summary statistics of numerical variables \nrp.summary_cont(num_cols[['AGE', 'STEROID', 'ANTIVIRALS', 'FATIGUE', 'MALAISE', 'ANOREXIA',\n       'LIVER BIG', 'LIVER FIRM', 'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES',\n       'VARICES', 'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMIN', 'PROTIME',\n       'HISTOLOGY']])\n\n\n\n\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:60: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nN\nMean\nSD\nSE\n95% Conf.\nInterval\n\n\n\n\n0\nAGE\n155.0\n41.2000\n12.5659\n1.0093\n39.2061\n43.1939\n\n\n1\nSTEROID\n155.0\n1.4968\n0.5144\n0.0413\n1.4152\n1.5784\n\n\n2\nANTIVIRALS\n155.0\n1.8452\n0.3629\n0.0292\n1.7876\n1.9027\n\n\n3\nFATIGUE\n155.0\n1.3419\n0.4894\n0.0393\n1.2643\n1.4196\n\n\n4\nMALAISE\n155.0\n1.5935\n0.5058\n0.0406\n1.5133\n1.6738\n\n\n5\nANOREXIA\n155.0\n1.7806\n0.4305\n0.0346\n1.7123\n1.8490\n\n\n6\nLIVER BIG\n155.0\n1.7097\n0.5807\n0.0466\n1.6175\n1.8018\n\n\n7\nLIVER FIRM\n155.0\n1.4710\n0.6274\n0.0504\n1.3714\n1.5705\n\n\n8\nSPLEEN PALPABLE\n155.0\n1.7419\n0.5076\n0.0408\n1.6614\n1.8225\n\n\n9\nSPIDERS\n155.0\n1.6065\n0.5524\n0.0444\n1.5188\n1.6941\n\n\n10\nASCITES\n155.0\n1.8065\n0.4712\n0.0378\n1.7317\n1.8812\n\n\n11\nVARICES\n155.0\n1.8194\n0.4625\n0.0371\n1.7460\n1.8927\n\n\n12\nBILIRUBIN\n155.0\n0.9355\n1.2674\n0.1018\n0.7344\n1.1366\n\n\n13\nALK PHOSPHATE\n155.0\n85.6194\n62.0617\n4.9849\n75.7717\n95.4670\n\n\n14\nSGOT\n155.0\n83.6774\n89.5277\n7.1910\n69.4716\n97.8832\n\n\n15\nALBUMIN\n155.0\n3.0839\n1.2739\n0.1023\n2.8817\n3.2860\n\n\n16\nPROTIME\n155.0\n35.1161\n35.2219\n2.8291\n29.5273\n40.7050\n\n\n17\nHISTOLOGY\n155.0\n1.4516\n0.4993\n0.0401\n1.3724\n1.5308\n\n\n\n\n\n\n\n\n# select categorical data \ncat_cols = data.select_dtypes(include = 'object')\ncat_cols.head() \n\n\n\n\n\n\n\n\nClass\nSEX\n\n\n\n\n0\n2\n2\n\n\n1\n2\n1\n\n\n2\n2\n1\n\n\n3\n2\n1\n\n\n4\n2\n1\n\n\n\n\n\n\n\n\n# summary statistics of categorical variables \nrp.summary_cat(cat_cols[['Class', 'SEX']])\n\nC:\\Users\\JHossain\\anaconda3\\lib\\site-packages\\researchpy\\summary.py:225: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n  for ix, df_col in group1.iteritems():\n\n\n\n\n\n\n\n\n\nVariable\nOutcome\nCount\nPercent\n\n\n\n\n0\nClass\n2\n123\n79.35\n\n\n1\n\n1\n32\n20.65\n\n\n2\nSEX\n1\n139\n89.68\n\n\n3\n\n2\n16\n10.32"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#correlations-between-variables",
    "href": "projects/Hepatitis/02_Hepatitis.html#correlations-between-variables",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Correlations between Variables",
    "text": "Correlations between Variables\n\n# correlation: Pearson’s by default \ndata.corr(method='pearson')\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_20536\\427603040.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  data.corr(method='pearson')\n\n\n\n\n\n\n\n\n\nAGE\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\nAGE\n1.000000\n-0.067710\n-0.013100\n-0.264635\n-0.099516\n0.100589\n-0.126358\n-0.100979\n-0.127258\n-0.206551\n-0.125020\n-0.114412\n0.123546\n0.045328\n0.078021\n-0.215645\n-0.059956\n0.170780\n\n\nSTEROID\n-0.067710\n1.000000\n0.101657\n0.146292\n0.232041\n0.084752\n0.159877\n0.095280\n0.071401\n0.029781\n-0.002593\n-0.029759\n-0.000321\n-0.042245\n-0.021032\n0.203561\n0.032994\n-0.070144\n\n\nANTIVIRALS\n-0.013100\n0.101657\n1.000000\n-0.065578\n-0.026703\n-0.052554\n-0.060626\n-0.019871\n-0.183070\n-0.176367\n-0.176382\n-0.167721\n0.161671\n0.041764\n0.095980\n-0.154318\n-0.276452\n0.209242\n\n\nFATIGUE\n-0.264635\n0.146292\n-0.065578\n1.000000\n0.617660\n0.419991\n0.168796\n0.254621\n0.226850\n0.380935\n0.288877\n0.217311\n-0.246897\n-0.110505\n-0.175476\n0.234946\n0.073783\n-0.131177\n\n\nMALAISE\n-0.099516\n0.232041\n-0.026703\n0.617660\n1.000000\n0.631662\n0.192556\n0.197903\n0.145236\n0.376666\n0.376182\n0.267029\n-0.253914\n-0.114810\n-0.177297\n0.274981\n0.067186\n-0.116964\n\n\nANOREXIA\n0.100589\n0.084752\n-0.052554\n0.419991\n0.631662\n1.000000\n0.185164\n0.144556\n0.185004\n0.371874\n0.301514\n0.288883\n-0.240332\n0.035984\n-0.188183\n0.152168\n0.108322\n-0.049702\n\n\nLIVER BIG\n-0.126358\n0.159877\n-0.060626\n0.168796\n0.192556\n0.185164\n1.000000\n0.698517\n0.338963\n0.370229\n0.339108\n0.238645\n-0.043260\n0.018895\n-0.026293\n0.103349\n0.174677\n-0.104757\n\n\nLIVER FIRM\n-0.100979\n0.095280\n-0.019871\n0.254621\n0.197903\n0.144556\n0.698517\n1.000000\n0.322955\n0.407114\n0.310338\n0.295100\n-0.075869\n-0.179977\n-0.149876\n0.096499\n0.083312\n-0.206632\n\n\nSPLEEN PALPABLE\n-0.127258\n0.071401\n-0.183070\n0.226850\n0.145236\n0.185004\n0.338963\n0.322955\n1.000000\n0.515462\n0.495691\n0.574616\n-0.177461\n-0.160211\n-0.116444\n0.204411\n0.124451\n-0.100840\n\n\nSPIDERS\n-0.206551\n0.029781\n-0.176367\n0.380935\n0.376666\n0.371874\n0.370229\n0.407114\n0.515462\n1.000000\n0.553650\n0.609491\n-0.240555\n-0.136225\n-0.180494\n0.185624\n0.204276\n-0.246077\n\n\nASCITES\n-0.125020\n-0.002593\n-0.176382\n0.288877\n0.376182\n0.301514\n0.339108\n0.310338\n0.495691\n0.553650\n1.000000\n0.672806\n-0.238515\n-0.058935\n-0.125707\n0.200303\n0.120694\n-0.178075\n\n\nVARICES\n-0.114412\n-0.029759\n-0.167721\n0.217311\n0.267029\n0.288883\n0.238645\n0.295100\n0.574616\n0.609491\n0.672806\n1.000000\n-0.285886\n-0.148553\n-0.160748\n0.169160\n0.124468\n-0.178705\n\n\nBILIRUBIN\n0.123546\n-0.000321\n0.161671\n-0.246897\n-0.253914\n-0.240332\n-0.043260\n-0.075869\n-0.177461\n-0.240555\n-0.238515\n-0.285886\n1.000000\n0.036589\n0.227189\n-0.153487\n-0.136134\n0.261856\n\n\nALK PHOSPHATE\n0.045328\n-0.042245\n0.041764\n-0.110505\n-0.114810\n0.035984\n0.018895\n-0.179977\n-0.160211\n-0.136225\n-0.058935\n-0.148553\n0.036589\n1.000000\n0.141202\n0.024143\n0.135946\n0.126923\n\n\nSGOT\n0.078021\n-0.021032\n0.095980\n-0.175476\n-0.177297\n-0.188183\n-0.026293\n-0.149876\n-0.116444\n-0.180494\n-0.125707\n-0.160748\n0.227189\n0.141202\n1.000000\n-0.112325\n-0.038869\n0.138095\n\n\nALBUMIN\n-0.215645\n0.203561\n-0.154318\n0.234946\n0.274981\n0.152168\n0.103349\n0.096499\n0.204411\n0.185624\n0.200303\n0.169160\n-0.153487\n0.024143\n-0.112325\n1.000000\n0.337852\n-0.151829\n\n\nPROTIME\n-0.059956\n0.032994\n-0.276452\n0.073783\n0.067186\n0.108322\n0.174677\n0.083312\n0.124451\n0.204276\n0.120694\n0.124468\n-0.136134\n0.135946\n-0.038869\n0.337852\n1.000000\n-0.154030\n\n\nHISTOLOGY\n0.170780\n-0.070144\n0.209242\n-0.131177\n-0.116964\n-0.049702\n-0.104757\n-0.206632\n-0.100840\n-0.246077\n-0.178075\n-0.178705\n0.261856\n0.126923\n0.138095\n-0.151829\n-0.154030\n1.000000"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#skewness",
    "href": "projects/Hepatitis/02_Hepatitis.html#skewness",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Skewness",
    "text": "Skewness\n\n# skew \ndata.skew() \n\nClass             -1.464700\nAGE                0.365294\nSEX                2.633738\nSTEROID           -0.131978\nANTIVIRALS        -1.926980\nFATIGUE            0.503794\nMALAISE           -0.536814\nANOREXIA          -1.614447\nLIVER BIG         -1.880107\nLIVER FIRM        -0.768247\nSPLEEN PALPABLE   -1.841965\nSPIDERS           -1.017367\nASCITES           -2.453535\nVARICES           -2.608785\nBILIRUBIN          2.526856\nALK PHOSPHATE      0.692180\nSGOT               3.159754\nALBUMIN           -1.266422\nPROTIME            0.369567\nHISTOLOGY          0.196367\ndtype: float64"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#data-visualizations",
    "href": "projects/Hepatitis/02_Hepatitis.html#data-visualizations",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Data visualizations",
    "text": "Data visualizations\n\n# Univariate distributions with histogram\ndata.select_dtypes(exclude = \"object\").hist(figsize=(20,15), edgecolor='black')\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with density plot \ndata.select_dtypes(exclude = \"object\").plot(kind='density', subplots=True, sharex=False, figsize=(20,15), layout=(4,5))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Univariate distributions with box plots \ndata.select_dtypes(exclude = \"object\").plot(kind='box', subplots=True, sharex=False, figsize=(20,15), layout=(4,5))\nplt.show() \n\n\n\n\n\n\n\n\n\n# Multivariate plots with correlations \nplt.figure(figsize=(20,15))\ncorr = data.corr() \nsns.heatmap(corr, annot=True)\nplt.show()\n\nC:\\Users\\JHossain\\AppData\\Local\\Temp\\ipykernel_20536\\667096660.py:3: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  corr = data.corr()"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#setup",
    "href": "projects/Hepatitis/02_Hepatitis.html#setup",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Setup",
    "text": "Setup\n\n# exmine first few rows of data \ndata.head() \n\n\n\n\n\n\n\n\nClass\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n2\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n\n\n1\n2\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n\n\n2\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n\n\n3\n2\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n\n\n4\n2\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\n# import pycaret classification and init setup\nfrom pycaret.classification import *\nsetup(data, target = 'Class', session_id = 123)\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n123\n\n\n1\nTarget\nClass\n\n\n2\nTarget type\nBinary\n\n\n3\nTarget mapping\n1: 0, 2: 1\n\n\n4\nOriginal data shape\n(155, 20)\n\n\n5\nTransformed data shape\n(155, 20)\n\n\n6\nTransformed train set shape\n(108, 20)\n\n\n7\nTransformed test set shape\n(47, 20)\n\n\n8\nOrdinal features\n1\n\n\n9\nNumeric features\n18\n\n\n10\nCategorical features\n1\n\n\n11\nPreprocess\nTrue\n\n\n12\nImputation type\nsimple\n\n\n13\nNumeric imputation\nmean\n\n\n14\nCategorical imputation\nmode\n\n\n15\nMaximum one-hot encoding\n25\n\n\n16\nEncoding method\nNone\n\n\n17\nFold Generator\nStratifiedKFold\n\n\n18\nFold Number\n10\n\n\n19\nCPU Jobs\n-1\n\n\n20\nUse GPU\nFalse\n\n\n21\nLog Experiment\nFalse\n\n\n22\nExperiment Name\nclf-default-name\n\n\n23\nUSI\n1871\n\n\n\n\n\n&lt;pycaret.classification.oop.ClassificationExperiment at 0x1bbae0b95a0&gt;"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#compare-models",
    "href": "projects/Hepatitis/02_Hepatitis.html#compare-models",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Compare Models",
    "text": "Compare Models\n\n# compare baseline models\nbest = compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\ndt\nDecision Tree Classifier\n0.8527\n0.7625\n0.9083\n0.9182\n0.9084\n0.4756\n0.4904\n0.3550\n\n\nrf\nRandom Forest Classifier\n0.8445\n0.8486\n0.9542\n0.8663\n0.9074\n0.3947\n0.4109\n0.4180\n\n\ngbc\nGradient Boosting Classifier\n0.8445\n0.8556\n0.9444\n0.8778\n0.9075\n0.4177\n0.4411\n0.3870\n\n\net\nExtra Trees Classifier\n0.8355\n0.8514\n0.9306\n0.8760\n0.9001\n0.3912\n0.4058\n0.4180\n\n\nlr\nLogistic Regression\n0.8264\n0.8500\n0.9083\n0.8819\n0.8931\n0.3996\n0.4108\n0.8670\n\n\nridge\nRidge Classifier\n0.8264\n0.0000\n0.9208\n0.8724\n0.8937\n0.3839\n0.3983\n0.3510\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.8264\n0.8319\n0.9306\n0.8679\n0.8949\n0.3766\n0.3902\n0.4770\n\n\nqda\nQuadratic Discriminant Analysis\n0.8255\n0.5847\n0.9778\n0.8371\n0.8998\n0.2293\n0.2427\n0.3560\n\n\nada\nAda Boost Classifier\n0.8164\n0.8090\n0.9097\n0.8702\n0.8873\n0.3717\n0.3846\n0.3690\n\n\nlda\nLinear Discriminant Analysis\n0.8164\n0.8431\n0.8972\n0.8799\n0.8858\n0.3743\n0.3852\n0.3560\n\n\ndummy\nDummy Classifier\n0.7964\n0.5000\n1.0000\n0.7964\n0.8862\n0.0000\n0.0000\n0.3560\n\n\nknn\nK Neighbors Classifier\n0.7527\n0.6944\n0.9083\n0.8067\n0.8523\n0.0649\n0.0666\n0.3790\n\n\nsvm\nSVM - Linear Kernel\n0.7409\n0.0000\n0.9097\n0.7965\n0.8393\n0.0213\n0.0239\n0.3550\n\n\nnb\nNaive Bayes\n0.7345\n0.8458\n0.7125\n0.9381\n0.8076\n0.4001\n0.4358\n0.3510"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#create-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#create-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Create Model",
    "text": "Create Model\n\n# create model \ndt = create_model('dt')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.7273\n0.4444\n0.8889\n0.8000\n0.8421\n-0.1379\n-0.1491\n\n\n1\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\n2\n0.8182\n0.8889\n0.7778\n1.0000\n0.8750\n0.5600\n0.6236\n\n\n3\n0.8182\n0.5000\n1.0000\n0.8182\n0.9000\n0.0000\n0.0000\n\n\n4\n0.8182\n0.8889\n0.7778\n1.0000\n0.8750\n0.5600\n0.6236\n\n\n5\n0.7273\n0.4444\n0.8889\n0.8000\n0.8421\n-0.1379\n-0.1491\n\n\n6\n0.9091\n0.9375\n0.8750\n1.0000\n0.9333\n0.7925\n0.8101\n\n\n7\n0.9091\n0.8333\n1.0000\n0.8889\n0.9412\n0.7442\n0.7698\n\n\n8\n0.8000\n0.6875\n0.8750\n0.8750\n0.8750\n0.3750\n0.3750\n\n\n9\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n1.0000\n\n\nMean\n0.8527\n0.7625\n0.9083\n0.9182\n0.9084\n0.4756\n0.4904\n\n\nStd\n0.0936\n0.2138\n0.0841\n0.0861\n0.0554\n0.4154\n0.4251\n\n\n\n\n\n\n\n\n\n# print model parameters\nprint(dt)\n\nDecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       random_state=123, splitter='best')"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#tune-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#tune-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Tune Model",
    "text": "Tune Model\n\n# tune hyperparameters of rf\ntuned_dt = tune_model(dt)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9091\n0.6111\n1.0000\n0.9000\n0.9474\n0.6207\n0.6708\n\n\n1\n0.9091\n0.9167\n0.8889\n1.0000\n0.9412\n0.7442\n0.7698\n\n\n2\n0.9091\n0.6944\n1.0000\n0.9000\n0.9474\n0.6207\n0.6708\n\n\n3\n0.8182\n0.5833\n1.0000\n0.8182\n0.9000\n0.0000\n0.0000\n\n\n4\n0.7273\n0.8611\n0.7778\n0.8750\n0.8235\n0.2326\n0.2406\n\n\n5\n0.7273\n0.7778\n0.8889\n0.8000\n0.8421\n-0.1379\n-0.1491\n\n\n6\n0.8182\n0.8750\n0.8750\n0.8750\n0.8750\n0.5417\n0.5417\n\n\n7\n0.9091\n0.7917\n1.0000\n0.8889\n0.9412\n0.7442\n0.7698\n\n\n8\n0.8000\n0.8750\n0.7500\n1.0000\n0.8571\n0.5455\n0.6124\n\n\n9\n0.8000\n0.9375\n0.7500\n1.0000\n0.8571\n0.5455\n0.6124\n\n\nMean\n0.8327\n0.7924\n0.8931\n0.9057\n0.8932\n0.4457\n0.4739\n\n\nStd\n0.0694\n0.1188\n0.1002\n0.0691\n0.0457\n0.2921\n0.3103\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\n# to access the tuner object you can set return_tuner = True\ntuned_dt, tuner = tune_model(dt, return_tuner=True)\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9091\n0.6111\n1.0000\n0.9000\n0.9474\n0.6207\n0.6708\n\n\n1\n0.9091\n0.9167\n0.8889\n1.0000\n0.9412\n0.7442\n0.7698\n\n\n2\n0.9091\n0.6944\n1.0000\n0.9000\n0.9474\n0.6207\n0.6708\n\n\n3\n0.8182\n0.5833\n1.0000\n0.8182\n0.9000\n0.0000\n0.0000\n\n\n4\n0.7273\n0.8611\n0.7778\n0.8750\n0.8235\n0.2326\n0.2406\n\n\n5\n0.7273\n0.7778\n0.8889\n0.8000\n0.8421\n-0.1379\n-0.1491\n\n\n6\n0.8182\n0.8750\n0.8750\n0.8750\n0.8750\n0.5417\n0.5417\n\n\n7\n0.9091\n0.7917\n1.0000\n0.8889\n0.9412\n0.7442\n0.7698\n\n\n8\n0.8000\n0.8750\n0.7500\n1.0000\n0.8571\n0.5455\n0.6124\n\n\n9\n0.8000\n0.9375\n0.7500\n1.0000\n0.8571\n0.5455\n0.6124\n\n\nMean\n0.8327\n0.7924\n0.8931\n0.9057\n0.8932\n0.4457\n0.4739\n\n\nStd\n0.0694\n0.1188\n0.1002\n0.0691\n0.0457\n0.2921\n0.3103\n\n\n\n\n\n\n\n\nFitting 10 folds for each of 10 candidates, totalling 100 fits\nOriginal model was better than the tuned model, hence it will be returned. NOTE: The display metrics are for the tuned model (not the original one).\n\n\n\ntuned_dt\n\nDecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       random_state=123, splitter='best')In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.DecisionTreeClassifierDecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=None, max_features=None, max_leaf_nodes=None,\n                       min_impurity_decrease=0.0, min_samples_leaf=1,\n                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n                       random_state=123, splitter='best')\n\n\n\ntuner\n\nRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__max_features': [1.0,\n                                                                           'sqrt',\n                                                                           'log2'],\n                                        'actual_estimator__min_impurity_decrease': [0,\n                                                                                    0.0001,\n                                                                                    0.001,\n                                                                                    0.01,\n                                                                                    0.0002,\n                                                                                    0.002,\n                                                                                    0.02,\n                                                                                    0.0005,\n                                                                                    0.005,\n                                                                                    0.05,\n                                                                                    0.1,\n                                                                                    0.2,\n                                                                                    0.3,\n                                                                                    0.4,\n                                                                                    0.5],\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.RandomizedSearchCVRandomizedSearchCV(cv=StratifiedKFold(n_splits=10, random_state=None, shuffle=False),\n                   error_score=nan,\n                   estimator=Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n                                      steps=[('label_encoding',\n                                              TransformerWrapperWithInverse(exclude=None,\n                                                                            include=None,\n                                                                            transformer=LabelEncoder())),\n                                             ('numerical_imputer',\n                                              TransformerWrapper(exclude=No...\n                                        'actual_estimator__max_features': [1.0,\n                                                                           'sqrt',\n                                                                           'log2'],\n                                        'actual_estimator__min_impurity_decrease': [0,\n                                                                                    0.0001,\n                                                                                    0.001,\n                                                                                    0.01,\n                                                                                    0.0002,\n                                                                                    0.002,\n                                                                                    0.02,\n                                                                                    0.0005,\n                                                                                    0.005,\n                                                                                    0.05,\n                                                                                    0.1,\n                                                                                    0.2,\n                                                                                    0.3,\n                                                                                    0.4,\n                                                                                    0.5],\n                                        'actual_estimator__min_samples_leaf': [2,\n                                                                               3,\n                                                                               4,\n                                                                               5,\n                                                                               6],\n                                        'actual_estimator__min_samples_split': [2,\n                                                                                5,\n                                                                                7,\n                                                                                9,\n                                                                                10]},\n                   pre_dispatch='2*n_jobs', random_state=123, refit=False,\n                   return_train_score=False, scoring='accuracy', verbose=1)estimator: PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                             'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                             'LIVER BIG', 'LIVER FIRM',\n                                             'SPLEEN PALPABL...\n                                    transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                ('actual_estimator',\n                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        random_state=123, splitter='best'))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(include=['AGE', 'STEROID', 'ANTIVIRALS', 'FATIGUE',\n                            'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM',\n                            'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES',\n                            'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMIN',\n                            'PROTIME', 'HISTOLOGY'],\n                   transformer=SimpleImputer())transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(include=['SEX'],\n                   transformer=SimpleImputer(strategy='most_frequent'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(include=['SEX'],\n                   transformer=OrdinalEncoder(cols=['SEX'],\n                                              handle_missing='return_nan',\n                                              mapping=[{'col': 'SEX',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}]))transformer: OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])clean_column_names: TransformerWrapperTransformerWrapper(transformer=CleanColumnNames())transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()DecisionTreeClassifierDecisionTreeClassifier(random_state=123)"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#analyze-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#analyze-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Analyze Model",
    "text": "Analyze Model\n\n# plot confusion matrix\nplot_model(dt, plot = 'confusion_matrix')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot AUC\nplot_model(dt, plot = 'auc')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot class report\nplot_model(dt, plot = 'class_report')\n\n\n\n\n\n\n\n\n\n\n\n\n# plot feature importance\nplot_model(dt, plot = 'feature')"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#evaluate-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#evaluate-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Evaluate Model",
    "text": "Evaluate Model\n\n# evaluate model \nevaluate_model(dt)"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#finalize-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#finalize-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Finalize Model",
    "text": "Finalize Model\n\n# finalize a model\nfinalize_model(dt)\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                             'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                             'LIVER BIG', 'LIVER FIRM',\n                                             'SPLEEN PALPABL...\n                                    transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                ('actual_estimator',\n                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        random_state=123, splitter='best'))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                             'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                             'LIVER BIG', 'LIVER FIRM',\n                                             'SPLEEN PALPABL...\n                                    transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                ('actual_estimator',\n                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        random_state=123, splitter='best'))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['AGE', 'STEROID', 'ANTIVIRALS', 'FATIGUE',\n                            'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM',\n                            'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES',\n                            'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMIN',\n                            'PROTIME', 'HISTOLOGY'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['SEX'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['SEX'],\n                   transformer=OrdinalEncoder(cols=['SEX'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'SEX',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])clean_column_names: TransformerWrapperTransformerWrapper(exclude=None, include=None,\n                   transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()DecisionTreeClassifierDecisionTreeClassifier(random_state=123)"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#prediction",
    "href": "projects/Hepatitis/02_Hepatitis.html#prediction",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Prediction",
    "text": "Prediction\n\n# predict on test set\nholdout_pred = predict_model(dt)\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\n\n\n0\nDecision Tree Classifier\n0.6809\n0.4689\n0.8378\n0.7750\n0.8052\n-0.0698\n-0.0715\n\n\n\n\n\n\n# show predictions df\nholdout_pred.head()\n\n\n\n\n\n\n\n\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\n...\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\nClass\nprediction_label\nprediction_score\n\n\n\n\n122\n42\n1\n2\n2\n2\n2\n2\n2\n2\n1\n...\n2\n1.0\n85\n40\n0.0\n0\n2\n1\n2\n1.0\n\n\n44\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n...\n2\n0.0\n0\n86\n0.0\n0\n1\n1\n2\n1.0\n\n\n112\n52\n1\n1\n2\n1\n2\n2\n2\n2\n2\n...\n2\n1.0\n85\n30\n4.0\n0\n2\n1\n2\n1.0\n\n\n75\n32\n1\n1\n1\n1\n1\n2\n2\n2\n2\n...\n2\n1.0\n55\n45\n4.0\n56\n1\n1\n2\n1.0\n\n\n150\n46\n1\n2\n2\n1\n1\n1\n2\n2\n2\n...\n1\n7.0\n0\n242\n3.0\n50\n2\n0\n2\n1.0\n\n\n\n\n5 rows × 22 columns\n\n\n\n\n# copy data and drop Class variable\nnew_data = data.copy()\nnew_data.drop('Class', axis=1, inplace=True)\nnew_data.head()\n\n\n\n\n\n\n\n\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\nSPIDERS\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\n\n\n\n\n0\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n2\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n\n\n1\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n2\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n\n\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n\n\n3\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n\n\n4\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n\n\n\n\n\n\n\n\n# predict model on new_data\npredictions = predict_model(best, data = new_data)\npredictions.head()\n\n\n\n\n\n\n\n\n\n\n\nAGE\nSEX\nSTEROID\nANTIVIRALS\nFATIGUE\nMALAISE\nANOREXIA\nLIVER BIG\nLIVER FIRM\nSPLEEN PALPABLE\n...\nASCITES\nVARICES\nBILIRUBIN\nALK PHOSPHATE\nSGOT\nALBUMIN\nPROTIME\nHISTOLOGY\nprediction_label\nprediction_score\n\n\n\n\n0\n30\n2\n1\n2\n2\n2\n2\n1\n2\n2\n...\n2\n2\n1.0\n85\n18\n4.0\n0\n1\n2\n1.0\n\n\n1\n50\n1\n1\n2\n1\n2\n2\n1\n2\n2\n...\n2\n2\n0.0\n135\n42\n3.0\n0\n1\n2\n1.0\n\n\n2\n78\n1\n2\n2\n1\n2\n2\n2\n2\n2\n...\n2\n2\n0.0\n96\n32\n4.0\n0\n1\n2\n1.0\n\n\n3\n31\n1\n0\n1\n2\n2\n2\n2\n2\n2\n...\n2\n2\n0.0\n46\n52\n4.0\n80\n1\n2\n1.0\n\n\n4\n34\n1\n2\n2\n2\n2\n2\n2\n2\n2\n...\n2\n2\n1.0\n0\n200\n4.0\n0\n1\n2\n1.0\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "projects/Hepatitis/02_Hepatitis.html#save-model",
    "href": "projects/Hepatitis/02_Hepatitis.html#save-model",
    "title": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "section": "Save Model",
    "text": "Save Model\n\n# save pipeline\nsave_model(dt, '../models/hepatitis')\n\nTransformation Pipeline and Model Successfully Saved\n\n\n(Pipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n          steps=[('label_encoding',\n                  TransformerWrapperWithInverse(exclude=None, include=None,\n                                                transformer=LabelEncoder())),\n                 ('numerical_imputer',\n                  TransformerWrapper(exclude=None,\n                                     include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                              'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                              'LIVER BIG', 'LIVER FIRM',\n                                              'SPLEEN PALPABL...\n                                     transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                 ('trained_model',\n                  DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                         criterion='gini', max_depth=None,\n                                         max_features=None, max_leaf_nodes=None,\n                                         min_impurity_decrease=0.0,\n                                         min_samples_leaf=1, min_samples_split=2,\n                                         min_weight_fraction_leaf=0.0,\n                                         random_state=123, splitter='best'))],\n          verbose=False),\n '../models/hepatitis.pkl')\n\n\n\n# load pipeline\nloaded_best_pipeline = load_model('../models/hepatitis')\nloaded_best_pipeline\n\nTransformation Pipeline and Model Successfully Loaded\n\n\nPipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                             'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                             'LIVER BIG', 'LIVER FIRM',\n                                             'SPLEEN PALPABL...\n                                    transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                ('trained_model',\n                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        random_state=123, splitter='best'))],\n         verbose=False)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(memory=FastMemory(location=C:\\Users\\JHossain\\AppData\\Local\\Temp\\joblib),\n         steps=[('label_encoding',\n                 TransformerWrapperWithInverse(exclude=None, include=None,\n                                               transformer=LabelEncoder())),\n                ('numerical_imputer',\n                 TransformerWrapper(exclude=None,\n                                    include=['AGE', 'STEROID', 'ANTIVIRALS',\n                                             'FATIGUE', 'MALAISE', 'ANOREXIA',\n                                             'LIVER BIG', 'LIVER FIRM',\n                                             'SPLEEN PALPABL...\n                                    transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))),\n                ('trained_model',\n                 DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None,\n                                        criterion='gini', max_depth=None,\n                                        max_features=None, max_leaf_nodes=None,\n                                        min_impurity_decrease=0.0,\n                                        min_samples_leaf=1, min_samples_split=2,\n                                        min_weight_fraction_leaf=0.0,\n                                        random_state=123, splitter='best'))],\n         verbose=False)label_encoding: TransformerWrapperWithInverseTransformerWrapperWithInverse(exclude=None, include=None,\n                              transformer=LabelEncoder())transformer: LabelEncoderLabelEncoder()LabelEncoderLabelEncoder()numerical_imputer: TransformerWrapperTransformerWrapper(exclude=None,\n                   include=['AGE', 'STEROID', 'ANTIVIRALS', 'FATIGUE',\n                            'MALAISE', 'ANOREXIA', 'LIVER BIG', 'LIVER FIRM',\n                            'SPLEEN PALPABLE', 'SPIDERS', 'ASCITES', 'VARICES',\n                            'BILIRUBIN', 'ALK PHOSPHATE', 'SGOT', 'ALBUMIN',\n                            'PROTIME', 'HISTOLOGY'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='mean',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer()SimpleImputerSimpleImputer()categorical_imputer: TransformerWrapperTransformerWrapper(exclude=None, include=['SEX'],\n                   transformer=SimpleImputer(add_indicator=False, copy=True,\n                                             fill_value=None,\n                                             keep_empty_features=False,\n                                             missing_values=nan,\n                                             strategy='most_frequent',\n                                             verbose='deprecated'))transformer: SimpleImputerSimpleImputer(strategy='most_frequent')SimpleImputerSimpleImputer(strategy='most_frequent')ordinal_encoding: TransformerWrapperTransformerWrapper(exclude=None, include=['SEX'],\n                   transformer=OrdinalEncoder(cols=['SEX'],\n                                              drop_invariant=False,\n                                              handle_missing='return_nan',\n                                              handle_unknown='value',\n                                              mapping=[{'col': 'SEX',\n                                                        'data_type': dtype('float64'),\n                                                        'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}],\n                                              return_df=True, verbose=0))transformer: OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])OrdinalEncoderOrdinalEncoder(cols=['SEX'], handle_missing='return_nan',\n               mapping=[{'col': 'SEX', 'data_type': dtype('float64'),\n                         'mapping': 1.0    0\n2.0    1\nNaN   -1\ndtype: int64}])clean_column_names: TransformerWrapperTransformerWrapper(exclude=None, include=None,\n                   transformer=CleanColumnNames(match='[\\\\]\\\\[\\\\,\\\\{\\\\}\\\\\"\\\\:]+'))transformer: CleanColumnNamesCleanColumnNames()CleanColumnNamesCleanColumnNames()DecisionTreeClassifierDecisionTreeClassifier(random_state=123)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Preonath Chondrow Dev",
    "section": "",
    "text": "&lt;img src=\"files/profiles/preonath_profile.jpg\" alt=\"Profile Image\" class=\"profile-img\"&gt;\n\n\n&lt;h2&gt;Preonath Chondrow Dev&lt;/h2&gt;\n&lt;h3&gt;Research Officer and Bioinformatician&lt;/h3&gt;\n&lt;p&gt;I am a passionate researcher from Bangladesh...&lt;/p&gt;\n&lt;button class=\"btn btn-primary\"&gt;Curriculum Vitae&lt;/button&gt;\n\n\n\nWelcome!ExperienceSkillsEducation\n\n\nHello, I am Preonath Chondrow Dev. I am a Bioinformatician at the Child Health Research Foundation to improve child health in Bangladesh and around the world by facilitating appropriate policy decisions through research and advocacy. I am also working as a mentor at Bioinformatics School.\nAs a bioinformatician, I combine my knowledge in biochemistry, molecular biology, and computer science to further the cause of improving human health by developing and implementing state-of-the-art computational methods with the sincere intention of illuminating the route to new discoveries and solutions by teasing apart the genomic complexities of diseases through highly engaging and fruitful collaborations.\n\n\n\n Bioinformatician | Child Health Research Foundation(CHRF), Dhaka, Bangladesh | (2022 - Present)\n\n\n \n\n Research Fellow | National Institute of Biotechnology(NIB), Dhaka, Bangladesh | (Sept 2021 - June 2022)\n\n\n \n\n Academic Team Member | Bangladesh Mathematical Olympiad(BdMO), Dhaka, Bangladesh | (2017 - 2019)\n\n\n\n\n\nTechnical Skills\n\n\nLanguages\n\n\n\nBash\n\n\nPython\n\n\nC\n\n\nR\n\n\nNextflow\n\n\nJava\n\n\n\nData Structure and Algorithms\n\n\n\nImplemented various data structures and algorithms for efficient data processing and analysis.\n\n\n\nDatabase Management Systems\n\n\n\nSQL\n\n\nRedis\n\n\nSQLite3\n\n\n\nMachine Learning and Deep Learning\n\n\n\nNumpy, Pandas, Matplotlib, Seaborn\n\n\nKeras, SciKit-Learn\n\n\nTensorFlow, PyTorch\n\n\nNeural Networks (NN)\n\n\nConvolutional Neural Networks (CNN)\n\n\nRecurrent Neural Networks (RNN)\n\n\nLong Short-Term Memory (LSTM)\n\n\nTransformers, Transfer Learning\n\n\nReinforcement Learning, Autoencoders\n\n\n\nTools\n\n\n\nGit\n\n\nJupyter Notebook\n\n\nVisual Studio\n\n\nR Studio\n\n\nPyCharm\n\n\nVisual Studio Code\n\n\nTerra\n\n\nPosit\n\n\n\nBioinformatics\n\n\n\nProficient in Docker for pipeline building\n\n\nMicroarray and Bulk RNA sequencing Analysis using DESeq2, stats, ggplot, corrplot, pheatmap, EDASeq, gProfileR\n\n\nGenome Quality Checking and Assembly:\n\n\nIllumina: BCL2fastq, fastQC, MultiQC, Quast, Trimmomatic, fastp, Unicycler, Spades, Megahit\n\n\nNanopore: fast52fasta, Pilon, Flye\n\n\n\n\nGenome Annotation: Kraken2, Prokka, PGAp, Kleborate, Seroba, AMRFinderPlus, Abricat, SRST, MLST, Snippy, Mafft, fasta2phylip, Raxml-ng, Poppunk, PlasmidFinder, ResFinder\n\n\nMolecular Dynamics Simulation using GROMACS\n\n\nMetagenomics Analysis using qiime2 and CZID\n\n\nNextstrain Built and maintained from Bangladesh for SARS-CoV-2\n\n\nSingle Cell RNA sequencing Analysis using Scanpy and Seurat - Data pre‑processing, Clustering, Cell annotation, Integration and Batch correction, Cell‑cell communication, BCR background and 10x analysis, Trajectory inference, Differential abundance, Multiomic scATAC\n\n\n\nWet Lab\n\n\n\nSingle Cell RNA sequencing Analysis using HoneyComb, 10x Genomics, Parse Biosciences, Library preparation, Sequencing on Illumina and Nanopore\n\n\nMolecular Techniques: Conventional and Real‑time PCR, DNA & RNA extraction, Agarose & Polyacrylamide Gel Electrophoresis and Imaging, Blotting techniques, Molecular cloning, ELISA & ICT\n\n\nMicrobiological Techniques: Culture, Biochemical tests\n\n\n\n\n\n Master of Science (2020 - 2022)\nInstitution: Shahjalal University of Science and Technology(SUST), Sylhet, Bangladesh\nSchool of Life Sciences\nMajor: Biochemistry and Molecular Biology\n\n\n \n\n Bachelor of Science (2016 - 2019)\nInstitution: Shahjalal University of Science and Technology(SUST), Sylhet, Bangladesh\nSchool of Life Sciences\nMajor: Biochemistry and Molecular Biology"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "&lt;!DOCTYPE html&gt;\n\n\n\nPreonath Chondrow Dev\n\n\n\n\n\n\n        &lt;img src=\"files/profiles/preonath.jpg\" alt=\"Preonath Chondrow Dev\" class=\"profile-photo\"&gt;\n        &lt;h1&gt;Preonath Chondrow Dev&lt;/h1&gt;\n        &lt;h2&gt;Research Officer and Bioinformatician&lt;/h2&gt;\n        &lt;p&gt;I work at the Child Health Research Foundation in Dhaka, Bangladesh, improving child health through bioinformatics research.&lt;/p&gt;\n    &lt;/div&gt;\n    &lt;nav class=\"navigation\"&gt;\n        &lt;ul&gt;\n            &lt;li&gt;&lt;a href=\"#bio\"&gt;Bio&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#education\"&gt;Education&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#experience\"&gt;Experience&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#volunteering\"&gt;Volunteering&lt;/a&gt;&lt;/li&gt;\n            &lt;li&gt;&lt;a href=\"#skills\"&gt;Skills&lt;/a&gt;&lt;/li&gt;\n        &lt;/ul&gt;\n    &lt;/nav&gt;\n&lt;/header&gt;\n\n&lt;!-- Additional sections for Bio, Education, Experience, etc. --&gt;\n\n&lt;footer&gt;\n    &lt;p&gt;© Copyright 2023, Preonath Chondrow Dev&lt;/p&gt;\n&lt;/footer&gt;"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "About me",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\nMaster of Science (2020 - 2022)\n\n\n\n\n\nInstitution: Shahjalal University of Science and Technology, Sylhet, Bangladesh\n\n\n\nSchool of Life Sciences\n\n\n\nMajor: Biochemistry and Molecular Biology\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBachelor of Science (2016 - 2019)\n\n\n\n\n\nInstitution: Shahjalal University of Science and Technology, Sylhet, Bangladesh\n\n\n\nSchool of Life Sciences\n\n\n\nMajor: Biochemistry and Molecular Biology"
  },
  {
    "objectID": "research.html#paediatric-pneumonia-detection-from-xray-images-using-deep-learning",
    "href": "research.html#paediatric-pneumonia-detection-from-xray-images-using-deep-learning",
    "title": "Research Projects",
    "section": "Paediatric pneumonia detection from X‑ray images using deep learning",
    "text": "Paediatric pneumonia detection from X‑ray images using deep learning\n\nAbstract\nPneumonia continues to be the most common cause of illness and death for children under five years old in low- and middle-income countries, posing a persistent threat to their health. Although paediatric pneumonia is common, the diagnosis process is largely based on the subjective interpretation of chest X-ray images, which introduces variability and necessitates expert opinion. The current paradigm, in which physicians or radiologists evaluate images on an individual basis, is non-standardized and makes it more difficult to make timely and accurate diagnoses.\nOur research attempts to address this difficulty by developing a deep learning-based computational tool that is especially intended for the diagnosis of paediatric pneumonia. Unlike existing approaches, ours is trained on an extensive dataset of X-ray images that have been carefully reviewed by a group of experts. Of note, these professionals have."
  },
  {
    "objectID": "research.html#predicting-disease-spread-of-dengue-using-lstm",
    "href": "research.html#predicting-disease-spread-of-dengue-using-lstm",
    "title": "Research Projects",
    "section": "Predicting Disease Spread of Dengue using LSTM",
    "text": "Predicting Disease Spread of Dengue using LSTM\n\n\nAbstract\nMosquito-borne dengue fever is a disease found in tropical and subtropical regions of the world. In mild cases, fever, rash, and sore muscles and joints resemble flu symptoms. serious dengue fever can result in hypotension, serious bleeding, and even death.\nDengue transmission dynamics are influenced by climate variables like temperature and precipitation since the virus is spread by mosquitoes. Despite the complexity of the relationship, an increasing number of scientists contend that changes in distribution brought about by climate change would likely have a major impact on public health globally.\nDengue fever has been increasing in recent years. Southeast Asia and the Pacific islands have historically had the highest rates of the illness."
  },
  {
    "objectID": "research.html#prediction-of-missing-dna-methylation-fromwhole-genome-bisulfite-data-using-knn",
    "href": "research.html#prediction-of-missing-dna-methylation-fromwhole-genome-bisulfite-data-using-knn",
    "title": "Research Projects",
    "section": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN",
    "text": "Prediction of Missing DNA Methylation fromWhole Genome Bisulfite Data Using KNN\n\n\nAbstract\nOne important epigenetic alteration that is essential for controlling gene expression and other biological functions is DNA methylation. One effective method for single-base resolution DNA methylation profiling is Whole Genome Bisulfite Sequencing (WGBS). However, missing methylation values in WGBS datasets are frequently the consequence of technical difficulties and experimental limitations. It is imperative to address these absent values in order to gain a thorough understanding of the epigenetic landscape. In this work, we suggest a novel method for predicting missing DNA methylation values in WGBS data, which is based on K-Nearest Neighbours (KNN). Our approach accurately imputes missing values by utilising the methylation data’s inherent structure and patterns. KNN, an instance-based, non-parametric machine learning algorithm, is used to find comparable methylation profiles for each"
  },
  {
    "objectID": "research.html#exploring-diversity-and-environmental-dynamics-of-salmonella-typhi-and-its-bacteriophages",
    "href": "research.html#exploring-diversity-and-environmental-dynamics-of-salmonella-typhi-and-its-bacteriophages",
    "title": "Research Projects",
    "section": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages",
    "text": "Exploring diversity and environmental dynamics of Salmonella Typhi and its bacteriophages\n\n\nAbstract\nBacteriophages (or phages) are viruses that infect bacteria and regulate their abundance, phenotypic characteristics, and long-term evolutionary trajectory. Recently, our group has shown that Typhi-specific phages are abundant in surface water collected from high typhoid burden settings in Bangladesh and phage abundance correlates with typhoid burden. However, no information is available regarding the infecting mechanisms of Typhi-phages and the role these phages play in the evolution and dynamics of Salmonella Typhi in the environment. In this study, we used a combination of whole genome sequencing and bacterial killing assays against a diverse panel of Salmonella Typhi isolates to characterize the Typhi-phages present in Bangladesh.\n\nKeyword: Water-borne diseases, factors, vibrio cholerae, public health, health impacts, calamities, water.\n\n\nOn review"
  },
  {
    "objectID": "research.html#a-global-pediatric-cell-atlas-of-nasal-and-oral-mucosa",
    "href": "research.html#a-global-pediatric-cell-atlas-of-nasal-and-oral-mucosa",
    "title": "Research Projects",
    "section": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "text": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa\n\n\nAbstract\nThe nasopharyngeal and oral mucosa represent the initial sites of interaction with many environmental agents and microbes. Recent single-cell studies have revealed a rich diversity of epithelial and immune cell types and states within nasopharyngeal epithelium in diseases of global significance, including allergic inflammation and viral infection. Yet, beyond an accessible window into disease biology, minimally-invasive sampling of the nose and mouth in children represents a truly unique opportunity to characterize healthy mucosal epithelial and immune function worldwide. However, a comprehensive map of epithelial and immune system development across diverse ancestries and environments is lacking. To more broadly investigate the nasal and oral mucosa and understand how the normal variation present in healthy children maintains health or may inform disease, we have assembled an interdisciplinary team: unifying experts in 7 cities in 5 countries who are deeply invested in understanding the single-cell biology of the nasopharyngeal and oral mucosa in children living within our communities. Our plan aims to generate scientific and community engagement in all phases of our research to establish the foundation in Years 1 and 2 that will enable us to carefully and considerately analyze 80 pediatric participants at each site (560 total) across the age range from 1-month to 18-years of age in the next phase of our network (potential Year 3 and beyond). Our team will pilot and analyze single-cell data jointly with scientists from all locations, and share important lessons in global science, protocols and resultant data openly with the community. Ultimately, our global single-cell based characterization of the developing nasopharyngeal mucosa will reveal principles of epithelial and immune system development that will facilitate the equitable development of novel therapies for diseases of the aerodigestive tracts."
  },
  {
    "objectID": "research.html#rsv-vaccine-impact-monte-carlo-simulation",
    "href": "research.html#rsv-vaccine-impact-monte-carlo-simulation",
    "title": "Research Projects",
    "section": "RSV Vaccine Impact Monte Carlo Simulation",
    "text": "RSV Vaccine Impact Monte Carlo Simulation\n\n\nAbstract\nThalassemia, a hereditary blood disorder characterized by abnormal hemoglobin production, poses a significant public health challenge in Bangladesh. This project outlines a comprehensive initiative aimed at the prevention and control of thalassemia within the country. The multifaceted approach integrates awareness campaigns, genetic screening, accessible healthcare services, and community engagement to mitigate the prevalence and impact of thalassemia.\nThe project’s primary objectives include raising public awareness about thalassemia, educating communities about its genetic nature, and promoting informed family planning decisions. Additionally, the initiative seeks to establish a robust infrastructure for genetic screening, enabling early detection and intervention. This involves collaboration with healthcare institutions, laboratories, and medical professionals to ensure accurate and affordable testing.\nTo ensure equitable access to quality healthcare, the project focuses on enhancing healthcare facilities’ capacity to provide specialized thalassemia care. This involves training healthcare personnel, ensuring the availability of necessary treatments, and creating a supportive environment for patients and their families. Moreover, the project emphasizes the importance of support groups and counseling services to address the psychological and emotional aspects of thalassemia."
  },
  {
    "objectID": "teaching/Bioinformatics for Biotechnology Research/index.html",
    "href": "teaching/Bioinformatics for Biotechnology Research/index.html",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "",
    "text": "Applied Machine Learning for Healthcare (AML4H) is an advanced-level course that focuses on the intersection of machine learning and healthcare. This course is designed to provide students with the knowledge and skills needed to effectively apply machine learning techniques to healthcare data, leading to improved medical diagnostics, treatment plans, and patient outcomes. Through a combination of lectures, hands-on projects, and case studies, students will gain a deep understanding of the challenges and opportunities in this rapidly evolving field."
  },
  {
    "objectID": "teaching/Bioinformatics for Biotechnology Research/index.html#overview",
    "href": "teaching/Bioinformatics for Biotechnology Research/index.html#overview",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "",
    "text": "Applied Machine Learning for Healthcare (AML4H) is an advanced-level course that focuses on the intersection of machine learning and healthcare. This course is designed to provide students with the knowledge and skills needed to effectively apply machine learning techniques to healthcare data, leading to improved medical diagnostics, treatment plans, and patient outcomes. Through a combination of lectures, hands-on projects, and case studies, students will gain a deep understanding of the challenges and opportunities in this rapidly evolving field."
  },
  {
    "objectID": "teaching/Bioinformatics for Biotechnology Research/index.html#learning-objectives",
    "href": "teaching/Bioinformatics for Biotechnology Research/index.html#learning-objectives",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUpon completion of the course, students will be able to:\n\nTopics Covered in the Training: Linux OS, NGS Data Analysis, Python\nIdentify and preprocess various types of healthcare data, including electronic health records (EHR), medical images, and genomics data.\nApply a wide range of machine learning algorithms to healthcare problems, such as classification, regression, clustering, and sequence analysis.\nEvaluate the performance of machine learning models using appropriate metrics for healthcare tasks.\nInterpret and communicate the results of machine learning models to healthcare professionals and stakeholders.\nExplore ethical considerations and potential biases when applying machine learning in healthcare.\nImplement techniques to handle imbalanced datasets and small sample sizes common in medical applications.\nDevelop predictive models for disease diagnosis, prognosis, and patient risk stratification.\nUtilize deep learning techniques for medical image analysis, such as image segmentation and object detection.\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "teaching/Training on Advanced Next Generation Sequencing/index.html",
    "href": "teaching/Training on Advanced Next Generation Sequencing/index.html",
    "title": "Training on Advanced Next Generation Sequencing",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/Training on Advanced Next Generation Sequencing/index.html#teaching-assistant",
    "href": "teaching/Training on Advanced Next Generation Sequencing/index.html#teaching-assistant",
    "title": "Training on Advanced Next Generation Sequencing",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/Training on Advanced Next Generation Sequencing/index.html#learning-objectives",
    "href": "teaching/Training on Advanced Next Generation Sequencing/index.html#learning-objectives",
    "title": "Training on Advanced Next Generation Sequencing",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nTopics Covered in the Training: Linux OS, NGS Data Analysis(Quality checking, Quality filter, Genome Assembly, Annotation Using Pathogenwatch, Building Consensus using CZID, Sample Sheet Preparation for NextSeq2000\nBacterial Genomics Antimicrobial Resistance, Topics Covered in the Training: Linux OS, Bash Scripting Genome Assembly, Genome Mapping, Genome Annotation, Genome Submission, MLST Prediction at Total 2 Batches\nAI in Public Health, This workshop support from the Bill and Melinda Gates foundation, through the ’Democratizing Public Health Modeling Using AI‑based Tools’; Topics Covered in the Training: Data cleaning and preprocessing, Dengue prediction in Puerto Rico using LSTM and ARIMA, Predicting Pneumonia in X‑rays using CNN\n\nProgram Website\n\nApply to the Program"
  },
  {
    "objectID": "talks/Nasal Swab Dissociation Livestream/index.html",
    "href": "talks/Nasal Swab Dissociation Livestream/index.html",
    "title": "Nasal Swab Dissociation Livestream",
    "section": "",
    "text": "Isolate single cells from frozen nasopharyngeal swab, Single cell capture, Whole transcriptomic amplification, Library preparation, Loading on NextSeq 2000 and Computational Analysis by Jaclyn Long from Ordovas‑Montanes Lab, Boston Childrens Hospital\nCollaboration with MIT, Boston Childrens Hospitaland Bill and Melinda Gates Foundation"
  },
  {
    "objectID": "talks/NIB-WorkShop2/index.html",
    "href": "talks/NIB-WorkShop2/index.html",
    "title": "Sanger Institute",
    "section": "",
    "text": "The Undergraduate Research Training Program(URTP) provides intermediate-level biomedical research training for undergraduates in a supportive environment with supplement educational activities. Furthermore, this course is designed to enhance the undergraduate research experience by focusing on critical communication skills for success in research and broadly transferable professional skills. Students will become part of a community of scholars in health sciences. The course is suitable for students beginning the first semester of undergraduate studies and completing their second or."
  },
  {
    "objectID": "talks/NIB-WorkShop/index.html",
    "href": "talks/NIB-WorkShop/index.html",
    "title": "NIB Advanced Bioinformatics Workshop",
    "section": "",
    "text": "Introduction to Bioinformatics, Exploring Nucleic Acid Databases and Data mining, Proteomics: Biochemical Properties, Structure and Interac‑ tions of Proteins, Sanger Sequence data analysis: Quality Assurance and Information Mining From DNA Sequence Data, Sequence Data submis‑ sion to public databases, DNA Amplification and Cloning, Comparative Genomics:Species or Gene identification and sequence determination, Evolutionary Relationship Determination\nInstructor: Dr. Abul Bashar Mir Md. Khademul Islam, Dept. of Genetic Engineering and Biotechnology University of Dhaka\nCenter for Next Generation Sequencing and Analytics by Ministry of Science and Technology, Bangladesh"
  },
  {
    "objectID": "talks/Sanger Institute/index.html",
    "href": "talks/Sanger Institute/index.html",
    "title": "Sanger Institute",
    "section": "",
    "text": "Next generation sequencing (NGS) and analysis of bacterial genomes,NGS data quality control, assembly, reference mapping, in silico isolate characterisation, developing a portable bioinformatics pipeline, and downstream analyses such as phylogenetic inference\nRecipient of the scholarship, Wellcome Sanger Institute ‑ Pathogen Genomics (GPS and JUNO projects), Bill and Melinda Gates Foundation"
  },
  {
    "objectID": "index.html#bioinformatics",
    "href": "index.html#bioinformatics",
    "title": "Preonath Chondrow Dev",
    "section": "Bioinformatics:",
    "text": "Bioinformatics:\n\nProficient in Docker for pipeline building\n\nMicroarray and Bulk RNA sequencing Analysis: using DESeq2, stats, ggplot, corrplot, pheatmap, EDASeq, gProfileR\n\nGenome Quality Checking and Assembly:\n\nIllumina: BCL2fastq, fastQC, MultiQC, Quast, Trimmomatic, fastp, Unicycler, Spades, Megahit\n\nNanopore: fast52fasta, Pilon, Flye\n\n\nGenome Annotation: Kraken2, Prokka, PGAp, Kleborate, Seroba, AMRFinderPlus, Abricat, SRST, MLST, Snippy, Mafft, fasta2phylip, Raxml‑ng, Poppunk, PlasmidFinder, ResFinder\n\nMolecular Dynamics Simulation: using GROMACS\n\nMetagenomics Analysis: using qiime2 and CZID\n\nNextstrain: Built and maintained from Bangladesh for SARS‑CoV‑2\n\nSingle Cell RNA sequencing Analysis: using Scanpy and Seurat - Data pre‑processing, Clustering and Cell annotation, Integration and Batch correction, Cell‑cell communication, BCR background and 10x analysis, Trajectory inference, Differential abundance, Multiomic scATAC"
  },
  {
    "objectID": "index.html#wet-lab",
    "href": "index.html#wet-lab",
    "title": "Preonath Chondrow Dev",
    "section": "Wet Lab:",
    "text": "Wet Lab:\n\nSingle Cell RNA sequencing Analysis: using HoneyComb, 10x Genomics, Parse Biosciences, Library preparation, Sequencing on Illumina and Nanopore\n\nMolecular Techniques: Conventional and Real‑time PCR, DNA & RNA extraction, Agarose & Polyacrylamide Gel Electrophoresis and Imaging, Blotting techniques, Molecular cloning, ELISA & ICT\n\nMicrobiological Techniques: Culture, Biochemical tests"
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/index.html",
    "href": "teaching/Advanced Next Generation Sequencing/index.html",
    "title": "Bacterial Genomics & Antimicrobial Resistance Workshop",
    "section": "",
    "text": "This five-day workshop provided participants with a comprehensive overview of bacterial genomics and its applications in antimicrobial resistance research. Through lectures and hands-on sessions, participants learned how to analyze bacterial genomes for antimicrobial resistance and virulence factors, conduct comparative genomic analysis, and apply bacterial genomics to investigate clinical and scientific problems. The workshop has been conducted three times in 2023-2024."
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/index.html#theaching-assistant",
    "href": "teaching/Advanced Next Generation Sequencing/index.html#theaching-assistant",
    "title": "Bacterial Genomics & Antimicrobial Resistance Workshop",
    "section": "",
    "text": "This five-day workshop provideed participants with a comprehensive overview of bacterial genomics and its applications in antimicrobial resistance research. Through lectures and hands-on sessions, participants had been learned how to analyze bacterial genomes for antimicrobial resistance and virulence factors, conduct comparative genomic analysis, and apply bacterial genomics to investigate clinical and scientific problems. The workshop had been commenceed on 3 time in 2023-2024"
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/index.html#learning-objectives",
    "href": "teaching/Advanced Next Generation Sequencing/index.html#learning-objectives",
    "title": "Bacterial Genomics & Antimicrobial Resistance Workshop",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nIntroduction to bacterial genomics and bioinformatics\nBacterial Genome Analysis: from sequence data to genomes\nBacterial Genome Analysis: Antimicrobial Resistance\nComparative genomics and phylogenetics\nApplication of bacterial genomics\n\nProgram Website"
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/index.html#topics-covered-in-the-training",
    "href": "teaching/Advanced Next Generation Sequencing/index.html#topics-covered-in-the-training",
    "title": "Bacterial Genomics & Antimicrobial Resistance Workshop",
    "section": "Topics Covered in the Training:",
    "text": "Topics Covered in the Training:\nLinux OS, Bash Scripting Genome Assembly, Genome Mapping, Genome Annotation, Genome Submission, MLST Prediction at Total 2 Batches\nProgram Website"
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/index.html#teaching-assistant",
    "href": "teaching/Advanced Next Generation Sequencing/index.html#teaching-assistant",
    "title": "Bacterial Genomics & Antimicrobial Resistance Workshop",
    "section": "",
    "text": "This five-day workshop provided participants with a comprehensive overview of bacterial genomics and its applications in antimicrobial resistance research. Through lectures and hands-on sessions, participants learned how to analyze bacterial genomes for antimicrobial resistance and virulence factors, conduct comparative genomic analysis, and apply bacterial genomics to investigate clinical and scientific problems. The workshop has been conducted three times in 2023-2024."
  },
  {
    "objectID": "teaching/Advanced Next Generation Sequencing/news.html",
    "href": "teaching/Advanced Next Generation Sequencing/news.html",
    "title": "Latest News",
    "section": "",
    "text": "News Section\nContent about recent news or updates can go here."
  },
  {
    "objectID": "teaching/AI_in_Public_Health/index.html",
    "href": "teaching/AI_in_Public_Health/index.html",
    "title": "AI in Public Health",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/AI_in_Public_Health/index.html#teaching-assistant",
    "href": "teaching/AI_in_Public_Health/index.html#teaching-assistant",
    "title": "AI in Public Health",
    "section": "",
    "text": "Bioinformatics BootCamp is an intensive and comprehensive training program designed to equip participants with the essential skills and knowledge in bioinformatics. This bootcamp offers a hands-on learning experience, covering a wide range of topics such as sequence analysis, genomics, proteomics, and data analysis using popular bioinformatics tools and software. Led by experienced instructors, participants will gain practical expertise, enabling them to effectively analyze biological data and make meaningful discoveries in the field of bioinformatics."
  },
  {
    "objectID": "teaching/AI_in_Public_Health/index.html#learning-objectives",
    "href": "teaching/AI_in_Public_Health/index.html#learning-objectives",
    "title": "AI in Public Health",
    "section": "Learning Objectives",
    "text": "Learning Objectives\n\nAI in Public Health, This workshop support from the Bill and Melinda Gates foundation, through the ’Democratizing Public Health Modeling Using AI‑based Tools’; Topics Covered in the Training: Data cleaning and preprocessing, Dengue prediction in Puerto Rico using LSTM and ARIMA, Predicting Pneumonia in X‑rays using CNN\n\nProgram Website"
  },
  {
    "objectID": "teaching/Bioinformatics_for_Biotechnology_Research/index.html",
    "href": "teaching/Bioinformatics_for_Biotechnology_Research/index.html",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "",
    "text": "The Bioinformatics Division at the National Institute of Biotechnology (NIB) offers comprehensive training on Bioinformatics for Biotechnology Research. This training focuses on essential bioinformatics techniques including Linux OS, Bash scripting, and Biopython for Next Generation Sequencing (NGS) data analysis. The course provides both theoretical and practical knowledge, equipping participants with skills to manage and analyze complex datasets in biotechnology research.\nDuring the 3rd Batch of the program held from September 5-9, 2021, key concepts covered included:\n\nLinux Operating System\nBash Scripting for Automation\nPython and Biopython for NGS Data Analysis"
  },
  {
    "objectID": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#overview",
    "href": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#overview",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "",
    "text": "The Bioinformatics Division at the National Institute of Biotechnology (NIB) offers comprehensive training on Bioinformatics for Biotechnology Research. This training focuses on essential bioinformatics techniques including Linux OS, Bash scripting, and Biopython for Next Generation Sequencing (NGS) data analysis. The course provides both theoretical and practical knowledge, equipping participants with skills to manage and analyze complex datasets in biotechnology research.\nDuring the 3rd Batch of the program held from September 5-9, 2021, key concepts covered included:\n\nLinux Operating System\nBash Scripting for Automation\nPython and Biopython for NGS Data Analysis"
  },
  {
    "objectID": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#learning-objectives",
    "href": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#learning-objectives",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nUpon completion of the course, participants will be able to:\n\nUnderstand the basics of Linux OS and apply Bash scripting for managing bioinformatics workflows.\nPerform Next Generation Sequencing (NGS) data analysis using Python and Biopython.\nApply bioinformatics tools for sequence alignment, variant calling, and genomic data interpretation.\nAutomate repetitive tasks using scripting techniques to enhance workflow efficiency.\nGain practical experience in handling large-scale bioinformatics datasets through hands-on activities."
  },
  {
    "objectID": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#topics-covered-in-the-training",
    "href": "teaching/Bioinformatics_for_Biotechnology_Research/index.html#topics-covered-in-the-training",
    "title": "Bioinformatics Division, National Institute of Biotechnology (NIB)",
    "section": "Topics Covered in the Training",
    "text": "Topics Covered in the Training\n\nLinux OS: Practical usage for bioinformatics workflows.\nNGS Data Analysis: Utilizing Python and Biopython for processing sequencing data.\nBash Scripting: Automation techniques for managing large bioinformatics projects.\n\nProgram Website\nApply to the Program"
  },
  {
    "objectID": "index.html#research-experience",
    "href": "index.html#research-experience",
    "title": "Preonath Chondrow Dev",
    "section": "Research Experience",
    "text": "Research Experience\nNational Gene Bank, National Institute of Biotechnology | Dhaka, Bangladesh | (5 Sept 2021 ‑ 30 June 2022)\n\nThesis Project: Development of a disease detection tool from metagenomic sequence data utilizing k‑mer frequency based algorithms: A gut microbiome study\nRevealing the Disease complexity and Population Structure of Human Gut Microbiota from Diverse Cohort of Bangladesh Population\nDeep learning model based tools development, metagenomic analysis, vaccine trial in the mouse model\nRNA seq data analysis and molecular dynamic simulation\nWet Lab: Sample preparation, and DNA extraction‑related activities\n\nChild Health Research Foundation  | Dhaka, Bangladesh | (01 Aug 2022 ‑ Present)\n\nDevelop and apply bioinformatics tools to process, analyze, and interpret high throughput datasets, including bacterial genome sequencing, viral genome sequencing, and unbiased metagenomic sequencing\nApply biological and statistical knowledge to draw rigorous, actionable conclusions from complex data sets. Discuss findings with colleagues and supervisors\nCollaborate with groups across the institution to support research projects\nParticipate in the preparation of written reports and oral presentations summarizing data analysis results, including a detailed description of bioinformatics methods, analysis of results, and synthesis of conclusions from summary tables.\nTake the initiative to acquire new bioinformatics, statistical, or programming skills as needed.\n\n\nCenter for Health Innovation, Research, Action, and Learning ‑ Bangladesh | Dhaka, Bangladesh | (Nov 2023 ‑ Present)\n\nDesign and develop bioinformatics courses for undergraduates and professionals\nDesign and implement bioinformatics solutions for biologists\nLead the omics team to design and implement multi‑omics projects"
  },
  {
    "objectID": "index.html#teaching-experience",
    "href": "index.html#teaching-experience",
    "title": "Preonath Chondrow Dev",
    "section": "Teaching Experience",
    "text": "Teaching Experience\nINSTRUCTOR | [Bioinformatics Division, National Institute of Biotechnology (NIB)] | Dhaka, Bangladesh | (2021)\n\nDelivered training on Bioinformatics for Biotechnology Research\nTopics covered included Linux OS, NGS Data Analysis, and Python.\n\nTEACHiNG ASSiSTANT | [Child Health Research Foundation (CHRF)] | Dhaka, Bangladesh | (2021)\n\nAdvanced Next Generation Sequencing, Topics Covered in the Training: Linux OS, NGS Data Analysis: Quality checking, Quality filter, Genome Assembly, Annotation Using Pathogenwatch, Building Consensus using CZID, Sample Sheet Preparation for NextSeq2000; Total 25 Batches\nBacterial Genomics Antimicrobial Resistance, Topics Covered in the Training: Linux OS, Bash Scripting Genome Assembly, Genome Mapping, Genome Annotation, Genome Submission, MLST Prediction at Total 2 Batches\nAI in Public Health, This workshop support from the Bill and Melinda Gates foundation, through the ’Democratizing Public Health Modeling Using AI‑based Tools’; Topics Covered in the Training: Data cleaning and preprocessing, Dengue prediction in Puerto Rico using LSTM and ARIMA, Predicting Pneumonia in X‑rays using CNN"
  },
  {
    "objectID": "index.html#volunteering",
    "href": "index.html#volunteering",
    "title": "Preonath Chondrow Dev",
    "section": "Volunteering",
    "text": "Volunteering\nACADEMiC TEAM MEMBER | [Bangladesh Mathematical Olympiad] | Dhaka, Bangladesh | (2021)\nEngaged in evaluating examination papers and overseeing exam hall activities to ensure a smooth conduct of the Olympiad\nACADEMiC TEAM MEMBER | [Bangladesh Physics Olympiad] | Dhaka, Bangladesh | (2021)\nPlayed a crucial role in monitoring examination procedures and providing assistance to participants throughout the event.\nACADEMiC TEAM MEMBER | [Bangladesh Physics Olympiad] | Dhaka, Bangladesh | (2021)\nTasked with the assessment of solutions and maintaining order among candidates during the Olympiad competitions\nENZYME | [Bangladesh Biology Olympiad ] | Dhaka, Bangladesh | (2021)\nAssisted in the evaluation process and managed the exam hall logistics to facilitate a fair and efficient Olympiad\nMENTOR | [Bioinformatics School ] | Dhaka, Bangladesh | (2021)\nSolve the different kinds of bioinformatics related problem of the group members"
  },
  {
    "objectID": "training.html",
    "href": "training.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nAdvanced Bioinformatics Training\n\n\n\n\n\nWellcome Sanger Institute\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAdvanced Next Generation Sequencing and Analysis\n\n\n\n\n\nMinistry of Science and Technology, Bangladesh\n\n\n\n\n\nOct 15, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nHCA Thailand 2024 Workshop\n\n\n\n\n\nComputational and Experimental Design Workshop\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndustrial Training (Academic)\n\n\n\n\n\nBangladesh Council of Scientific and Industrial Research (BCSIR), National Forensic DNA Profiling Laboratory ( NFDPL), Delta Pharma Ltd.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNasal Swab Cell Isolation, scRNA Sequencing and Analysis\n\n\n\n\n\nOrdovas-Montanes Lab, Boston Children’s Hospital\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSeroCalculator Workshop\n\n\n\n\n\nUniversity of California, Davis; Sabin Vaccine Institute\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "training/undergraduate-research-importance-benefits-and-challenges/index.html",
    "href": "training/undergraduate-research-importance-benefits-and-challenges/index.html",
    "title": "Advance Bioinformatics Training",
    "section": "",
    "text": "The Undergraduate Research Training Program(URTP) provides intermediate-level biomedical research training for undergraduates in a supportive environment with supplement educational activities. Furthermore, this course is designed to enhance the undergraduate research experience by focusing on critical communication skills for success in research and broadly transferable professional skills. Students will become part of a community of scholars in health sciences. The course is suitable for students beginning the first semester of undergraduate studies and completing their second or"
  },
  {
    "objectID": "training/Sanger Institute/index.html",
    "href": "training/Sanger Institute/index.html",
    "title": "Sanger Institute",
    "section": "",
    "text": "Next generation sequencing (NGS) and analysis of bacterial genomes,NGS data quality control, assembly, reference mapping, in silico isolate characterisation, developing a portable bioinformatics pipeline, and downstream analyses such as phylogenetic inference\nRecipient of the scholarship, Wellcome Sanger Institute ‑ Pathogen Genomics (GPS and JUNO projects), Bill and Melinda Gates Foundation"
  },
  {
    "objectID": "training/NIB-WorkShop/index.html",
    "href": "training/NIB-WorkShop/index.html",
    "title": "NIB Advanced Bioinformatics Workshop",
    "section": "",
    "text": "Introduction to Bioinformatics, Exploring Nucleic Acid Databases and Data mining, Proteomics: Biochemical Properties, Structure and Interac‑ tions of Proteins, Sanger Sequence data analysis: Quality Assurance and Information Mining From DNA Sequence Data, Sequence Data submis‑ sion to public databases, DNA Amplification and Cloning, Comparative Genomics:Species or Gene identification and sequence determination, Evolutionary Relationship Determination\nInstructor: Dr. Abul Bashar Mir Md. Khademul Islam, Dept. of Genetic Engineering and Biotechnology University of Dhaka\nCenter for Next Generation Sequencing and Analytics by Ministry of Science and Technology, Bangladesh"
  },
  {
    "objectID": "training/Nasal Swab Dissociation Livestream/index.html",
    "href": "training/Nasal Swab Dissociation Livestream/index.html",
    "title": "Nasal Swab Cell Isolation, scRNA Sequencing and Analysis",
    "section": "",
    "text": "/* Ensure no horizontal scrolling / html, body { overflow-x: hidden; margin: 0; padding: 0; width: 100vw; / Ensure the body spans the full width of the viewport */ }\n/* General container styling / .container, .slideshow-container, #program-details, div { width: 100vw; / Ensure containers fit full width of the window / max-width: 100%; / Prevent any element from exceeding the window / margin: 0; / Remove margins / padding: 0; / Remove padding / box-sizing: border-box; / Include padding in width calculations */ }\n/* Slideshow container adjustments / .slideshow-container { position: relative; height: 80vh; / Adjust the height for the slideshow */ width: 100vw; overflow: hidden; margin: 0; }\n/* Ensure that images also fit within the window / img { max-width: 100%; height: auto; margin: 0 auto; display: block; / Ensures the image centers itself */ }\n/* Program details section adjustments / #program-details { font-family: Arial, sans-serif; color: #333; line-height: 1.6; background-color: #f9f9f9; max-width: 100vw; / Ensure this section spans the full width / width: 100vw; margin: 0; / Remove margins / padding: 20px; / Padding for spacing */ box-sizing: border-box; }\n/* Header elements adjustments */ h2, h3 { margin-top: 10px; margin-bottom: 10px; font-weight: bold; text-align: center; }\n/* Adjust paragraphs for spacing and fitting / p { font-size: 1em; margin: 10px auto; text-align: center; / Center the text */ }\n/* Collaboration logos / .collaboration-logos img { max-width: 200px; / Adjust the width of the logos / margin: 10px; display: inline-block; vertical-align: middle; / Align logos to the center */ }\n/* Button alignment */ .btn { margin: 0 auto; display: block; }\n/* Footer / footer { text-align: center; padding: 10px 0; background-color: #003366; color: white; position: relative; width: 100vw; / Make sure the footer fits the full width */ }"
  },
  {
    "objectID": "training/state-the-art-of-microbial-genome-analysi/index.html",
    "href": "training/state-the-art-of-microbial-genome-analysi/index.html",
    "title": "Industrial Training (Academic)",
    "section": "",
    "text": "Genomics is the study of whole genomes of organisms, and incorporates elements from ge- netics. Genomics uses a combination of recombinant DNA, DNA sequencing methods, and bioinformatics to sequence, assemble, and analyse the structure and function of genomes. Genomics have become an inter-disciplinary(Computer Science, Statistics, Biology) sci- ence. The genomic data analysis steps typically include data collection, quality check and cleaning, processing, modeling, visualization and reporting.\n\nNational Forensic DNA Profiling Laboratory (NFDPL) - Dhaka\nBangladesh Council of Scientific and Industrial Research(BCSIR) - Dhaka\nDelta Pharma Ltd. Kishoreganj, Bangladesh"
  },
  {
    "objectID": "training/Advanced_Bioinformatics_Training_Sanger_Institute/index.html",
    "href": "training/Advanced_Bioinformatics_Training_Sanger_Institute/index.html",
    "title": "Advanced Bioinformatics Training",
    "section": "",
    "text": "Next generation sequencing (NGS) and analysis of bacterial genomes,NGS data quality control, assembly, reference mapping, in silico isolate characterisation, developing a portable bioinformatics pipeline, and downstream analyses such as phylogenetic inference\nRecipient of the scholarship, Wellcome Sanger Institute ‑ Pathogen Genomics (GPS and JUNO projects), Bill and Melinda Gates Foundation"
  },
  {
    "objectID": "conference.html",
    "href": "conference.html",
    "title": "",
    "section": "",
    "text": "Building a single‑cell atlas of the nasopharyngeal mucosa to investigate SARS‑CoV‑2 infection\n\n\n\n\n\n2024 HCA Single‑Cell Omics in‑person Computational and Experimental Design Workshop\n\n\n\n\n\nJun 24, 2024\n\n\n\n\n\n\n\n\n\n\n\n\nGenomic Epidemiology of Salmonella Paratyphi B Isolates from Bangladesh\n\n\n\n\n\n13th International Conference on Typhoid and Other Invasive Salmonelloses\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nParatype v1.1: Recent Updates to the Genotyping Tool for Paratyphoid Fever Surveillance\n\n\n\n\n\n13th International Conference on Typhoid and Other Invasive Salmonelloses\n\n\n\n\n\nDec 5, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "conferences/single_cell_atlas/index.html#nasopharyngeal-mucosa-to-investigate-sarscov2-infection",
    "href": "conferences/single_cell_atlas/index.html#nasopharyngeal-mucosa-to-investigate-sarscov2-infection",
    "title": "Building a single‑cell atlas of the nasopharyngeal mucosa to investigate SARS‑CoV‑2 infection",
    "section": "Nasopharyngeal Mucosa to Investigate SARS‑CoV‑2 Infection",
    "text": "Nasopharyngeal Mucosa to Investigate SARS‑CoV‑2 Infection"
  },
  {
    "objectID": "training/Industrial_training/index.html",
    "href": "training/Industrial_training/index.html",
    "title": "Industrial Training (Academic)",
    "section": "",
    "text": "Genomics is the study of whole genomes of organisms, and incorporates elements from ge- netics. Genomics uses a combination of recombinant DNA, DNA sequencing methods, and bioinformatics to sequence, assemble, and analyse the structure and function of genomes. Genomics have become an inter-disciplinary(Computer Science, Statistics, Biology) sci- ence. The genomic data analysis steps typically include data collection, quality check and cleaning, processing, modeling, visualization and reporting.\n\nNational Forensic DNA Profiling Laboratory (NFDPL) - Dhaka\nBangladesh Council of Scientific and Industrial Research(BCSIR) - Dhaka\nDelta Pharma Ltd. Kishoreganj, Bangladesh"
  },
  {
    "objectID": "training/NIB WorkShop/index.html",
    "href": "training/NIB WorkShop/index.html",
    "title": "Advanced Next Generation Sequencing and Analysis",
    "section": "",
    "text": "Revealing_the_Disease_Complexity_and_Population_Structure_of_Human_Gut_Microbiota_from_a_Diverse_Cohort_of_the_Bangladeshi_Population - Introduction to Bioinformatics, Exploring Nucleic Acid Databases and Data mining, Proteomics: Biochemical Properties, Structure and Interac‑ tions of Proteins, Sanger Sequence data analysis: Quality Assurance and Information Mining From DNA Sequence Data, Sequence Data submis‑ sion to public databases, DNA Amplification and Cloning, Comparative Genomics:Species or Gene identification and sequence determination, Evolutionary Relationship Determination - Instructor: Dr. Abul Bashar Mir Md. Khademul Islam, Dept. of Genetic Engineering and Biotechnology University of Dhaka - Center for Next Generation Sequencing and Analytics by Ministry of Science and Technology, Bangladesh"
  },
  {
    "objectID": "publications/conference/virustaxo.html",
    "href": "publications/conference/virustaxo.html",
    "title": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment",
    "section": "",
    "text": "Classification of viruses into their taxonomic ranks (e.g., order, family, genus) provides a framework to organize an abundant population of viruses. Next-generation metagenomic sequencing technologies lead to a rapid increase in generating sequencing data of viruses which require bioinformatics tools to analyze the taxonomy. Many metagenomic taxonomy classifiers have been developed to study microbiomes, but it is particularly challenging to assign the taxonomy of diverse virus sequences and there is a growing need for dedicated methods to be developed that are optimized to classify virus sequences into their taxa. VirusTaxo, developed using diverse (e.g., 402 DNA and 280 RNA) genera of viruses, has an average accuracy of 93% at genus level prediction in DNA and RNA viruses. VirusTaxo outperformed existing taxonomic classifiers by assigning taxonomy to a larger fraction of metagenomic contigs compared to other methods. Benchmarking of VirusTaxo on a collection of SARS-CoV-2 sequencing libraries and metavirome datasets suggests that VirusTaxo can characterize virus taxonomy from highly diverse contigs and provide a reliable decision on the taxonomy of viruses.\nKeywords: Virus Taxonomy, Hierarchical Classification, k-mer, Genome"
  },
  {
    "objectID": "publications/conference/virustaxo.html#abstract",
    "href": "publications/conference/virustaxo.html#abstract",
    "title": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment",
    "section": "",
    "text": "Classification of viruses into their taxonomic ranks (e.g., order, family, genus) provides a framework to organize an abundant population of viruses. Next-generation metagenomic sequencing technologies lead to a rapid increase in generating sequencing data of viruses which require bioinformatics tools to analyze the taxonomy. Many metagenomic taxonomy classifiers have been developed to study microbiomes, but it is particularly challenging to assign the taxonomy of diverse virus sequences and there is a growing need for dedicated methods to be developed that are optimized to classify virus sequences into their taxa. VirusTaxo, developed using diverse (e.g., 402 DNA and 280 RNA) genera of viruses, has an average accuracy of 93% at genus level prediction in DNA and RNA viruses. VirusTaxo outperformed existing taxonomic classifiers by assigning taxonomy to a larger fraction of metagenomic contigs compared to other methods. Benchmarking of VirusTaxo on a collection of SARS-CoV-2 sequencing libraries and metavirome datasets suggests that VirusTaxo can characterize virus taxonomy from highly diverse contigs and provide a reliable decision on the taxonomy of viruses.\nKeywords: Virus Taxonomy, Hierarchical Classification, k-mer, Genome"
  },
  {
    "objectID": "publications/conference/type_2_diabetic.html",
    "href": "publications/conference/type_2_diabetic.html",
    "title": "Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients",
    "section": "",
    "text": "Patients with type-2 diabetes mellitus (T2DM) are known to suffer from hyperlipidemia. How hyperlipidemia is associated with controlled and uncontrolled T2DM patients in Bangladesh remained to be addressed. In this cross-sectional study, 211 participants were enrolled who have been suffering from T2DM for more than 4 years from the northeastern part of Bangladesh. Controlled and uncontrolled patients were defined with their plasma glycated hemoglobin (HbA1c) levels. Among them, 39% and 61% were in the diabetic-controlled and uncontrolled groups, respectively. Indeed, the diabetic uncontrolled group showed a higher frequency of hypercholesterolemia, hypertriglyceridemia, hyper LDL-cholesterolemia, and hypo HDL-cholesterolemia compared to the diabetic controlled group. Lipid profiling analysis revealed significantly elevated (p&lt;0.0001) levels of cholesterol, triglyceride, and low-density lipoprotein (LDL) in uncontrolled than the controlled group, while high-density lipoprotein (HDL) was significantly (p&lt;0.0001) lower in uncontrolled diabetic patients. Interestingly, significantly (p&lt;0.05) higher dyslipidemia was also observed in individuals with a controlled diabetic population, who have been suffering from T2DM for more than 7 years. Therefore, these results highlight that not only the diabetic uncontrolled but also the controlled group patients have a high risk of developing hyperlipidemia after a certain period of diabetes.\nKeywords: Uncontrolled, Duration, Controlled, Hyperlipidemia, Diabetes"
  },
  {
    "objectID": "publications/conference/type_2_diabetic.html#abstract",
    "href": "publications/conference/type_2_diabetic.html#abstract",
    "title": "Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients",
    "section": "",
    "text": "Patients with type-2 diabetes mellitus (T2DM) are known to suffer from hyperlipidemia. How hyperlipidemia is associated with controlled and uncontrolled T2DM patients in Bangladesh remained to be addressed. In this cross-sectional study, 211 participants were enrolled who have been suffering from T2DM for more than 4 years from the northeastern part of Bangladesh. Controlled and uncontrolled patients were defined with their plasma glycated hemoglobin (HbA1c) levels. Among them, 39% and 61% were in the diabetic-controlled and uncontrolled groups, respectively. Indeed, the diabetic uncontrolled group showed a higher frequency of hypercholesterolemia, hypertriglyceridemia, hyper LDL-cholesterolemia, and hypo HDL-cholesterolemia compared to the diabetic controlled group. Lipid profiling analysis revealed significantly elevated (p&lt;0.0001) levels of cholesterol, triglyceride, and low-density lipoprotein (LDL) in uncontrolled than the controlled group, while high-density lipoprotein (HDL) was significantly (p&lt;0.0001) lower in uncontrolled diabetic patients. Interestingly, significantly (p&lt;0.05) higher dyslipidemia was also observed in individuals with a controlled diabetic population, who have been suffering from T2DM for more than 7 years. Therefore, these results highlight that not only the diabetic uncontrolled but also the controlled group patients have a high risk of developing hyperlipidemia after a certain period of diabetes.\nKeywords: Uncontrolled, Duration, Controlled, Hyperlipidemia, Diabetes"
  },
  {
    "objectID": "publications/conference/core_genome.html",
    "href": "publications/conference/core_genome.html",
    "title": "Exploration of Streptococcus core genome to reveal druggable targets and novel therapeutics against S. pneumoniae",
    "section": "",
    "text": "Streptococcus pneumoniae (S. pneumoniae), the major etiological agent of community-acquired pneumonia (CAP), contributes significantly to the global burden of infectious diseases which is getting resistant day by day. Nearly 30% of the S. pneumoniae genomes encode hypothetical proteins (HPs), and better understandings of these HPs in virulence and pathogenicity plausibly decipher new treatments. Some of the HPs are present across many Streptococcus species, and systematic assessment of these unexplored HPs will disclose prospective drug targets. In this study, through a stringent bioinformatics analysis of the core genome and proteome of S. pneumoniae PCS8235, we identified and analyzed 28 HPs that are common in many Streptococcus species and might have a potential role in the virulence or pathogenesis of the bacteria. Functional annotations of the proteins were conducted based on the physicochemical properties, subcellular localization, virulence prediction, protein-protein interactions, and identification of essential genes, to find potentially druggable proteins among 28 HPs. The majority of the HPs are involved in bacterial transcription and translation. Besides, some of them were homologs of enzymes, binding proteins, transporters, and regulators. Protein-protein interactions revealed HP PCS8235_RS05845 made the highest interactions with other HPs and also has TRP structural motif along with virulent and pathogenic properties indicating it has critical cellular functions and might go under unconventional protein secretions. The second highest interacting protein, HP PCS8235_RS02595, interacts with the Regulator of Chromosomal Segregation (RocS) which participates in chromosome segregation and nucleoid protection in S. pneumoniae. In this interacting network, 54% of protein members have virulent properties, and 40% contain pathogenic properties. Among them, most of these proteins circulate in the cytoplasmic area and have hydrophilic properties. Finally, molecular docking and dynamics simulation demonstrated that the antimalarial drug Artenimol can act as a drug repurposing candidate against HP PCS8235_RS04650 of S. pneumoniae. Hence, the present study could aid in developing drugs against S. pneumoniae.\nKeywords: Uncontrolled, Duration, Controlled, Hyperlipidemia, Diabetes"
  },
  {
    "objectID": "publications/conference/core_genome.html#abstract",
    "href": "publications/conference/core_genome.html#abstract",
    "title": "Exploration of Streptococcus core genome to reveal druggable targets and novel therapeutics against S. pneumoniae",
    "section": "",
    "text": "Streptococcus pneumoniae (S. pneumoniae), the major etiological agent of community-acquired pneumonia (CAP), contributes significantly to the global burden of infectious diseases which is getting resistant day by day. Nearly 30% of the S. pneumoniae genomes encode hypothetical proteins (HPs), and better understandings of these HPs in virulence and pathogenicity plausibly decipher new treatments. Some of the HPs are present across many Streptococcus species, and systematic assessment of these unexplored HPs will disclose prospective drug targets. In this study, through a stringent bioinformatics analysis of the core genome and proteome of S. pneumoniae PCS8235, we identified and analyzed 28 HPs that are common in many Streptococcus species and might have a potential role in the virulence or pathogenesis of the bacteria. Functional annotations of the proteins were conducted based on the physicochemical properties, subcellular localization, virulence prediction, protein-protein interactions, and identification of essential genes, to find potentially druggable proteins among 28 HPs. The majority of the HPs are involved in bacterial transcription and translation. Besides, some of them were homologs of enzymes, binding proteins, transporters, and regulators. Protein-protein interactions revealed HP PCS8235_RS05845 made the highest interactions with other HPs and also has TRP structural motif along with virulent and pathogenic properties indicating it has critical cellular functions and might go under unconventional protein secretions. The second highest interacting protein, HP PCS8235_RS02595, interacts with the Regulator of Chromosomal Segregation (RocS) which participates in chromosome segregation and nucleoid protection in S. pneumoniae. In this interacting network, 54% of protein members have virulent properties, and 40% contain pathogenic properties. Among them, most of these proteins circulate in the cytoplasmic area and have hydrophilic properties. Finally, molecular docking and dynamics simulation demonstrated that the antimalarial drug Artenimol can act as a drug repurposing candidate against HP PCS8235_RS04650 of S. pneumoniae. Hence, the present study could aid in developing drugs against S. pneumoniae.\nKeywords: Uncontrolled, Duration, Controlled, Hyperlipidemia, Diabetes"
  },
  {
    "objectID": "publications/conference/phage.html",
    "href": "publications/conference/phage.html",
    "title": "Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh",
    "section": "",
    "text": "This report presents the near-complete genome sequences of 14 bacteriophages that infect Salmonella Typhi, identified through environmental surveillance in Bangladesh between August 2021 and June 2022. The bacteriophages, belonging to the genus Kayfunavirus, Macdonaldcampvirus, and Teseptimavirus, share evolutionary ties with previously documented Typhi bacteriophages.\nKey Findings: - The bacteriophages show significant potential for furthering our understanding of Salmonella Typhi pathogenesis. - Highlighted the importance of environmental surveillance in tracking bacterial and viral interactions. - Provided potential targets for future therapeutic interventions."
  },
  {
    "objectID": "publications/conference/phage.html#abstract",
    "href": "publications/conference/phage.html#abstract",
    "title": "Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh",
    "section": "",
    "text": "This report presents the near-complete genome sequences of 14 bacteriophages that infect Salmonella Typhi, identified through environmental surveillance in Bangladesh between August 2021 and June 2022. The bacteriophages, belonging to the genus Kayfunavirus, Macdonaldcampvirus, and Teseptimavirus, share evolutionary ties with previously documented Typhi bacteriophages.\nKey Findings: - The bacteriophages show significant potential for furthering our understanding of Salmonella Typhi pathogenesis. - Highlighted the importance of environmental surveillance in tracking bacterial and viral interactions. - Provided potential targets for future therapeutic interventions."
  },
  {
    "objectID": "publications/conference/phage.html#figures",
    "href": "publications/conference/phage.html#figures",
    "title": "Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh",
    "section": "Figures",
    "text": "Figures\n\nFigure 1: Genome Maps of Identified Bacteriophages."
  },
  {
    "objectID": "publications/conference/phage.html#supporting-information",
    "href": "publications/conference/phage.html#supporting-information",
    "title": "Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh",
    "section": "Supporting Information",
    "text": "Supporting Information\n\nAuthor Cover Letter: Provides additional context on the study’s impact and novelty.\nRevision Response: Addresses the feedback from initial submission rounds.\nData Citation Compliance: Ensures all datasets used are properly cited according to ASM’s policies."
  },
  {
    "objectID": "publications/conference/phage.html#data-availability",
    "href": "publications/conference/phage.html#data-availability",
    "title": "Genome sequences of bacteriophages that infect Salmonella Typhi from Bangladesh",
    "section": "Data Availability",
    "text": "Data Availability\nAll relevant data are within the paper and its Supporting Information files."
  },
  {
    "objectID": "publications/conference/monkeypox.html",
    "href": "publications/conference/monkeypox.html",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "",
    "text": "Provide a brief summary of the research focus, methodology, key findings, and implications. This should encapsulate the essence of the transcriptomic network analysis and the machine learning approach used to identify potential biomarkers for Mpox virus infection.\nKeywords: Mpox Virus, Biomarkers, Transcriptomics, Network Analysis, Machine Learning"
  },
  {
    "objectID": "publications/conference/monkeypox.html#abstract",
    "href": "publications/conference/monkeypox.html#abstract",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "",
    "text": "Provide a brief summary of the research focus, methodology, key findings, and implications. This should encapsulate the essence of the transcriptomic network analysis and the machine learning approach used to identify potential biomarkers for Mpox virus infection.\nKeywords: Mpox Virus, Biomarkers, Transcriptomics, Network Analysis, Machine Learning"
  },
  {
    "objectID": "publications/conference/monkeypox.html#introduction",
    "href": "publications/conference/monkeypox.html#introduction",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Introduction",
    "text": "Introduction\nDetail the background and the importance of identifying biomarkers for Mpox virus. Discuss previous research in the area and how this paper aims to advance our understanding."
  },
  {
    "objectID": "publications/conference/monkeypox.html#methodology",
    "href": "publications/conference/monkeypox.html#methodology",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Methodology",
    "text": "Methodology\nDescribe the methods used in the study, including data sources, network analysis techniques, machine learning models, and validation approaches."
  },
  {
    "objectID": "publications/conference/monkeypox.html#results",
    "href": "publications/conference/monkeypox.html#results",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Results:",
    "text": "Results:"
  },
  {
    "objectID": "publications/conference/monkeypox.html#discussion",
    "href": "publications/conference/monkeypox.html#discussion",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Discussion",
    "text": "Discussion\nAnalyze the findings, compare with existing literature, discuss the limitations of the study, and suggest areas for future research."
  },
  {
    "objectID": "publications/conference/monkeypox.html#conclusion",
    "href": "publications/conference/monkeypox.html#conclusion",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Conclusion",
    "text": "Conclusion\nConclude with the implications of the findings for clinical practice or public health response to Mpox virus."
  },
  {
    "objectID": "publications/conference/monkeypox.html#data-availability",
    "href": "publications/conference/monkeypox.html#data-availability",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Data Availability",
    "text": "Data Availability\nState where the data supporting this research can be accessed if it is available to the public."
  },
  {
    "objectID": "publications/conference/monkeypox.html#conflict-of-interest",
    "href": "publications/conference/monkeypox.html#conflict-of-interest",
    "title": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "section": "Conflict of Interest",
    "text": "Conflict of Interest\nSpecify if any exists or state “No conflict of interest” if none.\nAdditional Notes: This document serves to prepare for potential conference presentations or further publications discussing the extended impacts of the research."
  },
  {
    "objectID": "publications/conference/virustaxo.html#results",
    "href": "publications/conference/virustaxo.html#results",
    "title": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k‑mer enrichment",
    "section": "Results:",
    "text": "Results:\n\nClassification of taxonomic ranks of viruses using VirusTaxo\n \n\n\nAccuracy of VirusTaxo for order, family, and genus level classification in the pilot dataset.\n \n\n\nBenchmarking of VirusTaxo for SARS-CoV-2 genomes"
  },
  {
    "objectID": "publications/conference/type_2_diabetic.html#results",
    "href": "publications/conference/type_2_diabetic.html#results",
    "title": "Prevalence of hyperlipidemia in controlled and uncontrolled type‑2 diabetic patients",
    "section": "Results:",
    "text": "Results:\n\nClassification of taxonomic ranks of viruses using VirusTaxo\n \n\n\nAccuracy of VirusTaxo for order, family, and genus level classification in the pilot dataset.\n \n\n\nBenchmarking of VirusTaxo for SARS-CoV-2 genomes"
  },
  {
    "objectID": "about_1.html",
    "href": "about_1.html",
    "title": "\nPreonath Chondrow Dev\n",
    "section": "",
    "text": "Preonath Chondrow Dev\n\n\nResearch Officer and Bioinformatician\n\n\nI work at the Child Health Research Foundation in Dhaka, Bangladesh, improving child health through bioinformatics research.\n\n\n\n&lt;a href=\"https://www.linkedin.com/in/preonath-shuvo-26aa1416b/\" class=\"icon\"&gt;&lt;i class=\"fab fa-linkedin\"&gt;&lt;/i&gt;&lt;/a&gt;\n&lt;a href=\"https://github.com/preonath\" class=\"icon\"&gt;&lt;i class=\"fab fa-github\"&gt;&lt;/i&gt;&lt;/a&gt;\n&lt;a href=\"https://twitter.com/PreonathShuvo\" class=\"icon\"&gt;&lt;i class=\"fab fa-twitter\"&gt;&lt;/i&gt;&lt;/a&gt;\n&lt;a href=\"mailto:preonath2838@gmail.com\" class=\"icon\"&gt;&lt;i class=\"fas fa-envelope\"&gt;&lt;/i&gt;&lt;/a&gt;\n\n\n\n\n\nAbout Me\n\n\nI am a Bioinformatician at the Child Health Research Foundation, combining biochemistry, molecular biology, and computer science to improve human health. I specialize in developing state-of-the-art computational methods to uncover the genomic complexities of diseases and collaborate with experts to deliver impactful solutions.\n\n\n\n\nEducation\n\n\n\n&lt;h3&gt;Master of Science (2020 - 2022)&lt;/h3&gt;\n&lt;p&gt;&lt;a href=\"https://www.sust.edu/\"&gt;Shahjalal University of Science and Technology (SUST)&lt;/a&gt;, Sylhet, Bangladesh&lt;br&gt; School of Life Sciences, Biochemistry and Molecular Biology&lt;/p&gt;\n&lt;img src=\"files/img/sust.jpg\" alt=\"SUST Logo\" style=\"max-width: 80px;\"&gt;\n\n\n\n&lt;h3&gt;Bachelor of Science (2016 - 2019)&lt;/h3&gt;\n&lt;p&gt;&lt;a href=\"https://www.sust.edu/\"&gt;Shahjalal University of Science and Technology (SUST)&lt;/a&gt;, Sylhet, Bangladesh&lt;br&gt; School of Life Sciences, Biochemistry and Molecular Biology&lt;/p&gt;\n&lt;img src=\"files/img/sust.jpg\" alt=\"SUST Logo\" style=\"max-width: 80px;\"&gt;\n\n\n\n\nExperience\n\n\n\nResearch Officer | Child Health Research Foundation\n\n\nDhaka, Bangladesh | (Aug 2022 - Present)\n\n\n\nDeveloped bioinformatics tools for bacterial and viral genome sequencing.\n\n\nAnalyzed high-throughput datasets for actionable research insights.\n\n\n\nResearcher | National Gene Bank\n\n\nDhaka, Bangladesh | (2021 - 2022)\n\n\n\nDeveloped a disease detection tool for gut microbiome studies using metagenomic data.\n\n\nLed a vaccine trial project and performed RNA seq data analysis.\n\n\n\n\n\nVolunteering\n\n\n\nAcademic Team Member | Bangladesh Physics Olympiad\n\n \n\nPlayed a crucial role in monitoring examination procedures and assisting participants throughout the event."
  },
  {
    "objectID": "conferences/single_cell_atlas/index.html",
    "href": "conferences/single_cell_atlas/index.html",
    "title": "Building a single‑cell atlas of the nasopharyngeal mucosa to investigate SARS‑CoV‑2 infection",
    "section": "",
    "text": "Building a comprehensive single-cell atlas of the nasopharyngeal mucosa to study SARS‑CoV‑2 infection\n\n\nThis research aims to build a single-cell atlas of the nasopharyngeal mucosa by integrating datasets from various studies to investigate SARS‑CoV‑2 infection.\n\n\n\n\n\nSteps in Building the Atlas\n\n\n\nDataset selection: Tissues related to the respiratory system, focusing on the nasopharynx.\n\n\nIntegration of datasets from studies (Yoshida, Ziegler, Ren), using Harmony to combine over 245,000 cells.\n\n\nClustering analysis of integrated dataset to define distinct cell types in the nasopharyngeal tissue.\n\n\nCell-type clusters identified with the integrated dataset and visualization of immune cell subtypes.\n\n\n\nNext Steps\n\n\n\nAssess expression of SARS‑CoV‑2 in the nasopharynx.\n\n\nReannotate cell types to identify novel or other subtypes.\n\n\nAnalyze the dataset with demographic details like age, SARS-CoV-2 status, and more.\n\n\nCompare findings from the Bangladesh cohort with the global integrated dataset.\n\n\n\nTeam\n\n\n\nPreonath Chondrow Dev, Apurba Rajib Malaker, Deb Purna Keya, Yogesh Hooda, Senjuti Saha*\n\n\n\nAcknowledgements\n\n\nSenjuti Saha & team - Child Health Research Foundation, José Ordovás-Montañes & team - Harvard Stem Cell Institute, Alex Shalek & team - MIT, Bruce Horwitz & team - Boston Children’s Hospital, etc."
  },
  {
    "objectID": "use.html#workflow-management-and-pipeline-development",
    "href": "use.html#workflow-management-and-pipeline-development",
    "title": "What I Use",
    "section": "Workflow Management and Pipeline Development:",
    "text": "Workflow Management and Pipeline Development:\n\nNextflow is used to create reproducible and scalable data analysis pipelines.\nIntegrates with software such as Docker, Singularity, and Conda for environment management.\nIdeal for processing large-scale data, especially in genomics and bioinformatics.\nCan manage complex workflows involving multiple tools like FastQC, STAR, HISAT2, DESeq2, and more."
  },
  {
    "objectID": "use.html#key-tools-in-nextflow-pipelines",
    "href": "use.html#key-tools-in-nextflow-pipelines",
    "title": "What I Use",
    "section": "Key Tools in Nextflow Pipelines:",
    "text": "Key Tools in Nextflow Pipelines:\n\nFastQC: For quality control of raw sequence data.\nTrimmomatic: For trimming low-quality reads and adapters.\nSTAR/HISAT2: For read alignment to reference genomes.\nDESeq2/edgeR: For differential expression analysis.\nMultiQC: To aggregate results across multiple samples for easy interpretation.\nARIBA: Rapid antimicrobial resistance genotyping directly from sequencing reads.\nBCFtools: For variant calling and manipulating VCF/BCF files.\nSAMtools: For handling BAM/SAM files.\nBWA: For sequence alignment.\nDocker Images (StaPH-B): Pre-built containerized environments with bioinformatics tools like ARIBA, BCFtools, Kraken 2, and more.\nfastp: Ultra-fast all-in-one FASTQ preprocessor.\nKraken 2: For metagenomic analysis and taxonomy assignment.\nmlst: For multi-locus sequence typing.\nPopPUNK: Bacterial genomic epidemiology analysis tool.\nQUAST: For genome assembly evaluation.\nSeroBA: High-throughput serotyping of Streptococcus pneumoniae.\nUnicycler: Bacterial genome assembly tool.\nSPN-PBP-AMR: Predicts penicillin resistance in Streptococcus pneumoniae."
  },
  {
    "objectID": "use.html#cloud-integration",
    "href": "use.html#cloud-integration",
    "title": "What I Use",
    "section": "Cloud Integration:",
    "text": "Cloud Integration:\n\nSupports cloud-based workflow execution on platforms like AWS, Google Cloud, and Azure.\nScalable and efficient for processing large datasets in distributed computing environments."
  },
  {
    "objectID": "use.html#example-use-cases",
    "href": "use.html#example-use-cases",
    "title": "What I Use",
    "section": "Example Use Cases:",
    "text": "Example Use Cases:\n\nGPS Pipeline\nKPN pipeline\nRNA-Seq pipelines\nWhole Genome Sequencing (WGS)\nSingle-cell RNA-Seq analysis\nMetagenomics analysis"
  },
  {
    "objectID": "use.html#python-libraries-for-data-analysis-and-scientific-computing",
    "href": "use.html#python-libraries-for-data-analysis-and-scientific-computing",
    "title": "What I Use",
    "section": "Python Libraries for Data Analysis and Scientific Computing:",
    "text": "Python Libraries for Data Analysis and Scientific Computing:\n\nNumPy: Used for numerical and scientific computing, providing support for arrays and matrices.\nPandas: For data manipulation and analysis, offering data structures and operations for numerical tables and time series.\nMatplotlib: A plotting library for creating static, animated, and interactive visualizations in Python.\nSeaborn: Built on top of Matplotlib, Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.\nPlotly: An interactive plotting library that enables the creation of web-based, interactive plots and dashboards.\nresearchpy: Simplifies statistical tests and summarizing data in Python, particularly for social science research.\nDask: Designed for parallel computing, it is used for processing big data and creating scalable data pipelines."
  },
  {
    "objectID": "use.html#machine-learning-and-image-processing",
    "href": "use.html#machine-learning-and-image-processing",
    "title": "What I Use",
    "section": "Machine Learning and Image Processing:",
    "text": "Machine Learning and Image Processing:\n\nscikit-learn: A powerful library for machine learning and data mining, supporting a wide range of supervised and unsupervised algorithms.\nscikit-image: For image processing tasks, especially used in life science applications like segmentation, filtering, and transformations."
  },
  {
    "objectID": "use.html#bioinformatics-tools-in-python",
    "href": "use.html#bioinformatics-tools-in-python",
    "title": "What I Use",
    "section": "Bioinformatics Tools in Python:",
    "text": "Bioinformatics Tools in Python:\n\nBiopython: A comprehensive library for biological computation, supporting sequence analysis, file parsing (FASTA, GenBank), phylogenetics, and more. Widely used in genomics, protein analysis, and molecular biology. Table of contents with Tools:\n\nSequence objects: Handling biological sequences using the Seq object from Biopython.\nSequence annotation objects: Annotating sequences with SeqRecord and Feature objects.\nSequence Input/Output: Tools like SeqIO and AlignIO for reading and writing sequence file formats (e.g., FASTA, GenBank).\nSequence alignments: Working with sequence alignment objects using AlignIO.\nPairwise sequence alignment: Performing pairwise sequence alignment with Bio.pairwise2.\nMultiple Sequence Alignment objects: Handling and manipulating multiple sequence alignments using MultipleSeqAlignment and AlignIO.\nPairwise alignments using pairwise2: Detailed use of the pairwise2 module for flexible sequence alignment.\nBLAST : Accessing NCBI BLAST services with the NCBIXML and NCBIWWW modules to parse BLAST output.\nAccessing NCBI’s Entrez databases: Using Bio.Entrez for accessing and retrieving data from NCBI’s Entrez databases (e.g., PubMed, GenBank).\nSwiss-Prot and ExPASy: Tools for accessing Swiss-Prot (UniProt) and ExPASy databases for protein sequences.\nGoing 3D: The PDB module: Parsing and analyzing 3D protein structures with Bio.PDB.\nBio.PopGen: Population genetics: Tools for analyzing population genetics using the Bio.PopGen module.\nPhylogenetics with Bio.Phylo: Tools for parsing, constructing, and analyzing phylogenetic trees using Bio.Phylo.\nSequence motif analysis using Bio.motifs: Finding and analyzing sequence motifs with the Bio.motifs module.\nCluster analysis: Performing cluster analysis of biological data.\nGraphics including GenomeDiagram: Creating high-quality genomic diagrams and visualizations with GenomeDiagram.\nKEGG: Tools for interacting with the Kyoto Encyclopedia of Genes and Genomes (KEGG) using Bio.KEGG.\nBio.phenotype: Analyze phenotypic data: Analyzing phenotype-genotype associations with Bio.phenotype.\nPyBigWig: For reading and writing BigWig files, commonly used for storing dense, continuous data such as coverage tracks from sequencing experiments.\nHTSeq: A Python framework to process high-throughput sequencing data, especially for RNA-Seq counting and alignment.\nPysam: A Python module for reading, manipulating, and writing genomic data in SAM/BAM/VCF format."
  },
  {
    "objectID": "use.html#additional-tools-for-specialized-tasks",
    "href": "use.html#additional-tools-for-specialized-tasks",
    "title": "What I Use",
    "section": "Additional Tools for Specialized Tasks:",
    "text": "Additional Tools for Specialized Tasks:\n\nPyCaret: An open-source, low-code machine learning library that automates machine learning workflows for classification, regression, and clustering.\nTensorFlow: An end-to-end open-source platform for machine learning, especially for building and training deep learning models.\nKeras: A high-level API for building and training neural networks, running on top of TensorFlow for deep learning tasks."
  },
  {
    "objectID": "use.html#deep-learning-frameworks-and-libraries",
    "href": "use.html#deep-learning-frameworks-and-libraries",
    "title": "What I Use",
    "section": "Deep Learning Frameworks and Libraries:",
    "text": "Deep Learning Frameworks and Libraries:\n\nTensorFlow: An open-source framework for deep learning developed by Google. TensorFlow supports building and training neural networks, including tasks such as image classification, natural language processing, and more.\nKeras: A high-level neural networks API that runs on top of TensorFlow, simplifying the construction of deep learning models with minimal code.\nPyTorch: Developed by Facebook, PyTorch is a widely used deep learning library with a dynamic computational graph, making it easier for research and experimentation. It’s highly popular in the research community for developing new models.\nTheano: An older deep learning framework, originally developed to enable efficient computation of mathematical expressions, including neural networks. While it’s less commonly used now, Theano laid the foundation for many modern deep learning tools.\nMXNet: A scalable deep learning framework supporting both imperative and symbolic programming. It’s often used for training deep learning models on large datasets in distributed environments.\nChainer: A flexible deep learning framework known for its support of dynamic computation graphs, which makes it easier to implement complex models.\nFastai: A deep learning library that simplifies training models using PyTorch. It provides easy-to-use functions and pre-built models for tasks such as computer vision and natural language processing.\nDLib: A toolkit for developing machine learning and deep learning models, often used for computer vision tasks like face recognition and object detection."
  },
  {
    "objectID": "use.html#deep-learning-tools-and-applications",
    "href": "use.html#deep-learning-tools-and-applications",
    "title": "What I Use",
    "section": "Deep Learning Tools and Applications:",
    "text": "Deep Learning Tools and Applications:\n\nAutoencoders: Used for unsupervised learning and dimensionality reduction, autoencoders learn compressed representations of data. They are often used in tasks such as anomaly detection, data denoising, and generative models.\nConvolutional Neural Networks (CNNs): Deep learning models primarily used for tasks like image classification, object detection, and image segmentation. CNNs excel in extracting spatial features from images.\nRecurrent Neural Networks (RNNs): A type of neural network used for sequence data, such as time-series analysis, language modeling, and speech recognition. Variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) help in capturing long-term dependencies.\nGenerative Adversarial Networks (GANs): A powerful framework for generating new data that mimics real datasets. GANs are used for generating images, improving image resolution, and generating synthetic datasets.\nTransfer Learning: A technique where pre-trained deep learning models are fine-tuned for a specific task with fewer data. Popular models include VGG, ResNet, Inception, and BERT.\nReinforcement Learning: Deep learning models that learn by interacting with environments, often used in robotics, game AI, and autonomous systems.\nTransformers: A deep learning model architecture used for tasks like natural language processing (NLP). Models like BERT, GPT, and T5 are transformer-based models."
  },
  {
    "objectID": "use.html#additional-libraries-for-deep-learning",
    "href": "use.html#additional-libraries-for-deep-learning",
    "title": "What I Use",
    "section": "Additional Libraries for Deep Learning:",
    "text": "Additional Libraries for Deep Learning:\n\nHorovod: A distributed training framework for TensorFlow, PyTorch, and Keras, designed to enable efficient scaling of deep learning models across multiple GPUs.\nONNX (Open Neural Network Exchange): An open-source format that allows interoperability between different deep learning frameworks, enabling the movement of models between PyTorch, TensorFlow, MXNet, and other frameworks.\nOpenCV: A computer vision library that integrates with deep learning frameworks like TensorFlow and PyTorch for tasks like image recognition, object detection, and video analysis.\nCaffe: A deep learning framework that is particularly popular for computer vision tasks and is known for its speed in deploying deep learning models."
  },
  {
    "objectID": "use.html#use-cases-in-deep-learning",
    "href": "use.html#use-cases-in-deep-learning",
    "title": "What I Use",
    "section": "Use Cases in Deep Learning:",
    "text": "Use Cases in Deep Learning:\n\nImage Classification: Using CNNs and pre-trained models like VGG or ResNet to classify images into categories.\nObject Detection: Models like YOLO (You Only Look Once) and Faster R-CNN used for identifying and classifying objects within images.\nNatural Language Processing (NLP): Deep learning models for tasks like sentiment analysis, language translation, and chatbot development using LSTMs, GRUs, or transformers like BERT and GPT.\nSpeech Recognition: Deep learning models like DeepSpeech are widely used in converting speech to text and understanding spoken commands.\nMedical Imaging: Using deep learning for detecting and diagnosing diseases through X-rays, MRIs, and CT scans. CNNs and GANs are often applied for tasks like tumor detection and segmentation.\nAutonomous Vehicles: Deep learning models are used in self-driving cars for tasks like object detection, lane detection, and decision-making processes."
  },
  {
    "objectID": "use.html#key-bash-commands-and-tools-for-bioinformatics",
    "href": "use.html#key-bash-commands-and-tools-for-bioinformatics",
    "title": "What I Use",
    "section": "Key Bash Commands and Tools for Bioinformatics:",
    "text": "Key Bash Commands and Tools for Bioinformatics:\n\n1. File Management and Text Processing:\n\ngrep: For searching and filtering text data from large files (e.g., logs, sequence data).\nawk: Used for text processing and pattern scanning, often in sequence data files like FASTA, FASTQ, or SAM.\nsed: Stream editor for basic text transformations in bioinformatics pipelines.\ncut: For extracting columns from text files such as tabular data (e.g., VCF or expression matrices).\nsort and uniq: To sort and filter unique lines in large datasets.\nfind: Locating files and directories based on name, size, or modification time.\n\n\n\n2. Sequence Data Handling:\n\nfastqc: Quality control tool for high-throughput sequence data (FASTQ files).\nmultiqc: Aggregates results from multiple fastqc reports into a single summary report.\nbwa: Burrows-Wheeler Aligner for mapping low-divergent sequences against a large reference genome.\nsamtools: Utilities for processing SAM, BAM, and CRAM files (e.g., sorting, indexing, and converting formats).\nbcftools: Tools for variant calling and manipulating VCF files, used in variant calling workflows.\nbedtools: Suite of tools for performing a wide range of genomic operations on BED files.\ncutadapt: Used for trimming adapter sequences from reads in FASTQ files.\ntrimmomatic: Tool for trimming low-quality regions and adapters from high-throughput sequencing reads.\n\n\n\n3. Working with Single-Cell Data:\n\ncellranger: Command-line tool from 10x Genomics for preprocessing single-cell RNA-seq data (alignment, counting, clustering).\nSTAR: RNA-seq aligner widely used in both bulk and single-cell RNA-seq data processing.\nUMI-tools: For handling Unique Molecular Identifiers (UMIs) in single-cell RNA-seq workflows.\nfeatureCounts: For assigning aligned reads to genomic features like genes.\nbedGraphToBigWig: Converts bedGraph files to BigWig format for visualization in genome browsers.\n\n\n\n4. Data Compression and Decompression:\n\ngzip / gunzip: For compressing and decompressing sequence files (e.g., FASTQ, BAM).\ntar: To compress and extract multiple files or directories (often used with gzip).\nbgzip: Used for compressing VCF and other large text files, often combined with tabix for indexing.\nzcat / bzcat: For reading compressed files without decompressing them.\n\n\n\n5. Pipeline and Workflow Automation:\n\nNextflow: Workflow manager for reproducible and scalable bioinformatics pipelines, designed to work with cloud resources and clusters.\nSnakemake: Workflow management system for creating reproducible and scalable bioinformatics pipelines.\nGNU Parallel: Tool for executing jobs in parallel across multiple CPU cores, useful for speeding up pipelines.\n\n\n\n6. Genomic Data Handling:\n\nbcl2fastq: Converts raw Illumina BCL data files into FASTQ format for downstream analysis.\nkraken2: For taxonomic classification of metagenomic sequence data.\nprokka: For genome annotation of bacterial sequences.\npilon: Tool for improving genome assemblies by correcting sequence errors.\nunicycler: Hybrid assembly pipeline optimized for bacterial genomes.\nquast: For quality assessment of genome assemblies.\n\n\n\n7. Visualization:\n\nIGV: Command-line interface for the Integrative Genomics Viewer, useful for visualizing sequence data.\ncircos: Tool for generating circular genome plots, often used for comparative genomics.\nsamtools tview: A text-based alignment viewer for BAM files.\n\n\n\n8. Monitoring and Resource Management:\n\ntop: Monitor system resource usage (CPU, memory).\nhtop: A more interactive version of top for monitoring system performance.\ndf and du: For checking disk space and usage, ensuring enough storage is available for large sequencing datasets.\nscreen and tmux: Tools for managing long-running jobs on remote servers.\n\n\n\n9. Parallel and Distributed Computing:\n\nxargs: For parallelizing commands across multiple cores, especially for processing multiple files or datasets.\nsbatch / squeue (Slurm): For submitting and managing jobs on high-performance computing clusters.\n\n\n\n10. Software Installation:\n\nconda: Package and environment manager for installing bioinformatics tools and managing dependencies.\ndocker: Containerization platform for running bioinformatics tools in isolated environments.\nsingularity: Containerization platform commonly used in high-performance computing environments."
  },
  {
    "objectID": "use.html#example-bash-workflow-for-single-cell-rna-seq",
    "href": "use.html#example-bash-workflow-for-single-cell-rna-seq",
    "title": "What I Use",
    "section": "Example Bash Workflow for Single-Cell RNA-seq:",
    "text": "Example Bash Workflow for Single-Cell RNA-seq:\n# Step 1: Run FastQC for quality control\nfastqc *.fastq.gz\n\n# Step 2: Use Cell Ranger for single-cell RNA-seq data alignment and counting\ncellranger count --id=sample1 --transcriptome=refdata-gex-GRCh38-2020-A --fastqs=./fastq_files\n\n# Step 3: Perform read trimming with cutadapt\ncutadapt -a AGATCGGAAGAG -o trimmed_sample.fastq.gz sample.fastq.gz\n\n# Step 4: Align reads using STAR\nSTAR --runThreadN 8 --genomeDir ./genome --readFilesIn trimmed_sample.fastq.gz --outFileNamePrefix sample1\n\n# Step 5: Convert SAM to BAM, sort and index with Samtools\nsamtools view -bS sample1.sam | samtools sort -o sample1_sorted.bam\nsamtools index sample1_sorted.bam\n\n# Step 6: Use featureCounts for gene-level read counting\nfeatureCounts -a genes.gtf -o counts.txt sample1_sorted.bam"
  },
  {
    "objectID": "research.html#identification-of-potential-biomarkers-for-2022-mpox-virus-infection-a-transcriptomic-network-analysis-and-machine-learning-approach",
    "href": "research.html#identification-of-potential-biomarkers-for-2022-mpox-virus-infection-a-transcriptomic-network-analysis-and-machine-learning-approach",
    "title": "Research Projects",
    "section": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach",
    "text": "Identification of Potential Biomarkers for 2022 Mpox Virus Infection: A Transcriptomic Network Analysis and Machine Learning Approach\n\n\nAbstract\nMonkeypox virus (MPXV), a zoonotic pathogen, resurged in 2022 with the Clade IIb variant, raising global health concerns due to its unprecedented spread in non-endemic regions. Recent studies revealed that Clade IIb (2022 MPXV) is characterized by unique genomic mutations and epidemiological behaviors, suggesting variations in host-virus interactions. This study aimed to identify differentially expressed genes (DEGs) induced by the 2022 MPXV infection through comprehensive bioinformatics analyses of microarray and RNA-Seq datasets from post-infected cell lines across different MPXV clades. Gene expression network analyses pinpointed key DEGs, followed by candidate drug assessment using the Drug SIGnatures DataBase (DSigDB) and validation by multiple machine learning models. Comparative differential gene expression (DGE) analysis revealed 798 DEGs exclusive to the 2022 MPXV invasion in skin cell lines (keratinocytes and fibroblasts). Intriguingly, 13 key DEGs were identified across hubs and clusters, highlighting their aberrant expression in cell cycle regulation, immune responses, and cancer pathways. Biomarker screening via a Random Forest (RF) model (selected with PyCaret from multiple models) and validation through t-distributed stochastic neighbor embedding (t-SNE) algorithm, principal component analysis (PCA), and ROC curve analysis employing Logistic Regression and Random Forest identified 6 key DEGs (TXNRD1, CCNB1, BUB1, CDC20, BUB1B, and CCNA2) as promising biomarkers (AUC &gt; 0.7) for Clade IIb infection. This study anticipates that further investigation and clinical trials will catalyze the development of novel detection and therapeutic options to combat the 2022 MPXV infection in humans.\n\nKeywords: Mpox (monkeypox), 2022 MPXV (Clade IIb), DEGs, machine learning (ML) models, biomarker, candidate drugs"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research Projects",
    "section": "",
    "text": "::: panel-tabset # AI for Healthcare"
  },
  {
    "objectID": "research.html#development-of-a-disease-detection-tool-from-shotgun-metagenomic-data-utilizing-algorithms-based-on-k-mer-frequency-a-gut-microbiome-study.",
    "href": "research.html#development-of-a-disease-detection-tool-from-shotgun-metagenomic-data-utilizing-algorithms-based-on-k-mer-frequency-a-gut-microbiome-study.",
    "title": "Research Projects",
    "section": "Development of a disease detection tool from shotgun metagenomic data utilizing algorithms based on K-mer frequency: A gut microbiome study.",
    "text": "Development of a disease detection tool from shotgun metagenomic data utilizing algorithms based on K-mer frequency: A gut microbiome study.\n\n\nAbstract\nHuman gut microbiome composition can be influenced by various factors, including dietary habits, lifestyle, ethnicity, geographic location, and many others. The gut microbiome plays a critical role in the development of various Non-Communicable Diseases (NCDs). Prediction of NCDs can be achieved by analyzing the gut microbiota. Using data from metagenomic sequencing in a multi-layered approach, it is possible to forecast a patient’s current state of health. Deep learning, a powerful form of machine learning, has been successfully applied in numerous biological fields by researchers. In this study, we developed a disease detection tool called the Metagenomic Disease-specific Classifier (MetaDSC), which employs algorithms based on the frequency of k-mers. MetaDSC was trained using whole genome shotgun sequencing data from different health conditions. When it comes to distinguishing between sick and healthy samples, MetaDSC achieves an accuracy rate that averages up to 92 percent. MetaDSC outperformed several existing tools in this regard. Specifically, MetaDSC can recognize Healthy, Type 2 Diabetes Mellitus (T2D), Non-Alcoholic Fatty Liver Disease (NAFLD), Inflammatory Bowel Disease (IBD), and Obesity from metagenomics sequences with 92.8%, 92.8%, 88.22%, 100%, and 90.0% accuracy respectively. MetaDSC has the potential to be utilized for accurate diagnosis across a wide variety of diseases, provided that new datasets are consistently incorporated into the tool.\n\nKeywords: Gut Microbiome, Non-Communicable Diseases, Metagenomic, Machine learning , Deep learning, k-mer, MetaDSC."
  },
  {
    "objectID": "research.html#virustaxo-taxonomic-classification-of-viruses-from-the-genome-sequence-using-k-mer-enrichment",
    "href": "research.html#virustaxo-taxonomic-classification-of-viruses-from-the-genome-sequence-using-k-mer-enrichment",
    "title": "Research Projects",
    "section": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k-mer enrichment",
    "text": "VirusTaxo: Taxonomic classification of viruses from the genome sequence using k-mer enrichment\n\n\nAbstract\nClassification of viruses into their taxonomic ranks (e.g., order, family, and genus) provides a framework to organize the vast population of viruses. Next-generation metagenomic sequencing technologies have led to a rapid increase in viral sequencing data, necessitating the development of bioinformatics tools to analyze viral taxonomy. While many metagenomic taxonomy classifiers have been created to study microbiomes, classifying the diverse range of virus sequences remains a significant challenge. There is a growing demand for specialized methods optimized for the classification of viral sequences into their respective taxa. To address this, we developed VirusTaxo, a tool for the taxonomic classification of viruses from metagenomic sequences, utilizing a diverse set of viral genera (e.g., 402 DNA and 280 RNA genera). VirusTaxo achieves an average accuracy of 93% at the genus level in predicting DNA and RNA viruses. The tool outperforms existing virus taxonomic classifiers by assigning taxonomy to a larger fraction of metagenomic contigs compared to other methods. Benchmarking VirusTaxo on a collection of SARS-CoV-2 sequencing libraries and metavirome datasets demonstrates that it can accurately characterize viral taxonomy from highly diverse contigs, providing reliable decisions on viral taxonomy.\n\nKeywords: Virus, Taxonomy, Hierarchical classification, k-mer, Genome"
  },
  {
    "objectID": "research.html#computational-framework-to-interpret-chest-x-rays-and-diagnose-pneumonia",
    "href": "research.html#computational-framework-to-interpret-chest-x-rays-and-diagnose-pneumonia",
    "title": "Research Projects",
    "section": "Computational framework to interpret chest X-rays and diagnose pneumonia",
    "text": "Computational framework to interpret chest X-rays and diagnose pneumonia\n\n\nAbstract\nIn low- and middle-income countries, pneumonia remains the leading cause of illness and death in children under 5 years. The recommended diagnostic tool for pediatric pneumonia is chest X-ray image interpretation, which is challenging to standardize and requires trained clinicians or radiologists. Current automated computational tools predominantly focus on assessing adult pneumonia and have been trained on images evaluated by a single specialist. This study aims to develop a computational tool using a deep learning approach to diagnose pediatric pneumonia from X-ray images assessed by multiple specialists trained by the WHO expert X-ray image reading panel.\n\nWorkingg to the previous extended project"
  },
  {
    "objectID": "research.html#parv4-detection-in-children-with-suspected-meningitis-is-associated-with-high-mortality.",
    "href": "research.html#parv4-detection-in-children-with-suspected-meningitis-is-associated-with-high-mortality.",
    "title": "Research Projects",
    "section": "Parv4 detection in children with suspected meningitis is associated with high mortality.",
    "text": "Parv4 detection in children with suspected meningitis is associated with high mortality.\n\n\nAbstract\n\nKeyword: Metagenomics analysis—consensus sequence generated using CZ ID, mapping, MAFFT, RAxM\n\n\nProcessing to submit"
  },
  {
    "objectID": "research.html#a-global-pediatric-cell-atlas-of-nasal-and-oral-mucosa-1",
    "href": "research.html#a-global-pediatric-cell-atlas-of-nasal-and-oral-mucosa-1",
    "title": "Research Projects",
    "section": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa",
    "text": "A Global Pediatric Cell Atlas of Nasal and Oral Mucosa\n\n\nAbstract"
  },
  {
    "objectID": "research.html#pcv-effectiveness-study-via-singlecell-analysis-of-pregnant-women.",
    "href": "research.html#pcv-effectiveness-study-via-singlecell-analysis-of-pregnant-women.",
    "title": "Research Projects",
    "section": "PCV effectiveness study via single‑cell analysis of pregnant women.",
    "text": "PCV effectiveness study via single‑cell analysis of pregnant women.\n\nAbstract\nA single-cell analytics platform to track the immune responses of babies before and after receiving a pneumococcal conjugate vaccine to determine the impact of various factors, including nutritional status and seasonality, on vaccine efficacy. Vaccines have successfully reduced childhood morbidity and mortality; however, their efficacy can be influenced by host factors and extrinsic factors through unknown cellular mechanisms. They will recruit 50 newborns in a rural district north of Dhaka and collect blood and nasopharyngeal swabs before, during and after a routine vaccination series. They will extract peripheral blood mononuclear cells and use them to perform single-cell RNA sequencing to identify cell subtypes and link differential vaccine responses to factors including gestational age, nutritional status and sex."
  },
  {
    "objectID": "research.html#global-pneumococcal-sequencing-project.",
    "href": "research.html#global-pneumococcal-sequencing-project.",
    "title": "Research Projects",
    "section": "Global Pneumococcal Sequencing Project.",
    "text": "Global Pneumococcal Sequencing Project.\n\n\nAbstract"
  },
  {
    "objectID": "research.html#building-a-singlecell-atlas-of-the-nasopharyngeal-mucosa-to-investigate-sarscov2-infection.",
    "href": "research.html#building-a-singlecell-atlas-of-the-nasopharyngeal-mucosa-to-investigate-sarscov2-infection.",
    "title": "Research Projects",
    "section": "Building a single‑cell atlas of the nasopharyngeal mucosa to investigate SARS‑CoV‑2 infection.",
    "text": "Building a single‑cell atlas of the nasopharyngeal mucosa to investigate SARS‑CoV‑2 infection.\n\nAbstract\nSteps in Building the Atlas: - Dataset selection: Tissues related to the respiratory system, focusing on the nasopharynx. - Integration of datasets from studies (Yoshida, Ziegler, Ren), using Harmony to combine over 245,000 cells. - Clustering analysis of integrated dataset to define distinct cell types in the nasopharyngeal tissue. - Cell-type clusters identified with the integrated dataset and visualization of immune cell subtypes. - Assess expression of SARS‑CoV‑2 in the nasopharynx. - Reannotate cell types to identify novel or other subtypes. - Analyze the dataset with demographic details like age, SARS-CoV-2 status, and more. - Compare findings from the Bangladesh cohort with the global integrated dataset."
  },
  {
    "objectID": "skill.html#key-bash-commands-and-tools-for-bioinformatics",
    "href": "skill.html#key-bash-commands-and-tools-for-bioinformatics",
    "title": "What I Use",
    "section": "Key Bash Commands and Tools for Bioinformatics:",
    "text": "Key Bash Commands and Tools for Bioinformatics:\n\n1. File Management and Text Processing:\n\ngrep: For searching and filtering text data from large files (e.g., logs, sequence data).\nawk: Used for text processing and pattern scanning, often in sequence data files like FASTA, FASTQ, or SAM.\nsed: Stream editor for basic text transformations in bioinformatics pipelines.\ncut: For extracting columns from text files such as tabular data (e.g., VCF or expression matrices).\nsort and uniq: To sort and filter unique lines in large datasets.\nfind: Locating files and directories based on name, size, or modification time.\n\n\n\n2. Sequence Data Handling:\n\nfastqc: Quality control tool for high-throughput sequence data (FASTQ files).\nmultiqc: Aggregates results from multiple fastqc reports into a single summary report.\nbwa: Burrows-Wheeler Aligner for mapping low-divergent sequences against a large reference genome.\nsamtools: Utilities for processing SAM, BAM, and CRAM files (e.g., sorting, indexing, and converting formats).\nbcftools: Tools for variant calling and manipulating VCF files, used in variant calling workflows.\nbedtools: Suite of tools for performing a wide range of genomic operations on BED files.\ncutadapt: Used for trimming adapter sequences from reads in FASTQ files.\ntrimmomatic: Tool for trimming low-quality regions and adapters from high-throughput sequencing reads.\n\n\n\n3. Data Compression and Decompression:\n\ngzip / gunzip: For compressing and decompressing sequence files (e.g., FASTQ, BAM).\ntar: To compress and extract multiple files or directories (often used with gzip).\nbgzip: Used for compressing VCF and other large text files, often combined with tabix for indexing.\nzcat / bzcat: For reading compressed files without decompressing them.\n\n\n\n4. Pipeline and Workflow Automation:\n\nNextflow: Workflow manager for reproducible and scalable bioinformatics pipelines, designed to work with cloud resources and clusters.\n\n\n\n5. Genomic Data Handling:\n\nbcl2fastq: Converts raw Illumina BCL data files into FASTQ format for downstream analysis.\nkraken2: For taxonomic classification of metagenomic sequence data.\nprokka: For genome annotation of bacterial sequences.\npilon: Tool for improving genome assemblies by correcting sequence errors.\nunicycler: Hybrid assembly pipeline optimized for bacterial genomes.\nquast: For quality assessment of genome assemblies.\n\n\n\n6. Visualization:\n\nIGV: Command-line interface for the Integrative Genomics Viewer, useful for visualizing sequence data.\nsamtools tview: A text-based alignment viewer for BAM files.\n\n\n\n7. Monitoring and Resource Management:\n\ntop: Monitor system resource usage (CPU, memory).\nhtop: A more interactive version of top for monitoring system performance.\ndf and du: For checking disk space and usage, ensuring enough storage is available for large sequencing datasets.\n\n\n\n8. Software Installation:\n\nconda: Package and environment manager for installing bioinformatics tools and managing dependencies.\ndocker: Containerization platform for running bioinformatics tools in isolated environments."
  },
  {
    "objectID": "skill.html#example-bash-workflow-for-single-cell-rna-seq",
    "href": "skill.html#example-bash-workflow-for-single-cell-rna-seq",
    "title": "What I Use",
    "section": "Example Bash Workflow for Single-Cell RNA-seq:",
    "text": "Example Bash Workflow for Single-Cell RNA-seq:\n# Step 1: Run FastQC for quality control\nfastqc *.fastq.gz\n\n# Step 2: Use Cell Ranger for single-cell RNA-seq data alignment and counting\ncellranger count --id=sample1 --transcriptome=refdata-gex-GRCh38-2020-A --fastqs=./fastq_files\n\n# Step 3: Perform read trimming with cutadapt\ncutadapt -a AGATCGGAAGAG -o trimmed_sample.fastq.gz sample.fastq.gz\n\n# Step 4: Align reads using STAR\nSTAR --runThreadN 8 --genomeDir ./genome --readFilesIn trimmed_sample.fastq.gz --outFileNamePrefix sample1\n\n# Step 5: Convert SAM to BAM, sort and index with Samtools\nsamtools view -bS sample1.sam | samtools sort -o sample1_sorted.bam\nsamtools index sample1_sorted.bam\n\n# Step 6: Use featureCounts for gene-level read counting\nfeatureCounts -a genes.gtf -o counts.txt sample1_sorted.bam"
  },
  {
    "objectID": "skill.html#python-libraries-for-data-analysis-and-scientific-computing",
    "href": "skill.html#python-libraries-for-data-analysis-and-scientific-computing",
    "title": "What I Use",
    "section": "Python Libraries for Data Analysis and Scientific Computing:",
    "text": "Python Libraries for Data Analysis and Scientific Computing:\n\nNumPy: Used for numerical and scientific computing, providing support for arrays and matrices.\nPandas: For data manipulation and analysis, offering data structures and operations for numerical tables and time series.\nMatplotlib: A plotting library for creating static, animated, and interactive visualizations in Python.\nSeaborn: Built on top of Matplotlib, Seaborn provides a high-level interface for drawing attractive and informative statistical graphics.\nPlotly: An interactive plotting library that enables the creation of web-based, interactive plots and dashboards.\nresearchpy: Simplifies statistical tests and summarizing data in Python, particularly for social science research.\nDask: Designed for parallel computing, it is used for processing big data and creating scalable data pipelines."
  },
  {
    "objectID": "skill.html#machine-learning-and-image-processing",
    "href": "skill.html#machine-learning-and-image-processing",
    "title": "What I Use",
    "section": "Machine Learning and Image Processing:",
    "text": "Machine Learning and Image Processing:\n\nscikit-learn: A powerful library for machine learning and data mining, supporting a wide range of supervised and unsupervised algorithms.\nscikit-image: For image processing tasks, especially used in life science applications like segmentation, filtering, and transformations."
  },
  {
    "objectID": "skill.html#bioinformatics-tools-in-python",
    "href": "skill.html#bioinformatics-tools-in-python",
    "title": "What I Use",
    "section": "Bioinformatics Tools in Python:",
    "text": "Bioinformatics Tools in Python:\n\nBiopython: A comprehensive library for biological computation, supporting sequence analysis, file parsing (FASTA, GenBank), phylogenetics, and more. Widely used in genomics, protein analysis, and molecular biology. Table of contents with Tools:\n\nSequence objects: Handling biological sequences using the Seq object from Biopython.\nSequence annotation objects: Annotating sequences with SeqRecord and Feature objects.\nSequence Input/Output: Tools like SeqIO and AlignIO for reading and writing sequence file formats (e.g., FASTA, GenBank).\nSequence alignments: Working with sequence alignment objects using AlignIO.\nPairwise sequence alignment: Performing pairwise sequence alignment with Bio.pairwise2.\nMultiple Sequence Alignment objects: Handling and manipulating multiple sequence alignments using MultipleSeqAlignment and AlignIO.\nPairwise alignments using pairwise2: Detailed use of the pairwise2 module for flexible sequence alignment.\nBLAST : Accessing NCBI BLAST services with the NCBIXML and NCBIWWW modules to parse BLAST output.\nAccessing NCBI’s Entrez databases: Using Bio.Entrez for accessing and retrieving data from NCBI’s Entrez databases (e.g., PubMed, GenBank).\nSwiss-Prot and ExPASy: Tools for accessing Swiss-Prot (UniProt) and ExPASy databases for protein sequences.\nGoing 3D: The PDB module: Parsing and analyzing 3D protein structures with Bio.PDB.\nBio.PopGen: Population genetics: Tools for analyzing population genetics using the Bio.PopGen module.\nPhylogenetics with Bio.Phylo: Tools for parsing, constructing, and analyzing phylogenetic trees using Bio.Phylo.\nSequence motif analysis using Bio.motifs: Finding and analyzing sequence motifs with the Bio.motifs module.\nCluster analysis: Performing cluster analysis of biological data.\nGraphics including GenomeDiagram: Creating high-quality genomic diagrams and visualizations with GenomeDiagram.\nKEGG: Tools for interacting with the Kyoto Encyclopedia of Genes and Genomes (KEGG) using Bio.KEGG.\nBio.phenotype: Analyze phenotypic data: Analyzing phenotype-genotype associations with Bio.phenotype.\nPyBigWig: For reading and writing BigWig files, commonly used for storing dense, continuous data such as coverage tracks from sequencing experiments.\nHTSeq: A Python framework to process high-throughput sequencing data, especially for RNA-Seq counting and alignment.\nPysam: A Python module for reading, manipulating, and writing genomic data in SAM/BAM/VCF format."
  },
  {
    "objectID": "skill.html#additional-tools-for-specialized-tasks",
    "href": "skill.html#additional-tools-for-specialized-tasks",
    "title": "What I Use",
    "section": "Additional Tools for Specialized Tasks:",
    "text": "Additional Tools for Specialized Tasks:\n\nPyCaret: An open-source, low-code machine learning library that automates machine learning workflows for classification, regression, and clustering.\nTensorFlow: An end-to-end open-source platform for machine learning, especially for building and training deep learning models.\nKeras: A high-level API for building and training neural networks, running on top of TensorFlow for deep learning tasks."
  },
  {
    "objectID": "skill.html#data-wrangling",
    "href": "skill.html#data-wrangling",
    "title": "What I Use",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nreadxl for importing data into R.\ndplyr, tidyr, and others from the tidyverse for data preparation."
  },
  {
    "objectID": "skill.html#data-visualization",
    "href": "skill.html#data-visualization",
    "title": "What I Use",
    "section": "Data Visualization",
    "text": "Data Visualization\n\nggplot2 for the majority of the graphics, together with the hrbrtheme for styling.\npatchwork to combine graphics.\nggraph and igraph for most network-related graphics.\nplotly and other HTML widgets for interactive graphics.\nRColorBrewer, viridis, and colormap for color control in charts.\nggrepel and other ggplot2 extensions for simplifying plotting tasks.\nheatmaply for heatmaps."
  },
  {
    "objectID": "skill.html#reproducible-research",
    "href": "skill.html#reproducible-research",
    "title": "What I Use",
    "section": "Reproducible Research",
    "text": "Reproducible Research\n\nR Markdown to produce statistical reports.\nQuarto to build websites for courses and more."
  },
  {
    "objectID": "skill.html#statistical-modeling",
    "href": "skill.html#statistical-modeling",
    "title": "What I Use",
    "section": "Statistical Modeling",
    "text": "Statistical Modeling\nStatic modeling in SPSS entails building and evaluating models that represent relationships within a dataset at a particular point in time. Common methods include ANOVA, descriptive statistics, and regression analysis. Data preparation, variable selection, model construction, and evaluation are part of the process. Coefficients, p-values, and R-squared help interpret output to determine the significance and strength of relationships. Applications include business analytics, social sciences, and market research."
  },
  {
    "objectID": "skill.html#workflow-management-and-pipeline-development",
    "href": "skill.html#workflow-management-and-pipeline-development",
    "title": "What I Use",
    "section": "Workflow Management and Pipeline Development:",
    "text": "Workflow Management and Pipeline Development:\n\nNextflow is used to create reproducible and scalable data analysis pipelines.\nIntegrates with software such as Docker, Singularity, and Conda for environment management.\nIdeal for processing large-scale data, especially in genomics and bioinformatics.\nCan manage complex workflows involving multiple tools like FastQC, STAR, HISAT2, DESeq2, and more."
  },
  {
    "objectID": "skill.html#key-tools-in-nextflow-pipelines",
    "href": "skill.html#key-tools-in-nextflow-pipelines",
    "title": "What I Use",
    "section": "Key Tools in Nextflow Pipelines:",
    "text": "Key Tools in Nextflow Pipelines:\n\nFastQC: For quality control of raw sequence data.\nTrimmomatic: For trimming low-quality reads and adapters.\nSTAR/HISAT2: For read alignment to reference genomes.\nDESeq2/edgeR: For differential expression analysis.\nMultiQC: To aggregate results across multiple samples for easy interpretation.\nARIBA: Rapid antimicrobial resistance genotyping directly from sequencing reads.\nBCFtools: For variant calling and manipulating VCF/BCF files.\nSAMtools: For handling BAM/SAM files.\nBWA: For sequence alignment.\nDocker Images (StaPH-B): Pre-built containerized environments with bioinformatics tools like ARIBA, BCFtools, Kraken 2, and more.\nfastp: Ultra-fast all-in-one FASTQ preprocessor.\nKraken 2: For metagenomic analysis and taxonomy assignment.\nmlst: For multi-locus sequence typing.\nPopPUNK: Bacterial genomic epidemiology analysis tool.\nQUAST: For genome assembly evaluation.\nSeroBA: High-throughput serotyping of Streptococcus pneumoniae.\nUnicycler: Bacterial genome assembly tool.\nSPN-PBP-AMR: Predicts penicillin resistance in Streptococcus pneumoniae."
  },
  {
    "objectID": "skill.html#example-use-cases",
    "href": "skill.html#example-use-cases",
    "title": "What I Use",
    "section": "Example Use Cases:",
    "text": "Example Use Cases:\n\nGPS Pipeline\nKPN pipeline\nRNA-Seq pipelines\nWhole Genome Sequencing (WGS)\nSingle-cell RNA-Seq analysis\nMetagenomics analysis"
  },
  {
    "objectID": "skill.html#deep-learning-frameworks-and-libraries",
    "href": "skill.html#deep-learning-frameworks-and-libraries",
    "title": "What I Use",
    "section": "Deep Learning Frameworks and Libraries:",
    "text": "Deep Learning Frameworks and Libraries:\n\nTensorFlow: An open-source framework for deep learning developed by Google. TensorFlow supports building and training neural networks, including tasks such as image classification, natural language processing, and more.\nKeras: A high-level neural networks API that runs on top of TensorFlow, simplifying the construction of deep learning models with minimal code.\nPyTorch: Developed by Facebook, PyTorch is a widely used deep learning library with a dynamic computational graph, making it easier for research and experimentation. It’s highly popular in the research community for developing new models.\nTheano: An older deep learning framework, originally developed to enable efficient computation of mathematical expressions, including neural networks. While it’s less commonly used now, Theano laid the foundation for many modern deep learning tools.\nMXNet: A scalable deep learning framework supporting both imperative and symbolic programming. It’s often used for training deep learning models on large datasets in distributed environments.\nChainer: A flexible deep learning framework known for its support of dynamic computation graphs, which makes it easier to implement complex models.\nFastai: A deep learning library that simplifies training models using PyTorch. It provides easy-to-use functions and pre-built models for tasks such as computer vision and natural language processing.\nDLib: A toolkit for developing machine learning and deep learning models, often used for computer vision tasks like face recognition and object detection."
  },
  {
    "objectID": "skill.html#deep-learning-tools-and-applications",
    "href": "skill.html#deep-learning-tools-and-applications",
    "title": "What I Use",
    "section": "Deep Learning Tools and Applications:",
    "text": "Deep Learning Tools and Applications:\n\nAutoencoders: Used for unsupervised learning and dimensionality reduction, autoencoders learn compressed representations of data. They are often used in tasks such as anomaly detection, data denoising, and generative models.\nConvolutional Neural Networks (CNNs): Deep learning models primarily used for tasks like image classification, object detection, and image segmentation. CNNs excel in extracting spatial features from images.\nRecurrent Neural Networks (RNNs): A type of neural network used for sequence data, such as time-series analysis, language modeling, and speech recognition. Variants like LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit) help in capturing long-term dependencies.\nGenerative Adversarial Networks (GANs): A powerful framework for generating new data that mimics real datasets. GANs are used for generating images, improving image resolution, and generating synthetic datasets.\nTransfer Learning: A technique where pre-trained deep learning models are fine-tuned for a specific task with fewer data. Popular models include VGG, ResNet, Inception, and BERT.\nReinforcement Learning: Deep learning models that learn by interacting with environments, often used in robotics, game AI, and autonomous systems.\nTransformers: A deep learning model architecture used for tasks like natural language processing (NLP). Models like BERT, GPT, and T5 are transformer-based models."
  },
  {
    "objectID": "skill.html#additional-libraries-for-deep-learning",
    "href": "skill.html#additional-libraries-for-deep-learning",
    "title": "What I Use",
    "section": "Additional Libraries for Deep Learning:",
    "text": "Additional Libraries for Deep Learning:\n\nHorovod: A distributed training framework for TensorFlow, PyTorch, and Keras, designed to enable efficient scaling of deep learning models across multiple GPUs.\nONNX (Open Neural Network Exchange): An open-source format that allows interoperability between different deep learning frameworks, enabling the movement of models between PyTorch, TensorFlow, MXNet, and other frameworks.\nOpenCV: A computer vision library that integrates with deep learning frameworks like TensorFlow and PyTorch for tasks like image recognition, object detection, and video analysis.\nCaffe: A deep learning framework that is particularly popular for computer vision tasks and is known for its speed in deploying deep learning models."
  },
  {
    "objectID": "skill.html#use-cases-in-deep-learning",
    "href": "skill.html#use-cases-in-deep-learning",
    "title": "What I Use",
    "section": "Use Cases in Deep Learning:",
    "text": "Use Cases in Deep Learning:\n\nImage Classification: Using CNNs and pre-trained models like VGG or ResNet to classify images into categories.\nObject Detection: Models like YOLO (You Only Look Once) and Faster R-CNN used for identifying and classifying objects within images.\nNatural Language Processing (NLP): Deep learning models for tasks like sentiment analysis, language translation, and chatbot development using LSTMs, GRUs, or transformers like BERT and GPT.\nSpeech Recognition: Deep learning models like DeepSpeech are widely used in converting speech to text and understanding spoken commands.\nMedical Imaging: Using deep learning for detecting and diagnosing diseases through X-rays, MRIs, and CT scans. CNNs and GANs are often applied for tasks like tumor detection and segmentation.\nAutonomous Vehicles: Deep learning models are used in self-driving cars for tasks like object detection, lane detection, and decision-making processes."
  },
  {
    "objectID": "skill.html#data-sources",
    "href": "skill.html#data-sources",
    "title": "What I Use",
    "section": "Data Sources",
    "text": "Data Sources\n\nThe Cancer Genome Atlas (TCGA): Comprehensive multi-dimensional cancer genomics data.\nInternational Cancer Genome Consortium (ICGC): Genomic and clinical data from cancer projects worldwide.\nGene Expression Omnibus (GEO): Public repository for gene expression data, including cancer datasets.\nEuropean Genome-phenome Archive (EGA): Repository for secure storage of human genetic and phenotypic data.\nNational Cancer Institute (NCI) Genomic Data Commons (GDC): Open-access data portal for cancer genomics datasets."
  },
  {
    "objectID": "skill.html#analysis-tools",
    "href": "skill.html#analysis-tools",
    "title": "What I Use",
    "section": "Analysis Tools",
    "text": "Analysis Tools\n\nGEPIA2: Web tool for analyzing gene expression data in cancer.\nTIMER2.0: Tool for immune infiltrate analysis in cancer.\nUALCAN: Platform for exploring cancer transcriptome data.\ncBioPortal for Cancer Genomics: Collection of cancer genomics datasets for exploration and visualization.\nGREIN: Interactive platform for exploring and analyzing GEO RNA-seq data.\nOncoLnc: Resource for survival analysis and gene expression correlation.\nUCSC Cancer Genomics Browser: Browser for cancer genomics data.\nONCOMINE: Platform for cancer transcriptomic data analysis."
  },
  {
    "objectID": "skill.html#r-packages",
    "href": "skill.html#r-packages",
    "title": "What I Use",
    "section": "R Packages",
    "text": "R Packages\n\nTCGAbiolinks: R package for integrative analysis with GDC data.\nmaftools: R package for summarizing, analyzing, and visualizing MAF files.\nSummarizedExperiment: Container for genomic data assays.\nMutationalPatterns: Tool for genome-wide analysis of mutational processes.\n\n\nSingle Cell RNA Sequence Analysis\n\n1. Data Pre-processing:\n\nScanpy: Preprocessing single-cell RNA-seq data with functions like pp.filter_genes, pp.normalize_total, and pp.log1p for normalization, scaling, and filtering.\nSeurat: Preprocessing with functions such as NormalizeData, ScaleData, and FindVariableFeatures to prepare the data for downstream analysis.\nCell Ranger: Primary analysis pipeline from 10x Genomics for generating FASTQ, aligning reads, and creating expression matrices.\n\n\n\n2. Clustering and Cell Annotation:\n\nScanpy: Clustering with algorithms like Louvain (tl.louvain) and Leiden (tl.leiden). Annotation with marker genes using tl.rank_genes_groups.\nSeurat: Clustering with FindClusters and RunPCA for dimensionality reduction and visualization. Annotation via FindAllMarkers to identify cell-type-specific markers.\nHarmony: Batch correction and integration during clustering to harmonize multiple datasets.\n\n\n\n3. Integration and Batch Correction:\n\nScanpy: Integration tools like tl.bbknn for batch-correction using nearest neighbors and external.pp.harmony_integrate for Harmony integration.\nSeurat: Integration tools such as Canonical Correlation Analysis (FindIntegrationAnchors) and Reciprocal PCA (IntegrateData) for batch correction across datasets.\nHarmony: A flexible tool for integration and batch effect correction during analysis.\nscVI: Scalable Variational Inference for integrating and analyzing multi-modal single-cell datasets.\n\n\n\n4. Cell-Cell Communication:\n\nCellPhoneDB: A Python tool for inferring cell-cell communication based on ligand-receptor interactions.\nNATMI: For predicting ligand-receptor interactions between cell populations.\niTALK: R package for analyzing and visualizing cell-cell communication using scRNA-seq data.\nCellChat: R package for inferring and visualizing intercellular communication networks.\nSingleCellSignalR: An R package for identifying cell-cell communication networks based on receptor-ligand interaction analysis.\n\n\n\n5. BCR Background and 10x Analysis:\n\nCell Ranger: For reconstructing BCR and TCR sequences from 10x Genomics single-cell data.\nVDJtools: Analyzing BCR/TCR repertoires from single-cell RNA-seq data.\nscRepertoire: R package for analyzing TCR/BCR sequences in single-cell datasets, providing clonotype tracking and diversity analysis.\n\n\n\n6. Trajectory Inference:\n\nMonocle3: R package for performing pseudotime analysis and reconstructing cell trajectories.\nSlingshot: R package for lineage inference and trajectory reconstruction from single-cell RNA-seq data.\nScanpy: Trajectory inference using methods like Palantir (tl.palantir) and pseudotime analysis with tl.dpt.\nPAGA: Partition-based graph abstraction for inferring lineage relationships and transitions between cell states in Scanpy.\nVelocyto: A tool for estimating RNA velocity in single cells, predicting the future states of cells in trajectory analysis.\ndynverse: A collection of methods and packages for trajectory inference, integrated with both Seurat and Scanpy workflows.\n\n\n\n7. Differential Abundance:\n\nDAseq: R package for differential abundance analysis of single-cell datasets, identifying significant differences in cell populations.\nMilo: For testing differential abundance of cell neighborhoods across conditions.\nedgeR: For differential expression analysis in scRNA-seq data.\nDESeq2: R package for differential expression analysis in bulk RNA-seq and scRNA-seq data.\n\n\n\n8. Multiomic scATAC:\n\nSignac: An R package built on top of Seurat for integrating scRNA-seq and scATAC-seq data.\nArchR: A comprehensive tool for single-cell ATAC-seq data analysis, including peak calling, dimensionality reduction, and clustering.\nCicero: Tool for inferring co-accessibility networks from scATAC-seq data.\nSnapATAC: For scATAC-seq clustering, visualization, and integration with scRNA-seq datasets.\nscVI: Probabilistic modeling for multi-modal single-cell data, supporting scRNA-seq and scATAC-seq integration.\n\n\n\nResearch Fellow: Developed ML-based pipeline for shotgun sequencing, 16S rRNA sequencing using QIIME2, RNA-seq using DESeq2, stats, ggplot, corrplot, pheatmap, EDASeq, gProfileR."
  },
  {
    "objectID": "research.html#building-a-singlecell-atlas-of-the-nasopharyngeal-mucosa-to-investigate-sarscov2-infection",
    "href": "research.html#building-a-singlecell-atlas-of-the-nasopharyngeal-mucosa-to-investigate-sarscov2-infection",
    "title": "Research Projects",
    "section": "Building a Single‑Cell Atlas of the Nasopharyngeal Mucosa to Investigate SARS‑CoV‑2 Infection",
    "text": "Building a Single‑Cell Atlas of the Nasopharyngeal Mucosa to Investigate SARS‑CoV‑2 Infection\n\nAbstract\nSteps in Building the Atlas:  - Dataset Selection: Tissues related to the respiratory system, focusing on the nasopharynx. - Integration of Datasets: From studies by Yoshida, Ziegler, and Ren, using Harmony to combine over 245,000 cells. - Clustering Analysis: Of the integrated dataset to define distinct cell types in the nasopharyngeal tissue. - Identification of Cell-Type Clusters: With the integrated dataset and visualization of immune cell subtypes. - SARS‑CoV‑2 Expression Assessment: In the nasopharynx.  - Reannotation of Cell Types: To identify novel or other subtypes.  - Demographic Analysis: Analyze the dataset with demographic details like age, SARS-CoV-2 status, and more. - Comparative Analysis: Compare findings from the Bangladesh cohort with the global integrated dataset."
  },
  {
    "objectID": "skill.html#genomics",
    "href": "skill.html#genomics",
    "title": "What I Use",
    "section": "Genomics",
    "text": "Genomics\n\nIllumina: , Sample sheet preparation, Converting (BCL2fastq), Quality checking (FastQC, MultiQC, Quast), Quality control (Trimmomatic), Assembly (Unicycler, Spades, Megahit).\nAnnotation: Kraken2, Prokka, Seroba, AMRFinderPlus, Abricate, SRST, MLST, Snippy, Mafft, fasta2phylip, Raxml-ng, Poppunk, PlasmidFinder, ResFinder, BLAST, Pharokka.\n\n\nData Sources\n\nNCBI\nThe Cancer Genome Atlas (TCGA): Comprehensive multi-dimensional cancer genomics data.\nGene Expression Omnibus (GEO): Public repository for gene expression data, including cancer datasets.\nCELLXGENE"
  },
  {
    "objectID": "skill.html#single-cell-rna-sequence-analysis",
    "href": "skill.html#single-cell-rna-sequence-analysis",
    "title": "What I Use",
    "section": "Single Cell RNA Sequence Analysis",
    "text": "Single Cell RNA Sequence Analysis\n\n1. Data Pre-processing:\n\nScanpy: Preprocessing single-cell RNA-seq data with functions like pp.filter_genes, pp.normalize_total, and pp.log1p for normalization, scaling, and filtering.\nSeurat: Preprocessing with functions such as NormalizeData, ScaleData, and FindVariableFeatures to prepare the data for downstream analysis.\nCell Ranger: Primary analysis pipeline from 10x Genomics for generating FASTQ, aligning reads, and creating expression matrices.\n\n\n\n2. Clustering and Cell Annotation:\n\nScanpy: Clustering with algorithms like Louvain (tl.louvain) and Leiden (tl.leiden). Annotation with marker genes using tl.rank_genes_groups.\nSeurat: Clustering with FindClusters and RunPCA for dimensionality reduction and visualization. Annotation via FindAllMarkers to identify cell-type-specific markers.\nHarmony: Batch correction and integration during clustering to harmonize multiple datasets.\n\n\n\n3. Integration and Batch Correction:\n\nScanpy: Integration tools like tl.bbknn for batch-correction using nearest neighbors and external.pp.harmony_integrate for Harmony integration.\nSeurat: Integration tools such as Canonical Correlation Analysis (FindIntegrationAnchors) and Reciprocal PCA (IntegrateData) for batch correction across datasets.\nHarmony: A flexible tool for integration and batch effect correction during analysis.\nscVI: Scalable Variational Inference for integrating and analyzing multi-modal single-cell datasets.\n\n\n\n4. Cell-Cell Communication:\n\nCellPhoneDB: A Python tool for inferring cell-cell communication based on ligand-receptor interactions.\nNATMI: For predicting ligand-receptor interactions between cell populations.\niTALK: R package for analyzing and visualizing cell-cell communication using scRNA-seq data.\nCellChat: R package for inferring and visualizing intercellular communication networks.\nSingleCellSignalR: An R package for identifying cell-cell communication networks based on receptor-ligand interaction analysis.\n\n\n\n5. BCR Background and 10x Analysis:\n\nCell Ranger: For reconstructing BCR and TCR sequences from 10x Genomics single-cell data.\nVDJtools: Analyzing BCR/TCR repertoires from single-cell RNA-seq data.\nscRepertoire: R package for analyzing TCR/BCR sequences in single-cell datasets, providing clonotype tracking and diversity analysis.\n\n\n\n6. Trajectory Inference:\n\nMonocle3: R package for performing pseudotime analysis and reconstructing cell trajectories.\nSlingshot: R package for lineage inference and trajectory reconstruction from single-cell RNA-seq data.\nScanpy: Trajectory inference using methods like Palantir (tl.palantir) and pseudotime analysis with tl.dpt.\nPAGA: Partition-based graph abstraction for inferring lineage relationships and transitions between cell states in Scanpy.\nVelocyto: A tool for estimating RNA velocity in single cells, predicting the future states of cells in trajectory analysis.\ndynverse: A collection of methods and packages for trajectory inference, integrated with both Seurat and Scanpy workflows.\n\n\n\n7. Differential Abundance:\n\nDAseq: R package for differential abundance analysis of single-cell datasets, identifying significant differences in cell populations.\nMilo: For testing differential abundance of cell neighborhoods across conditions.\nedgeR: For differential expression analysis in scRNA-seq data.\nDESeq2: R package for differential expression analysis in bulk RNA-seq and scRNA-seq data.\n\n\n\n8. Multiomic scATAC:\n\nSignac: An R package built on top of Seurat for integrating scRNA-seq and scATAC-seq data.\nArchR: A comprehensive tool for single-cell ATAC-seq data analysis, including peak calling, dimensionality reduction, and clustering.\nCicero: Tool for inferring co-accessibility networks from scATAC-seq data.\nSnapATAC: For scATAC-seq clustering, visualization, and integration with scRNA-seq datasets.\nscVI: Probabilistic modeling for multi-modal single-cell data, supporting scRNA-seq and scATAC-seq integration.\n\n\n\nResearch Fellow: Developed ML-based pipeline for shotgun sequencing, 16S rRNA sequencing using QIIME2, RNA-seq using DESeq2, stats, ggplot, corrplot, pheatmap, EDASeq, gProfileR."
  }
]